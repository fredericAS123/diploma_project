"""
Step 6: æµå¼æ¨ç† vs åŸç”Ÿç¦»çº¿æ¨ç†å¯¹æ¯”æµ‹è¯•ï¼ˆæ ¸å¿ƒéœ€æ±‚ï¼‰

æµ‹è¯•ç›®æ ‡ï¼š
  1) ä½¿ç”¨çœŸå®è§†é¢‘ 1.mp4 (~3s, 30fps)
  2) æµå¼æ¨¡å¼ï¼šæŒ‰ 4 å¸§ chunk æµå¼ç¼–ç è‡³ 2sï¼Œæš‚åœåå›ç­”é—®é¢˜
  3) åŸç”Ÿæ¨¡å¼ï¼šä¸€æ¬¡æ€§åŠ è½½å®Œæ•´è§†é¢‘å¹¶å›ç­”åŒä¸€é—®é¢˜
  4) å¯¹æ¯”ï¼šVRAM ä½¿ç”¨ã€å“åº”æ—¶é—´ï¼ˆTTFTã€æ€»å»¶è¿Ÿï¼‰ã€ç­”æ¡ˆè´¨é‡

éœ€è¦ GPU + æ¨¡å‹æƒé‡ + è§†é¢‘æ–‡ä»¶ã€‚
"""
import os
import sys
import cv2
import time
import torch
from datetime import datetime
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from model import StreamQwenModel, VideoStreamingInference

MODEL_PATH = os.environ.get("QWEN_MODEL_PATH", "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct")
VIDEO_PATH = os.environ.get("VIDEO_PATH", "/root/autodl-tmp/diploma_project/temporal_encoding/1.mp4")
REPORT_PATH = os.environ.get(
    "STEP6_REPORT_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/test_step6_stream_vs_native_report.txt",
)
STREAM_DURATION = 2.0  # æµå¼ç¼–ç è‡³ 2 ç§’
CHUNK_SIZE = 4         # æ¯ä¸ª chunk åŒ…å« 4 å¸§
TEST_QUESTION = "Describe what is happening in this video."
BASE_FRAME_STRIDE = int(os.environ.get("FRAME_STRIDE", "1"))


class NativeOOMError(RuntimeError):
    """Native offline mode OOM error."""
    pass


class TeeWriter:
    """Write stdout/stderr to both console and file."""

    def __init__(self, *writers):
        self._writers = writers

    def write(self, text):
        for w in self._writers:
            w.write(text)
        self.flush()

    def flush(self):
        for w in self._writers:
            w.flush()


def _get_vram_gb():
    """è·å–å½“å‰ CUDA VRAM ä½¿ç”¨é‡ï¼ˆGBï¼‰ã€‚"""
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        allocated = torch.cuda.memory_allocated() / (1024 ** 3)
        reserved = torch.cuda.memory_reserved() / (1024 ** 3)
        return {"allocated": round(allocated, 2), "reserved": round(reserved, 2)}
    return {"allocated": 0.0, "reserved": 0.0}


def _load_video_frames(video_path: str, max_duration: float = None, frame_stride: int = 1):
    """ä»è§†é¢‘åŠ è½½å¸§ï¼Œè¿”å› (frames, fps, total_duration)ã€‚"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    frames = []
    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_idx % max(frame_stride, 1) == 0:
            # è½¬æ¢ä¸º PIL Image (RGB)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)
            frames.append(pil_img)
        
        frame_idx += 1
        if max_duration is not None and fps > 0:
            if frame_idx / fps >= max_duration:
                break
    
    cap.release()
    return frames, fps, duration


def test_streaming_mode(frame_stride: int = 1):
    """æµå¼æ¨¡å¼ï¼šé€ chunk ç¼–ç è‡³æŒ‡å®šæ—¶é•¿ï¼Œç„¶åå›ç­”é—®é¢˜ã€‚"""
    print("\n" + "=" * 70)
    print("ğŸ“¹ STREAMING MODE TEST")
    print("=" * 70)
    
    if not os.path.exists(VIDEO_PATH):
        print(f"âš ï¸  Video not found: {VIDEO_PATH}. Skip streaming test.")
        return None
    
    if not os.path.exists(MODEL_PATH):
        print(f"âš ï¸  Model not found: {MODEL_PATH}. Skip streaming test.")
        return None
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    
    # 1) åŠ è½½è§†é¢‘å¸§ï¼ˆä»…å‰ STREAM_DURATION ç§’ï¼‰
    print(f"\n[1] Loading video frames (first {STREAM_DURATION}s, stride={frame_stride})...")
    frames, video_fps, total_duration = _load_video_frames(
        VIDEO_PATH,
        max_duration=STREAM_DURATION,
        frame_stride=frame_stride,
    )
    print(f"    âœ… Loaded {len(frames)} frames (fps={video_fps:.2f}, total={total_duration:.2f}s)")
    
    # 2) åˆå§‹åŒ–æµå¼å¼•æ“
    print("\n[2] Initializing streaming engine...")
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    model = StreamQwenModel.from_pretrained(MODEL_PATH, torch_dtype=dtype).to(device)
    model.eval()
    engine = VideoStreamingInference(model, processor, device)
    
    vram_init = _get_vram_gb()
    print(f"    âœ… VRAM after model load: {vram_init}")
    
    # 3) æµå¼ç¼–ç ï¼ˆæŒ‰ CHUNK_SIZE åˆ† chunkï¼‰
    print(f"\n[3] Streaming encoding ({CHUNK_SIZE}-frame chunks)...")
    encode_start = time.time()
    
    num_chunks = (len(frames) + CHUNK_SIZE - 1) // CHUNK_SIZE
    for i in range(num_chunks):
        start_idx = i * CHUNK_SIZE
        end_idx = min(start_idx + CHUNK_SIZE, len(frames))
        chunk_frames = frames[start_idx:end_idx]
        
        chunk_fps = video_fps / frame_stride  # ä½¿ç”¨åŸå§‹è§†é¢‘å¸§ç‡/é‡‡æ ·æ­¥é•¿ä½œä¸º chunk fps
        status = engine.append_video_chunk(
            chunk_frames,
            fps=chunk_fps,
            text_content=f"Processing video chunk {i+1}/{num_chunks}."
        )
        print(f"    Chunk {i+1}/{num_chunks}: {status}")
    
    encode_end = time.time()
    encode_time = encode_end - encode_start
    
    cache_info = engine.get_cache_info()
    vram_after_encode = _get_vram_gb()
    print(f"\n    âœ… Encoding completed in {encode_time:.3f}s")
    print(f"    Cache info: {cache_info}")
    print(f"    VRAM after encoding: {vram_after_encode}")
    
    # 4) å›ç­”é—®é¢˜
    print(f"\n[4] Asking question: '{TEST_QUESTION}'")
    answer, metrics = engine.ask(TEST_QUESTION, max_new_tokens=128, update_state=False)
    
    vram_after_qa = _get_vram_gb()
    print(f"\n    âœ… Answer: {answer}")
    print(f"    TTFT: {metrics['ttft']:.3f}s")
    print(f"    Total QA latency: {metrics['total_latency']:.3f}s")
    print(f"    VRAM after QA: {vram_after_qa}")
    
    return {
        "mode": "streaming",
        "frames_encoded": len(frames),
        "encoding_time": round(encode_time, 3),
        "frame_stride": frame_stride,
        "cache_seq_length": cache_info["cache_seq_length"],
        "cache_memory_gb": cache_info["cache_memory_gb"],
        "vram_init": vram_init,
        "vram_after_encode": vram_after_encode,
        "vram_after_qa": vram_after_qa,
        "ttft": round(metrics["ttft"], 3),
        "total_qa_latency": round(metrics["total_latency"], 3),
        "answer": answer,
    }


def test_native_offline_mode(frame_stride: int = 1):
    """åŸç”Ÿç¦»çº¿æ¨¡å¼ï¼šä¸€æ¬¡æ€§åŠ è½½å®Œæ•´è§†é¢‘å¹¶å›ç­”é—®é¢˜ã€‚"""
    print("\n" + "=" * 70)
    print("ğŸ¬ NATIVE OFFLINE MODE TEST")
    print("=" * 70)
    
    if not os.path.exists(VIDEO_PATH):
        print(f"âš ï¸  Video not found: {VIDEO_PATH}. Skip native test.")
        return None
    
    if not os.path.exists(MODEL_PATH):
        print(f"âš ï¸  Model not found: {MODEL_PATH}. Skip native test.")
        return None
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    
    # 1) åŠ è½½è§†é¢‘å¸§ï¼ˆç›¸åŒçš„å‰ STREAM_DURATION ç§’ï¼Œä¿è¯å…¬å¹³å¯¹æ¯”ï¼‰
    print(f"\n[1] Loading video frames (first {STREAM_DURATION}s, stride={frame_stride})...")
    frames, video_fps, total_duration = _load_video_frames(
        VIDEO_PATH,
        max_duration=STREAM_DURATION,
        frame_stride=frame_stride,
    )
    print(f"    âœ… Loaded {len(frames)} frames (fps={video_fps:.2f}, total={total_duration:.2f}s)")
    
    # 2) åˆå§‹åŒ–åŸç”Ÿæ¨¡å‹
    print("\n[2] Initializing native model...")
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(MODEL_PATH, torch_dtype=dtype).to(device)
    model.eval()
    
    vram_init = _get_vram_gb()
    print(f"    âœ… VRAM after model load: {vram_init}")
    
    # 3) ä¸€æ¬¡æ€§ç¼–ç å®Œæ•´è§†é¢‘ + é—®é¢˜
    print(f"\n[3] Encoding full video + question...")
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": frames, "fps": video_fps},
                {"type": "text", "text": TEST_QUESTION},
            ]
        }
    ]
    
    text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(
        text=[text_prompt],
        videos=[frames],
        padding=True,
        return_tensors="pt",
        videos_kwargs={"fps": video_fps}
    ).to(device)
    
    encode_start = time.time()
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Prefill: è®¡ç®—é¦–ä¸ª token
    with torch.inference_mode():
        outputs = model(**inputs, use_cache=True)
        past_key_values = outputs.past_key_values
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    encode_end = time.time()
    ttft = encode_end - encode_start
    
    vram_after_prefill = _get_vram_gb()
    print(f"    âœ… Prefill completed, TTFT: {ttft:.3f}s")
    print(f"    VRAM after prefill: {vram_after_prefill}")
    
    # 4) Decode ç”Ÿæˆç­”æ¡ˆ
    print(f"\n[4] Generating answer...")
    decode_start = time.time()
    
    try:
        with torch.inference_mode():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=False,
            )
    except torch.OutOfMemoryError as exc:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        raise NativeOOMError("Native offline generate OOM") from exc
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    decode_end = time.time()
    total_latency = decode_end - encode_start
    
    # æå–ç”Ÿæˆçš„ tokenï¼ˆç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
    input_len = inputs.input_ids.shape[1]
    generated_tokens = generated_ids[0, input_len:]
    answer = processor.decode(generated_tokens, skip_special_tokens=True)
    
    vram_after_decode = _get_vram_gb()
    print(f"\n    âœ… Answer: {answer}")
    print(f"    TTFT: {ttft:.3f}s")
    print(f"    Total latency: {total_latency:.3f}s")
    print(f"    VRAM after decode: {vram_after_decode}")
    
    # ä¼°ç®— cache å¤§å°
    cache_seq_len = 0
    if past_key_values is not None:
        if hasattr(past_key_values, "get_seq_length"):
            cache_seq_len = past_key_values.get_seq_length()
        elif isinstance(past_key_values, (tuple, list)) and len(past_key_values) > 0:
            cache_seq_len = past_key_values[0][0].shape[2]
    
    return {
        "mode": "native_offline",
        "frames_encoded": len(frames),
        "encoding_time": round(ttft, 3),
        "frame_stride": frame_stride,
        "cache_seq_length": cache_seq_len,
        "vram_init": vram_init,
        "vram_after_prefill": vram_after_prefill,
        "vram_after_decode": vram_after_decode,
        "ttft": round(ttft, 3),
        "total_latency": round(total_latency, 3),
        "answer": answer,
    }


def print_comparison(streaming_result, native_result, attempt_logs):
    """æ‰“å°è¯¦ç»†å¯¹æ¯”æŠ¥å‘Šã€‚"""
    print("\n" + "=" * 70)
    print("ğŸ“Š COMPARISON REPORT")
    print("=" * 70)
    
    if streaming_result is None or native_result is None:
        print("âš ï¸  One or both tests failed. Cannot compare.")
        return
    
    print("\n[Encoding Performance]")
    print(f"  Streaming encoding time: {streaming_result['encoding_time']}s")
    print(f"  Native prefill time:     {native_result['encoding_time']}s")

    print("\n[Frame Sampling]")
    print(f"  Streaming frame stride:  {streaming_result['frame_stride']}")
    print(f"  Native frame stride:     {native_result['frame_stride']}")
    
    print("\n[QA Performance]")
    print(f"  Streaming TTFT:          {streaming_result['ttft']}s")
    print(f"  Native TTFT:             {native_result['ttft']}s")
    print(f"  Streaming total QA:      {streaming_result['total_qa_latency']}s")
    print(f"  Native total latency:    {native_result['total_latency']}s")
    
    print("\n[VRAM Usage (Allocated GB)]")
    print(f"  Streaming after encode:  {streaming_result['vram_after_encode']['allocated']} GB")
    print(f"  Native after prefill:    {native_result['vram_after_prefill']['allocated']} GB")
    print(f"  Streaming after QA:      {streaming_result['vram_after_qa']['allocated']} GB")
    print(f"  Native after decode:     {native_result['vram_after_decode']['allocated']} GB")
    
    print("\n[Cache Info]")
    print(f"  Streaming cache length:  {streaming_result['cache_seq_length']}")
    print(f"  Streaming cache memory:  {streaming_result['cache_memory_gb']} GB")
    print(f"  Native cache length:     {native_result['cache_seq_length']}")
    
    print("\n[Answers]")
    print(f"  Streaming: {streaming_result['answer'][:100]}...")
    print(f"  Native:    {native_result['answer'][:100]}...")

    print("\n[Conversation Log]")
    print(f"  Question: {TEST_QUESTION}")
    print(f"  Streaming Answer: {streaming_result['answer']}")
    print(f"  Native Answer: {native_result['answer']}")

    print("\n[Retry/Iteration Log]")
    for entry in attempt_logs:
        print(f"  - {entry}")

    print("\n[Analysis]")
    ttft_speedup = native_result["ttft"] / max(streaming_result["ttft"], 1e-6)
    total_speedup = native_result["total_latency"] / max(streaming_result["total_qa_latency"], 1e-6)
    vram_prefill = native_result["vram_after_prefill"]["allocated"]
    vram_stream = streaming_result["vram_after_encode"]["allocated"]
    vram_delta = vram_prefill - vram_stream
    print(f"  TTFT speedup (Native/Streaming): {ttft_speedup:.2f}x")
    print(f"  Total latency speedup (Native/Streaming): {total_speedup:.2f}x")
    print(f"  VRAM delta (Native prefill - Streaming encode): {vram_delta:.2f} GB")
    if streaming_result["frame_stride"] != 1:
        print("  Note: Frame stride > 1 was used to avoid native OOM; results are fair within the same stride.")
    
    print("\n" + "=" * 70)
    print("âœ… Step 6 COMPLETED: Streaming vs Native Offline Comparison")
    print("=" * 70)


def main():
    report_dir = os.path.dirname(REPORT_PATH)
    if report_dir:
        os.makedirs(report_dir, exist_ok=True)

    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        tee = TeeWriter(sys.stdout, f)
        original_stdout = sys.stdout
        original_stderr = sys.stderr
        sys.stdout = tee
        sys.stderr = tee
        try:
            print("=" * 70)
            print("TEST Step 6: Streaming vs Native Offline Inference Comparison")
            print("=" * 70)
            print(f"Report time: {datetime.now().isoformat(timespec='seconds')}")
            print(f"Video: {VIDEO_PATH}")
            print(f"Model: {MODEL_PATH}")
            print(f"Stream duration: {STREAM_DURATION}s")
            print(f"Chunk size: {CHUNK_SIZE} frames")
            print(f"Question: '{TEST_QUESTION}'")
    
    # å°è¯•ä¸åŒå¸§é‡‡æ ·æ­¥é•¿ï¼Œé¿å… Native OOM
            stride_candidates = [max(BASE_FRAME_STRIDE, 1)]
            stride_candidates += [stride_candidates[0] * 2, stride_candidates[0] * 4]
            stride_candidates = [s for i, s in enumerate(stride_candidates) if s not in stride_candidates[:i]]

            streaming_result = None
            native_result = None
            attempt_logs = []

            for stride in stride_candidates:
                print("\n" + "-" * 70)
                print(f"Attempt with frame stride = {stride}")
                print("-" * 70)

                # æµ‹è¯•æµå¼æ¨¡å¼
                streaming_result = test_streaming_mode(frame_stride=stride)
                attempt_logs.append(f"Stride {stride}: streaming OK")

                # æ¸…ç† GPU å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # æµ‹è¯•åŸç”Ÿæ¨¡å¼
                try:
                    native_result = test_native_offline_mode(frame_stride=stride)
                    attempt_logs.append(f"Stride {stride}: native OK")
                    break
                except NativeOOMError:
                    attempt_logs.append(f"Stride {stride}: native OOM")
                    print("âš ï¸  Native offline mode OOM. Retrying with higher frame stride...")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    continue

            # æ‰“å°å¯¹æ¯”æŠ¥å‘Š
            print_comparison(streaming_result, native_result, attempt_logs)
            print(f"\nReport saved to: {REPORT_PATH}")
        finally:
            sys.stdout = original_stdout
            sys.stderr = original_stderr


if __name__ == "__main__":
    main()
