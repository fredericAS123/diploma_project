"""
Delayed Batch Inference Engine - æ–°æ–¹æ¡ˆæ ¸å¿ƒå®ç°

æ ¸å¿ƒæ€æƒ³ï¼š
1. æµå¼æ”¶é›†å¸§ï¼ˆadd_frameï¼‰
2. å»¶è¿Ÿåˆ°æé—®æ—¶æ‰æ‰¹é‡ç¼–ç ï¼ˆaskï¼‰
3. ä½¿ç”¨ video æ¨¡å¼äº«å— temporal merge å‹ç¼©
4. Vision Encoder èƒ½çœ‹åˆ°æ‰€æœ‰å¸§ï¼Œå»ºç«‹å®Œæ•´çš„è·¨å¸§æ³¨æ„åŠ›

ä¼˜åŠ¿ï¼š
- âœ… Vision Encoder è·¨å¸§æ³¨æ„åŠ›ï¼šå®Œæ•´
- âœ… KV Cache å®Œæ•´æ€§ï¼šå®Œæ•´ï¼ˆä¸ä¸¢å¤±å†å²ï¼‰
- âœ… å®ç°éš¾åº¦ï¼šä¸­ç­‰
- âœ… æ˜¾å­˜æ§åˆ¶ï¼šé€šè¿‡æ™ºèƒ½å¸§ç®¡ç†
"""

import torch
from transformers import AutoProcessor
import gc
import time
from typing import List, Dict, Optional, Tuple
from PIL import Image

from .smart_frame_manager import SmartFrameManager


class DelayedBatchInferenceEngine:
    """å»¶è¿Ÿæ‰¹é‡ç¼–ç æ¨ç†å¼•æ“"""
    
    def __init__(
        self,
        model,
        processor,
        device: str = "cuda",
        star_memory_size: int = 20,
        stream_window_size: int = 20,
        max_pixels: int = 4 * 224 * 224,  # å€Ÿé‰´Flash-VStreamçš„ä½åˆ†è¾¨ç‡ç­–ç•¥
        min_pixels: int = 4 * 28 * 28,
    ):
        """
        Args:
            model: Qwen2.5-VL æ¨¡å‹
            processor: å¯¹åº”çš„ processor
            device: è®¾å¤‡
            star_memory_size: Star Memory å®¹é‡
            stream_window_size: Stream Memory çª—å£å¤§å°
            max_pixels: æœ€å¤§åƒç´ ï¼ˆç”¨äº video æ¨¡å¼ï¼‰
            min_pixels: æœ€å°åƒç´ 
        """
        self.model = model
        self.processor = processor
        self.device = device
        self.max_pixels = max_pixels
        self.min_pixels = min_pixels
        
        # æ™ºèƒ½å¸§ç®¡ç†å™¨
        self.frame_manager = SmartFrameManager(
            star_memory_size=star_memory_size,
            stream_window_size=stream_window_size,
        )
        
        # KV Cacheï¼ˆåœ¨æé—®æ—¶ç”Ÿæˆï¼‰
        self.video_cache = None
        self.cache_is_valid = False
        
        # è®°å½•æ¨¡å‹æ•°æ®ç±»å‹
        try:
            self.model_dtype = next(model.parameters()).dtype
        except StopIteration:
            self.model_dtype = torch.float32
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.encode_count = 0
        self.total_frames_processed = 0
        
        print(f"âœ… DelayedBatchInferenceEngine Initialized")
        print(f"   ğŸ“ Max Pixels: {max_pixels} ({int(max_pixels**0.5)}x{int(max_pixels**0.5)})")
        print(f"   ğŸ¬ Strategy: æµå¼æ”¶é›† + å»¶è¿Ÿæ‰¹é‡ç¼–ç ")
    
    def add_frame(self, frame: Image.Image, timestamp: float = None) -> str:
        """
        æ·»åŠ æ–°å¸§ï¼ˆæµå¼æ”¶é›†ï¼‰
        
        Args:
            frame: PIL Image
            timestamp: æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        
        Returns:
            çŠ¶æ€ä¿¡æ¯
        """
        if timestamp is None:
            timestamp = time.time()
        
        result = self.frame_manager.add_frame(frame, timestamp)
        self.total_frames_processed += 1
        
        # æ ‡è®° cache å¤±æ•ˆï¼ˆæœ‰æ–°å¸§åŠ å…¥ï¼‰
        self.cache_is_valid = False
        
        # æ„å»ºçŠ¶æ€æ¶ˆæ¯
        status = f"Frame #{self.total_frames_processed} added to Stream Memory"
        if result['added_to_star']:
            status += f" + Star Memory ({result['reason']})"
        
        status += f" | Star: {result['star_count']}, Stream: {result['stream_count']}"
        
        return status
    
    def _encode_all_frames(self) -> Dict[str, any]:
        """
        æ‰¹é‡ç¼–ç æ‰€æœ‰å¸§ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œåœ¨æé—®æ—¶è°ƒç”¨ï¼‰
        
        ä½¿ç”¨ video æ¨¡å¼ï¼š
        - Vision Encoder èƒ½çœ‹åˆ°æ‰€æœ‰å¸§
        - äº«å— temporal merge çš„å‹ç¼©
        - å»ºç«‹å®Œæ•´çš„è·¨å¸§æ³¨æ„åŠ›
        """
        # è·å–æ‰€æœ‰éœ€è¦ç¼–ç çš„å¸§
        all_frames, metadata = self.frame_manager.get_all_frames()
        
        if not all_frames:
            return {'success': False, 'reason': 'no frames'}
        
        print(f"\nğŸ”„ [ç¼–ç  #{self.encode_count + 1}] æ‰¹é‡ç¼–ç  {metadata['unique_frames']} å¸§")
        print(f"   ğŸ“Š Star: {metadata['star_frames']}, Stream: {metadata['stream_frames']}, "
              f"é‡å : {metadata['overlap_frames']}")
        print(f"   ğŸ“‰ å‹ç¼©æ¯”: {metadata['compression_ratio']:.2f}x "
              f"(ä» {metadata['total_added']} å¸§å‹ç¼©åˆ° {metadata['unique_frames']} å¸§)")
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆä½¿ç”¨ video æ¨¡å¼ï¼‰
        messages = [{
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": all_frames,
                    "min_pixels": self.min_pixels,
                    "max_pixels": self.max_pixels,
                },
                {"type": "text", "text": "Watch this video stream."},
            ],
        }]
        
        # åº”ç”¨ chat template
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        
        # å¤„ç†è¾“å…¥
        t_start = time.time()
        inputs = self.processor(
            text=[text_prompt],
            videos=[all_frames],
            padding=True,
            return_tensors="pt",
        ).to(self.device)
        
        # è½¬æ¢æ•°æ®ç±»å‹
        if "pixel_values_videos" in inputs and inputs["pixel_values_videos"] is not None:
            inputs["pixel_values_videos"] = inputs["pixel_values_videos"].to(
                device=self.device,
                dtype=self.model_dtype,
            )
        
        # ç¼–ç ï¼ˆç”Ÿæˆ KV Cacheï¼‰
        with torch.inference_mode():
            outputs = self.model(
                **{k: v for k, v in inputs.items() if k not in ["attention_mask"]},
                attention_mask=inputs.get("attention_mask", None),
                past_key_values=None,  # ä»é›¶å¼€å§‹
                use_cache=True,
                output_hidden_states=False,
                logits_to_keep=1,
            )
        
        # ä¿å­˜ KV Cache
        self.video_cache = self._detach_past(outputs.past_key_values)
        self.cache_is_valid = True
        self.encode_count += 1
        
        t_end = time.time()
        
        # æå–è§†è§‰ token æ•°é‡
        visual_tokens = self._extract_visual_tokens_from_inputs(inputs)
        cache_length = self._get_past_len(self.video_cache)
        
        print(f"   âœ… ç¼–ç å®Œæˆï¼")
        print(f"   â±ï¸  è€—æ—¶: {t_end - t_start:.2f}s")
        print(f"   ğŸ¯ Visual Tokens: {visual_tokens}")
        print(f"   ğŸ’¾ KV Cache Length: {cache_length}")
        print(f"   ğŸ“ video_grid_thw: {inputs.get('video_grid_thw')}")
        
        return {
            'success': True,
            'frames_encoded': metadata['unique_frames'],
            'visual_tokens': visual_tokens,
            'cache_length': cache_length,
            'encoding_time': t_end - t_start,
            'video_grid_thw': inputs.get('video_grid_thw'),
        }
    
    def ask(
        self,
        question: str,
        max_new_tokens: int = 512,
        min_new_tokens: int = 1,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1,
    ) -> Tuple[str, Dict]:
        """
        æé—®ï¼ˆå¦‚æœ cache å¤±æ•ˆï¼Œä¼šè‡ªåŠ¨é‡æ–°ç¼–ç æ‰€æœ‰å¸§ï¼‰
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            å…¶ä»–å‚æ•°: ç”Ÿæˆå‚æ•°
        
        Returns:
            (answer, metrics)
        """
        t_total_start = time.time()
        
        # 1. å¦‚æœ cache å¤±æ•ˆï¼Œé‡æ–°ç¼–ç æ‰€æœ‰å¸§
        encode_result = None
        if not self.cache_is_valid:
            encode_result = self._encode_all_frames()
            if not encode_result['success']:
                return f"Error: {encode_result['reason']}", {}
        
        # 2. ç¼–ç é—®é¢˜
        question_prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        question_inputs = self.processor.tokenizer(
            question_prompt,
            return_tensors="pt",
            add_special_tokens=False,
        ).to(self.device)
        
        # 3. æ„å»ºå®Œæ•´çš„ attention mask
        past_len = self._get_past_len(self.video_cache)
        full_mask = self._build_full_attention_mask(question_inputs.attention_mask, past_len)
        
        cache_position = None
        if past_len and past_len > 0:
            cache_position = torch.arange(
                past_len,
                past_len + question_inputs.input_ids.shape[1],
                device=question_inputs.input_ids.device,
            )
        
        # 4. ç”Ÿæˆå›ç­”
        t_gen_start = time.time()
        with torch.inference_mode():
            generated_ids = self.model.generate(
                input_ids=question_inputs.input_ids,
                attention_mask=full_mask,
                past_key_values=self.video_cache,
                cache_position=cache_position,
                max_new_tokens=max_new_tokens,
                min_new_tokens=min_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                pad_token_id=self.processor.tokenizer.pad_token_id,
                eos_token_id=self.processor.tokenizer.eos_token_id,
                use_cache=True,
            )
        t_gen_end = time.time()
        
        # 5. è§£ç è¾“å‡º
        output_ids = generated_ids[0, question_inputs.input_ids.shape[1]:]
        answer = self.processor.tokenizer.decode(output_ids, skip_special_tokens=True)
        
        t_total_end = time.time()
        
        # 6. ç»Ÿè®¡ä¿¡æ¯
        metrics = {
            'total_latency': t_total_end - t_total_start,
            'generation_latency': t_gen_end - t_gen_start,
            'output_tokens': len(output_ids),
        }
        
        if encode_result:
            metrics['encoding_latency'] = encode_result['encoding_time']
            metrics['frames_encoded'] = encode_result['frames_encoded']
            metrics['visual_tokens'] = encode_result['visual_tokens']
        
        return answer, metrics
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        frame_stats = self.frame_manager.get_statistics()
        
        return {
            'total_frames_added': self.total_frames_processed,
            'encode_count': self.encode_count,
            'cache_valid': self.cache_is_valid,
            **frame_stats,
        }
    
    def reset(self):
        """é‡ç½®å¼•æ“"""
        self.frame_manager.reset()
        self.video_cache = None
        self.cache_is_valid = False
        self.encode_count = 0
        self.total_frames_processed = 0
        
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        
        print("ğŸ”„ DelayedBatchInferenceEngine Reset.")
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _detach_past(self, past_key_values):
        """åˆ†ç¦» past_key_values ä»è®¡ç®—å›¾"""
        if past_key_values is None:
            return None
        if hasattr(past_key_values, "get_seq_length"):
            return past_key_values
        return tuple(tuple(p.detach() for p in layer) for layer in past_key_values)
    
    def _get_past_len(self, past_key_values):
        """è·å– past_key_values çš„é•¿åº¦"""
        if past_key_values is None:
            return 0
        if hasattr(past_key_values, "get_seq_length"):
            return past_key_values.get_seq_length()
        return past_key_values[0][0].shape[-2]
    
    def _build_full_attention_mask(self, attention_mask, past_len):
        """æ„å»ºå®Œæ•´çš„ attention maskï¼ˆåŒ…æ‹¬ pastï¼‰"""
        if past_len is None or past_len == 0:
            return attention_mask
        past_mask = torch.ones(
            (attention_mask.shape[0], past_len),
            dtype=attention_mask.dtype,
            device=attention_mask.device,
        )
        return torch.cat([past_mask, attention_mask], dim=1)
    
    def _extract_visual_tokens_from_inputs(self, inputs):
        """ä» processor è¾“å‡ºä¸­æå– visual tokens çš„æ•°é‡"""
        if "video_grid_thw" not in inputs:
            return 0
        
        video_grid_thw = inputs["video_grid_thw"]
        merge_length = getattr(self.processor, "merge_size", 2) ** 2
        
        # video_grid_thw shape: (num_videos, 3) -> (T, H, W)
        num_video_tokens = (video_grid_thw[0].prod() // merge_length).item() if len(video_grid_thw) > 0 else 0
        return num_video_tokens
