"""
QwenInferenceWrapper â€” Web Demo åç«¯æ¨ç†å¼•æ“å°è£…

è¿æ¥ temporal_encoding.model æµå¼æ¨ç†å¼•æ“ä¸ Gradio Web å‰ç«¯ã€‚
å·²é€‚é…æ–°çš„ APIï¼ˆæ—  manual_timeï¼Œä½¿ç”¨ append_video_chunk / ask / ask_choiceï¼‰ã€‚
"""
import os
import sys
import time
import threading
import torch
from transformers import AutoProcessor

_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
_PROJECT_ROOT = os.path.abspath(os.path.join(_CURRENT_DIR, os.pardir))
_TEMPORAL_DIR = os.path.join(_PROJECT_ROOT, "temporal_encoding")
if _TEMPORAL_DIR not in sys.path:
    sys.path.insert(0, _TEMPORAL_DIR)

from model import StreamQwenModel, VideoStreamingInference


class QwenInferenceWrapper:
    """Streaming backend adapter for Qwen2.5-VL with automatic mRoPE position tracking."""

    def __init__(self, model_path: str = "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct", device: str | None = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.inference_lock = threading.Lock()

        print(f"Loading StreamQwenModel from: {model_path} ...")

        if self.device.startswith("cuda"):
            self.model = StreamQwenModel.from_pretrained(
                model_path,
                torch_dtype="auto",
                device_map="auto",
            )
        else:
            self.model = StreamQwenModel.from_pretrained(
                model_path,
                torch_dtype="auto",
            ).to(self.device)

        self.processor = AutoProcessor.from_pretrained(model_path)
        self.engine = VideoStreamingInference(self.model, self.processor, device=self.device)
        print("âœ… StreamQwenModel + VideoStreamingInference ready.")

    def log_vram(self, tag: str = ""):
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            print(f"ğŸ“Š [VRAM-{tag}] Alloc: {allocated:.2f}GB | Reserved: {reserved:.2f}GB")

    def reset(self):
        """é‡ç½®æµå¼å¼•æ“ï¼ˆæ–°è§†é¢‘å¼€å§‹æ—¶è°ƒç”¨ï¼‰ã€‚"""
        with self.inference_lock:
            self.engine.reset()

    def process_frame(self, image, **kwargs) -> str:
        """
        ç¼–ç å•å¸§åˆ°æŒä¹… KV Cacheã€‚

        Args:
            image: PIL Image
            **kwargs: å…¼å®¹æ—§æ¥å£ï¼ˆå¿½ç•¥ manual_time ç­‰åºŸå¼ƒå‚æ•°ï¼‰
        """
        with self.inference_lock:
            return self.engine.append_frame(image)

    def process_video_chunk(self, frames, fps: float = 2.0) -> str:
        """
        ç¼–ç å¤šå¸§è§†é¢‘ chunk åˆ°æŒä¹… KV Cacheï¼ˆæ¨èæ–¹å¼ï¼‰ã€‚

        Args:
            frames: PIL Image åˆ—è¡¨
            fps: å¸§ç‡
        """
        with self.inference_lock:
            return self.engine.append_video_chunk(frames, fps=fps)

    def ask_question(
        self,
        question: str,
        max_new_tokens: int = 256,
        min_new_tokens: int = 1,
        do_sample: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.9,
        update_state: bool = False,
        return_metrics: bool = False,
        **kwargs,
    ):
        """
        åŸºäºå·²ç¼“å­˜çš„è§†é¢‘è®°å¿†å›ç­”é—®é¢˜ã€‚

        Args:
            **kwargs: å…¼å®¹æ—§æ¥å£ï¼ˆå¿½ç•¥ manual_time ç­‰åºŸå¼ƒå‚æ•°ï¼‰
        """
        with self.inference_lock:
            response, metrics = self.engine.ask(
                question,
                max_new_tokens=max_new_tokens,
                min_new_tokens=min_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                update_state=update_state,
            )
        if return_metrics:
            return response, metrics
        return response

    def ask_choice(self, question: str, choices: list[str]) -> str:
        """å¤šé€‰é¢˜è¯„åˆ†ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„é€‰é¡¹ã€‚"""
        with self.inference_lock:
            return self.engine.ask_choice(question, choices)

    def get_cache_info(self) -> dict:
        """è·å–å½“å‰ KV Cache çŠ¶æ€ä¿¡æ¯ã€‚"""
        return self.engine.get_cache_info()

    def native_video_inference(
        self,
        frames,
        question: str,
        fps: float = 2.0,
        max_new_tokens: int = 256,
        min_new_tokens: int = 1,
        do_sample: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ):
        """
        åŸç”Ÿè§†é¢‘æ¨ç†ï¼šä¸€æ¬¡æ€§å°†æ‰€æœ‰å¸§+é—®é¢˜é€å…¥æ¨¡å‹ï¼ˆéæµå¼ï¼‰ã€‚
        ç”¨äºä¸æµå¼æ¨ç†è¿›è¡Œ TTFT / VRAM å¯¹æ¯”ã€‚

        Returns:
            (response_text, metrics_dict)
        """
        with self.inference_lock:
            # ä¿å­˜å½“å‰æµå¼çŠ¶æ€ï¼ˆæ¨ç†åæ¢å¤ï¼Œé¿å…ç ´å streaming å¼•æ“ï¼‰
            saved_state = None
            if hasattr(self.model, 'stream_state'):
                saved_state = self.model.stream_state
            if hasattr(self.model, 'reset_stream_state'):
                self.model.reset_stream_state()

            # é‡ç½® VRAM å³°å€¼ç»Ÿè®¡
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.empty_cache()

            system_prompt = (
                "You are a concise video analyst. Answer briefly and directly. "
                "Focus on visible facts only. Avoid speculation, avoid repetition. "
                "Strictly limit the response to at most 60 tokens."
            )

            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
                {"role": "user", "content": [
                    {"type": "video", "video": frames},
                    {"type": "text", "text": question},
                ]}
            ]

            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            inputs = self.processor(
                text=[text_prompt],
                videos=[frames],
                padding=True,
                return_tensors="pt",
                videos_kwargs={"fps": fps},
            ).to(self.device)

            input_token_count = inputs.input_ids.shape[1]

            if torch.cuda.is_available():
                torch.cuda.synchronize()
            t_start = time.perf_counter()

            gen_kwargs = {
                "max_new_tokens": max_new_tokens,
                "min_new_tokens": min_new_tokens,
                "do_sample": do_sample,
            }
            if do_sample:
                gen_kwargs["temperature"] = temperature
                gen_kwargs["top_p"] = top_p

            output_ids = self.model.generate(**inputs, **gen_kwargs)

            if torch.cuda.is_available():
                torch.cuda.synchronize()
            t_end = time.perf_counter()

            generated_ids = output_ids[:, input_token_count:]
            response = self.processor.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]

            vram_peak_gb = 0.0
            if torch.cuda.is_available():
                vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)

            total_latency = t_end - t_start
            metrics = {
                "ttft": total_latency,
                "total_latency": total_latency,
                "vram_peak_gb": round(vram_peak_gb, 3),
                "num_frames": len(frames),
                "input_tokens": input_token_count,
                "output_tokens": generated_ids.shape[1],
            }

            print(
                f"ğŸ“Š [Native] Frames={len(frames)}, InputTok={input_token_count}, "
                f"Latency={total_latency:.2f}s, VRAM Peak={vram_peak_gb:.2f}GB"
            )

            # æ¸…ç†ä¸´æ—¶å¼ é‡
            del inputs, output_ids, generated_ids
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # æ¢å¤æµå¼çŠ¶æ€
            if saved_state is not None:
                self.model.stream_state = saved_state
            elif hasattr(self.model, 'reset_stream_state'):
                self.model.reset_stream_state()

            return response, metrics

    def single_frame_inference(
        self,
        image,
        question: str,
        max_new_tokens: int = 256,
        min_new_tokens: int = 1,
        do_sample: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ):
        """
        å•å¸§å›¾ç‰‡æ¨ç†ï¼šä»…ä¼ å…¥ä¸€å¸§å›¾ç‰‡ + é—®é¢˜ï¼ˆimage æ¨¡æ€ï¼‰ã€‚
        ç”¨äºå¯¹æ¯”ã€Œè§†é¢‘ç†è§£ vs å›¾ç‰‡ç†è§£ã€ä»¥åŠã€Œæµå¼/åŸç”Ÿ vs å•å¸§ã€çš„å·®å¼‚ã€‚

        Returns:
            (response_text, metrics_dict)
        """
        with self.inference_lock:
            # ä¿å­˜å½“å‰æµå¼çŠ¶æ€
            saved_state = None
            if hasattr(self.model, 'stream_state'):
                saved_state = self.model.stream_state
            if hasattr(self.model, 'reset_stream_state'):
                self.model.reset_stream_state()

            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.empty_cache()

            system_prompt = (
                "You are a concise video analyst. Answer briefly and directly. "
                "Focus on visible facts only. Avoid speculation, avoid repetition. "
                "Strictly limit the response to at most 60 tokens."
            )

            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
                {"role": "user", "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": question},
                ]}
            ]

            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            inputs = self.processor(
                text=[text_prompt],
                images=[image],
                padding=True,
                return_tensors="pt",
            ).to(self.device)

            input_token_count = inputs.input_ids.shape[1]

            if torch.cuda.is_available():
                torch.cuda.synchronize()
            t_start = time.perf_counter()

            gen_kwargs = {
                "max_new_tokens": max_new_tokens,
                "min_new_tokens": min_new_tokens,
                "do_sample": do_sample,
            }
            if do_sample:
                gen_kwargs["temperature"] = temperature
                gen_kwargs["top_p"] = top_p

            output_ids = self.model.generate(**inputs, **gen_kwargs)

            if torch.cuda.is_available():
                torch.cuda.synchronize()
            t_end = time.perf_counter()

            generated_ids = output_ids[:, input_token_count:]
            response = self.processor.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]

            vram_peak_gb = 0.0
            if torch.cuda.is_available():
                vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)

            total_latency = t_end - t_start
            metrics = {
                "ttft": total_latency,
                "total_latency": total_latency,
                "vram_peak_gb": round(vram_peak_gb, 3),
                "input_tokens": input_token_count,
                "output_tokens": generated_ids.shape[1],
            }

            print(
                f"ğŸ“Š [Single-Frame] InputTok={input_token_count}, "
                f"Latency={total_latency:.2f}s, VRAM Peak={vram_peak_gb:.2f}GB"
            )

            del inputs, output_ids, generated_ids
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # æ¢å¤æµå¼çŠ¶æ€
            if saved_state is not None:
                self.model.stream_state = saved_state
            elif hasattr(self.model, 'reset_stream_state'):
                self.model.reset_stream_state()

            return response, metrics
