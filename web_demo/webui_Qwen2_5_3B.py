import re
import threading
import gradio as gr
import torch
from PIL import Image
import numpy as np
import time

# --- å¼•å…¥å¿…è¦çš„åº“ ---
try:
    from decord import VideoReader, cpu
except ImportError:
    print("Error: 'decord' not found. Please run `pip install decord`")

try:
    from modelscope import Qwen2_5_VLForConditionalGeneration, AutoProcessor
    from qwen_vl_utils import process_vision_info
except ImportError:
    # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœ modelscope æŠ¥é”™ï¼Œå°è¯•ç›´æ¥ä» transformers å¯¼å…¥
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        from qwen_vl_utils import process_vision_info
    except ImportError:
        print("Error: libs not found. Please run: pip install modelscope qwen-vl-utils transformers")


MODEL_PATH = "Qwen/Qwen2.5-VL-3B-Instruct" 

class HistorySynchronizer:
    def __init__(self):
        self.chat_history = []
        self.frame_count = 0

    def get_chat_history(self):
        return self.chat_history

    def update(self, role, content):
        self.chat_history.append(gr.ChatMessage(role=role, content=str(content)))

    def set_history(self, history):
        self.chat_history = history

    def reset(self):
        self.chat_history = []
        self.frame_count = 0

# --- WebUI ç±» ---
class VideoChatWebUI:
    def __init__(self, model_path=MODEL_PATH):
        """
        åˆå§‹åŒ–ï¼šåŠ è½½ Qwen2.5-VL æ¨¡å‹
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Loading Qwen2.5-VL model from: {model_path} ...")
        
        try:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype="auto",
                device_map="auto",
            )
            self.processor = AutoProcessor.from_pretrained(model_path)
            print("Model loaded successfully!")
        except Exception as e:
            print(f"Error loading model: {e}")
            self.model = None
            self.processor = None

        self.history_synchronizer = HistorySynchronizer()
        
        # [æ§åˆ¶æ ‡å¿—]
        self.pause_event = threading.Event()
        self.pause_event.set() 
        self.stop_signal = False  # [æ–°å¢] ç”¨äºå¼ºåˆ¶ä¸­æ–­å¾ªç¯çš„æ ‡å¿—
        
        # [çº¿ç¨‹å®‰å…¨]
        self.inference_lock = threading.Lock() 
        self.is_streaming = False            
        
        # [æ•°æ®ç¼“å­˜]
        self.cached_video_path = None # [æ–°å¢] è®°å½•ä¸Šæ¬¡åŠ è½½çš„è§†é¢‘è·¯å¾„
        self.cached_video_data = None # [æ–°å¢] å­˜å‚¨ (pil_frames, original_frames, timestamps)
        
        self.log_vram("Init")


    def _load_video(self, video_path, fps=1):
        """
        åŠ è½½è§†é¢‘å¹¶æŠ½å¸§
        """
        vr = VideoReader(video_path, ctx=cpu(0))
        max_frame = len(vr) - 1
        frame_indices = np.arange(0, max_frame, int(vr.get_avg_fps() / fps))
        
        pil_frames = []
        original_frames = []
        
        print(f"Processing video, extracting {len(frame_indices)} frames...")
        for frame_index in frame_indices:
            img_np = vr[frame_index].asnumpy()
            img_pil = Image.fromarray(img_np)
            pil_frames.append(img_pil)
            original_frames.append(img_np)

        timestamps = frame_indices / vr.get_avg_fps()
        return pil_frames, original_frames, timestamps

    def log_vram(self, tag=""):
        """
        [å·¥å…·] æ‰“å°å½“å‰æ˜¾å­˜å ç”¨
        """
        if torch.cuda.is_available():
            # å·²åˆ†é…ï¼šTensor å®é™…å ç”¨çš„æ˜¾å­˜
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            # å·²é¢„ç•™ï¼šPyTorch å‘ OS ç”³è¯·çš„æ€»æ˜¾å­˜ (åŒ…å«ç¢ç‰‡)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            print(f"ğŸ“Š [VRAM-{tag}] Alloc: {allocated:.2f}GB | Reserved: {reserved:.2f}GB")

    def _run_qwen_inference(self, pil_frames, prompt_text, use_system_prompt=True):
        """
        [æ™®é€šæ¨ç†] æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—ï¼Œä¸ä½¿ç”¨ KV Cache
        [æ–°å¢]ä½¿ç”¨ self.inference_lock ç¡®ä¿çº¿ç¨‹å®‰å…¨
        """
        if self.model is None:
            return "Error: Model not loaded."

        # ä½¿ç”¨é”åŒ…è£¹æ¨ç†è¿‡ç¨‹ï¼Œé˜²æ­¢æµå¼æ¨ç†å’Œæ‰‹åŠ¨æ¨ç†æ’è½¦
        with self.inference_lock:
            # System Prompt
            SYSTEM_PROMPT = (
                        "You are a professional video surveillance AI. "
                        "When asked to 'Report status', follow these rules strictly:"
                        "\n1. If the scene is static, empty, or contains only insignificant background movements (like trees blowing), output EXACTLY: '[WAIT]'."
                        "\n2. ONLY output a description if there is a meaningful EVENT or ACTION happening."
                        "\n3. Be concise."
                        "\n4. Do not repeat previous information."
                    )

        content_list = []
        for img in pil_frames:
            content_list.append({"type": "image", "image": img})
        
        # ç”¨æˆ·æŒ‡ä»¤
        content_list.append({"type": "text", "text": prompt_text})

        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åŒ…å«ç³»ç»Ÿæç¤º
        if use_system_prompt:
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": SYSTEM_PROMPT}]
                },
                {
                    "role": "user",
                    "content": content_list
                }
            ]
        else:
            messages = [
                {
                    "role": "user",
                    "content": content_list
                }
            ]

        # 2. å‡†å¤‡æ¨ç†
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        image_inputs, video_inputs = process_vision_info(messages)
        
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)

        # 3. ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs, 
                max_new_tokens=128,
                temperature=0.1, # ä½æ¸©åº¦å¯¹äºæŒ‡ä»¤éµå¾ªå¾ˆé‡è¦
                top_p=0.9
            )

        # 4. è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        return output_text

    # --- äº‹ä»¶å¤„ç†å‡½æ•° ---

    def generate_answer(self, question, pil_frames_state):
        """
        [ä¿®æ”¹] æ‰‹åŠ¨æé—®ï¼šå¢åŠ äº†è‡ªåŠ¨æš‚åœæµå¼å¤„ç†çš„é€»è¾‘
        """
        if pil_frames_state is None or len(pil_frames_state) == 0:
             yield self.history_synchronizer.get_chat_history() + [gr.ChatMessage(role="assistant", content="è¯·å…ˆæ’­æ”¾è§†é¢‘")]
             return 

        # [æ–°å¢ 3] è‡ªåŠ¨æš‚åœæœºåˆ¶
        # å¦‚æœå½“å‰æ­£åœ¨æµå¼æ’­æ”¾ä¸”æœªæš‚åœï¼Œåˆ™å¼ºåˆ¶æš‚åœï¼Œé¿å…æŠ¢å æ˜¾å­˜
        auto_paused = False
        if self.is_streaming and self.pause_event.is_set():
            print("âš ï¸ Detected streaming active. Auto-pausing for manual question...")
            self.pause_event.clear() # æš‚åœæµå¼å¾ªç¯
            auto_paused = True
            # ç»™ä¸€ç‚¹æ—¶é—´è®© start_chat å¾ªç¯å“åº”æš‚åœ
            time.sleep(0.1)

        self.history_synchronizer.update("user", question)
        
        # æç¤ºç”¨æˆ·å¦‚æœæœ‰è‡ªåŠ¨æš‚åœ
        status_msg = "Thinking..."
        if auto_paused:
            status_msg += " (Video Auto-Paused)"
            
        yield self.history_synchronizer.get_chat_history() + [gr.ChatMessage(role="assistant", content=status_msg)]

        # [æ˜¾å­˜ç›‘æ§] æé—®å‰
        self.log_vram("Manual-Pre")

        start_t = time.perf_counter()
        
        # ä¿®æ”¹ï¼šåªä¼ å…¥æœ€åä¸€å¸§ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
        context_frames = pil_frames_state[-1:] if len(pil_frames_state) >= 2 else pil_frames_state
        
        try:
            # è¿™é‡Œçš„ _run_qwen_inference å†…éƒ¨å·²ç»æœ‰é”äº†ï¼Œæ‰€ä»¥æ˜¯å®‰å…¨çš„
            response = self._run_qwen_inference(context_frames, question, use_system_prompt=False)
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            response = "Error: OOM (æ˜¾å­˜ä¸è¶³)ã€‚è¯·å°è¯•å‡å°‘è§†é¢‘å¸§æ•°ã€‚"
        
        cost = time.perf_counter() - start_t
        print(f"Manual Inference Latency: {cost:.4f}s")
        
        # [æ˜¾å­˜ç›‘æ§] æé—®å
        self.log_vram("Manual-Post")

        self.history_synchronizer.update("assistant", response)
        yield self.history_synchronizer.get_chat_history()

    def start_chat(self, video_path, frame_interval, current_history):
        """
        [ä¿®æ”¹] æ™ºèƒ½å¯åŠ¨ï¼šæ”¯æŒç¼“å­˜å¤ç”¨ï¼Œå¹¶ä¿®å¤äº†Stopåæ— æ³•Restartçš„Bug
        """
        if not video_path:
            raise gr.Error("Please upload a video file.")

        # 1. [å…³é”®ä¿®å¤] å¼ºåˆ¶é‡ç½®æš‚åœçŠ¶æ€ï¼Œé˜²æ­¢ Stop åçš„æ­»é”
        self.pause_event.set()
        self.stop_signal = False # é‡ç½®åœæ­¢æ ‡å¿—

        # 2. [ç¼“å­˜ä¼˜åŒ–] æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤ç”¨è§†é¢‘æ•°æ®
        if video_path == self.cached_video_path and self.cached_video_data is not None:
            print(f"âš¡ Cache hit! Reusing video data for: {video_path}")
            pil_frames_all, original_frames, timestamps = self.cached_video_data
        else:
            print(f"ğŸ”„ New video detected. Loading: {video_path}")
            # å¦‚æœæ˜¯æ–°è§†é¢‘ï¼Œæ¸…ç†æ—§ç¼“å­˜ä»¥é˜²æ˜¾å­˜æ³„éœ²
            self.cached_video_data = None 
            torch.cuda.empty_cache()
            
            pil_frames_all, original_frames, timestamps = self._load_video(
                video_path, fps=1 / frame_interval
            )
            # æ›´æ–°ç¼“å­˜
            self.cached_video_path = video_path
            self.cached_video_data = (pil_frames_all, original_frames, timestamps)

        self.history_synchronizer.reset()
        fps_display = 0.0
        
        print(f"ğŸš€ å¼€å§‹æ¨ç†ï¼Œæ€»å¸§æ•°: {len(pil_frames_all)}")
        self.log_vram("Start")

        self.is_streaming = True
        
        try:
            for idx, (pil_frame, original_frame, timestamp) in enumerate(
                zip(pil_frames_all, original_frames, timestamps)
            ):
                # [æ–°å¢] æ£€æŸ¥å¼ºåˆ¶åœæ­¢æ ‡å¿—
                if self.stop_signal:
                    print("ğŸ›‘ Inference loop stopped by user.")
                    break

                # æ£€æŸ¥æš‚åœçŠ¶æ€
                if not self.pause_event.is_set():
                    self.pause_event.wait()
                    # å”¤é†’åå†æ¬¡æ£€æŸ¥åœæ­¢æ ‡å¿—ï¼ˆé˜²æ­¢åœ¨æš‚åœæ—¶ç‚¹äº†åœæ­¢ï¼‰
                    if self.stop_signal: 
                        break
                
                inference_start = time.perf_counter()
                
                context_frames = [pil_frame] 
                prompt = "Report status."
                
                response = self._run_qwen_inference(context_frames, prompt)
                
                self.history_synchronizer.update("assistant", f"[{timestamp:.1f}s] {response}")

                inference_end = time.perf_counter()
                
                cost_time = inference_end - inference_start
                if cost_time > 0:
                    current_fps = 1.0 / cost_time
                    fps_display = 0.8 * fps_display + 0.2 * current_fps if idx > 0 else current_fps

                current_chat_history = self.history_synchronizer.get_chat_history()
                yield timestamp, original_frame, pil_frames_all[: idx + 1], current_chat_history, f"{fps_display:.2f}"
        
        except Exception as e:
            print(f"Runtime Error: {e}")
            raise e
        finally:
            self.is_streaming = False
            self.stop_signal = False # æ¢å¤æ ‡å¿—ä½
            self.log_vram("Finished")
            
        # å¦‚æœä¸æ˜¯è¢«å¼ºåˆ¶åœæ­¢çš„ï¼Œæ‰è¾“å‡ºæœ€åç»“æœ
        if not self.stop_signal:
            yield timestamps[-1], original_frames[-1], pil_frames_all, self.history_synchronizer.get_chat_history(), f"{fps_display:.2f}"

    def toggle_pause(self):
        if self.pause_event.is_set():
            self.pause_event.clear()
            return "Resume Video", self.history_synchronizer.get_chat_history()
        else:
            self.pause_event.set()
            return "Pause Video", self.history_synchronizer.get_chat_history()

    def stop_chat(self):
        """
        [ä¿®æ”¹] åœæ­¢é€»è¾‘ï¼šè®¾ç½®æ ‡å¿—ä½ï¼Œå¹¶ç¡®ä¿ä»æš‚åœä¸­å”¤é†’ä»¥ä¾¿é€€å‡º
        """
        print("ğŸ›‘ Stop command received.")
        self.stop_signal = True # 1. è®¾ç½®åœæ­¢æ ‡å¿—
        self.pause_event.set()  # 2. [å…³é”®] å¦‚æœå½“å‰å¤„äºæš‚åœç­‰å¾…ä¸­ï¼Œå¿…é¡»å”¤é†’å®ƒï¼Œå®ƒæ‰èƒ½æ£€æµ‹åˆ° stop_signal å¹¶ break
        
        # ç¨å¾®ç»™ä¸€ç‚¹æ—¶é—´è®©å¾ªç¯é€€å‡º
        time.sleep(0.1)
        self.is_streaming = False
        
        # è¿”å›é‡ç½® UI çš„å€¼
        return 0, None, None, [], "0.00"

    # --- UI æ„å»ºå‡½æ•° ---
    def create_interface(self):
        with gr.Blocks(title="Qwen2.5-VL Video Chat (No KV Cache)") as demo:
            pil_frames_state = gr.State()

            with gr.Row():
                with gr.Column(scale=3):
                    gr_frame_display = gr.Image(label="Current Frame", interactive=False, height=400)
                    with gr.Row():
                        gr_time_display = gr.Number(label="Video Time (s)", value=0)
                        gr_fps_display = gr.Textbox(label="Inference FPS", value="0.00")
                    with gr.Row():
                        gr_pause_button = gr.Button("Pause Video")
                        gr_stop_button = gr.Button("Stop Video", variant="stop")

                with gr.Column(scale=2):
                    gr_chat_interface = gr.Chatbot(label="Chat History", height=500)
                    gr_question_input = gr.Textbox(label="Manual Question (Auto-pauses video)")

            with gr.Row():
                with gr.Column():
                    gr_video_upload = gr.Video(label="1. Upload Video")
                with gr.Column():
                    gr_frame_interval = gr.Slider(minimum=0.1, maximum=5.0, step=0.1, value=1.0, interactive=True, label="2. Frame Interval")
                    gr_start_button = gr.Button("3. Start Online Inference", variant="primary")

            gr_question_input.submit(self.generate_answer, inputs=[gr_question_input, pil_frames_state], outputs=gr_chat_interface, queue=True)
            gr_start_button.click(self.start_chat, inputs=[gr_video_upload, gr_frame_interval, gr_chat_interface], outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display])
            gr_pause_button.click(self.toggle_pause, inputs=[], outputs=[gr_pause_button, gr_chat_interface])
            gr_stop_button.click(self.stop_chat, inputs=[], outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display])
        
        return demo

if __name__ == "__main__":
    web_ui = VideoChatWebUI()
    demo = web_ui.create_interface()
    print("Launching WebUI (No KV Cache)...")
    demo.launch(server_name="0.0.0.0", server_port=6006, share=False, debug=True)