# KV Cache æ·˜æ±°ç­–ç•¥å®éªŒéªŒè¯ Prompt

> **ç”¨é€”**: åœ¨ GPU æœºå™¨ (AutoDL, RTX 4090 24GB) çš„ Copilot Agent ä¸Šé€æ­¥æ‰§è¡Œ
> **å‰æ**: å·²éƒ¨ç½² `/root/autodl-tmp/diploma_project/` é¡¹ç›®ï¼Œæ¨¡å‹æƒé‡åœ¨ `/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct`
> **è§†é¢‘æ–‡ä»¶**: `/root/autodl-tmp/diploma_project/temporal_encoding/202208312002.mp4`

---

## æ€»ä½“è¯´æ˜

æœ¬æ–‡ä»¶åŒ…å« **3 ä¸ªå®éªŒ**ï¼Œåˆ†åˆ«éªŒè¯æœ¬æ¬¡ KV Cache æ·˜æ±°ç­–ç•¥å®ç°ä¸­çš„ 3 ä¸ªæ ¸å¿ƒè®¾è®¡ï¼š

| å®éªŒ | éªŒè¯ç›®æ ‡ | å¯¹åº”ä¿®æ­£ |
|------|---------|---------|
| **å®éªŒ A** | sink_size è‡ªåŠ¨æ£€æµ‹æ­£ç¡®æ€§ | ä¸ç¡¬ç¼–ç  128ï¼Œä»é¦– chunk å®é™…é•¿åº¦æ¨å¯¼ |
| **å®éªŒ B** | OOM-Free é•¿è§†é¢‘å¤„ç† | æ— æ·˜æ±° 120 å¸§å³ OOMï¼Œå¯ç”¨æ·˜æ±°ååº”æ— é™ |
| **å®éªŒ C** | æ·˜æ±°å ask() è´¨é‡ä¸é™çº§ | æ»‘çª— + å‘¨æœŸæ€§æé—®ï¼Œæå–æ­Œè¯/å­—å¹• |

### å…³é”®æ•°æ®å‚è€ƒ (test_step10 å®æµ‹, RTX 4090 24GB)

| å‚æ•° | å€¼ |
|------|-----|
| æ¨¡å‹ VRAM (Qwen2.5-VL-3B, bf16) | 7.1 GB allocated, 7.33 GB reserved |
| KV cache æ¯ token | ~36 KB (across 36 layers) |
| 1920Ã—1080, 4å¸§/chunk | ~5,389 tokens/chunk (~0.185 GB/chunk) |
| é¦– chunk (å« system prompt) | ~5,438 tokens (å¤š ~49 ä¸ªæ–‡æœ¬ token) |
| 30 chunks (120å¸§) æ— æ·˜æ±° | cache 161,719 tokens, VRAM reserved 22.89 GB |
| 40 chunks (160å¸§) æ— æ·˜æ±° | **OOM** |
| å®‰å…¨ max_cache_tokens | ~100,000 tokens (~3.4 GB cache) |

### æ‰§è¡ŒæŒ‡å¼•

- **é€ä¸ªå®éªŒæ‰§è¡Œ**: A â†’ B â†’ Cï¼Œæ¯ä¸ªå®éªŒç‹¬ç«‹
- **åå¤è¿­ä»£**: å¦‚æœå®éªŒå¤±è´¥æˆ–ç»“æœä¸ç¬¦åˆé¢„æœŸï¼Œè¯·é˜…è¯»æŠ¥å‘Šè¾“å‡ºã€åˆ†æåŸå› ã€ä¿®å¤ä»£ç ï¼Œç„¶åé‡æ–°è¿è¡Œï¼Œç›´è‡³é€šè¿‡
- **æ¯ä¸ªå®éªŒéƒ½æœ‰"é€šè¿‡æ ‡å‡†"**: è§å„å®éªŒæœ«å°¾çš„ âœ… åˆ¤å®šæ¡ä»¶
- **æŠ¥å‘Šæ–‡ä»¶**: æ¯ä¸ªå®éªŒä¼šè‡ªåŠ¨ç”Ÿæˆ `_report.txt`ï¼ŒåŠ¡å¿…æŸ¥çœ‹å®Œæ•´å†…å®¹

---

## å®éªŒ Aï¼šsink_size è‡ªåŠ¨æ£€æµ‹éªŒè¯

### ç›®æ ‡

éªŒè¯ `EvictionConfig(sink_size=0)` çš„è‡ªåŠ¨æ£€æµ‹æœºåˆ¶ï¼š
1. é¦– chunk è¿½åŠ åï¼Œ`effective_sink_size` ç­‰äºå®é™… cache é•¿åº¦
2. ä¸åŒåˆ†è¾¨ç‡/å¸§æ•°ç»„åˆä¸‹ï¼Œsink å€¼ä¸åŒä¸”åˆç†
3. åç»­ chunk çš„ `update_chunk_stats()` æ­£ç¡®è®°å½•å¹³å‡ token æ•°

### åŸç†

sink_size ä¸èƒ½ç¡¬ç¼–ç ï¼ˆå¦‚æ—§ç‰ˆçš„ 128ï¼‰ï¼Œå› ä¸ºï¼š
- é¦– chunk åŒ…å« system prompt (~49 text tokens) + é¦–å¸§è§†è§‰ token
- 1920Ã—1080 4å¸§/chunk â‰ˆ 5,438 tokens; 2å¸§/chunk â‰ˆ 2,750; 640Ã—480 ä¼šæ›´å°‘
- 128 è¿œå°äºä»»ä½•åˆç†çš„é¦– chunk å¤§å°ï¼Œä¼šé”™è¯¯åœ°æ·˜æ±°é¦–å¸§ä¸­çš„å¤§éƒ¨åˆ†è§†è§‰ token

### æ­¥éª¤

è¯·åœ¨ `/root/autodl-tmp/diploma_project/temporal_encoding/` ç›®å½•ä¸‹åˆ›å»º `test_eviction_exp_a.py`:

```python
"""
å®éªŒ A: sink_size è‡ªåŠ¨æ£€æµ‹éªŒè¯

éªŒè¯:
  1) é¦– chunk å effective_sink_size = å®é™… cache é•¿åº¦
  2) ä¸åŒ chunk å¸§æ•°ä¸‹ sink å€¼å˜åŒ–åˆç†
  3) update_chunk_stats() æ­£ç¡®è®°å½•åç»­ chunk å¹³å‡ token æ•°
  4) window_size è‡ªåŠ¨è®¡ç®— = max_cache_tokens - sink_size
"""
import os
import sys
import gc
import time
import torch
from datetime import datetime
from PIL import Image

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from model import StreamQwenModel, VideoStreamingInference, EvictionConfig

MODEL_PATH = os.environ.get(
    "QWEN_MODEL_PATH",
    "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct",
)
REPORT_PATH = os.environ.get(
    "REPORT_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/test_eviction_exp_a_report.txt",
)

# æµ‹è¯•ä¸åŒçš„ chunk å¸§æ•°
CHUNK_FRAME_CONFIGS = [2, 4]
# è¿½åŠ  chunk æ•° (è¶³å¤ŸéªŒè¯è‡ªåŠ¨æ£€æµ‹, ä¸éœ€è¦å¤ªå¤š)
NUM_CHUNKS = 5
# å®‰å…¨çš„ max_cache_tokens (è¶³å¤Ÿå¤§, æœ¬å®éªŒä¸è§¦å‘æ·˜æ±°)
MAX_CACHE_TOKENS = 100_000


class TeeWriter:
    def __init__(self, *writers):
        self._writers = writers
    def write(self, text):
        for w in self._writers:
            w.write(text)
        self.flush()
    def flush(self):
        for w in self._writers:
            w.flush()


def get_vram_gb():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        return {
            "allocated": round(torch.cuda.memory_allocated() / (1024 ** 3), 3),
            "reserved": round(torch.cuda.memory_reserved() / (1024 ** 3), 3),
        }
    return {}


def create_test_frames(n_frames, width=1920, height=1080):
    """åˆ›å»ºæµ‹è¯•å¸§ (çº¯è‰²æ¸å˜, æ¨¡æ‹ŸçœŸå®åˆ†è¾¨ç‡)ã€‚"""
    frames = []
    for i in range(n_frames):
        # ä¸åŒå¸§ç”¨ä¸åŒé¢œè‰², ä¾¿äºåŒºåˆ†
        r = int(255 * i / max(n_frames - 1, 1))
        img = Image.new("RGB", (width, height), (r, 128, 255 - r))
        frames.append(img)
    return frames


def test_sink_detection(model, processor, device, chunk_frames, report_lines):
    """æµ‹è¯•æŒ‡å®š chunk_frames ä¸‹çš„ sink è‡ªåŠ¨æ£€æµ‹ã€‚"""
    report_lines.append(f"\n{'='*60}")
    report_lines.append(f"Testing: chunk_frames={chunk_frames}, 1920Ã—1080")
    report_lines.append(f"{'='*60}")

    config = EvictionConfig(
        max_cache_tokens=MAX_CACHE_TOKENS,
        sink_size=0,     # è‡ªåŠ¨æ£€æµ‹
        window_size=0,   # è‡ªåŠ¨è®¡ç®—
    )
    engine = VideoStreamingInference(
        model, processor, device, eviction_config=config
    )

    evictor = engine.cache_manager.evictor

    # éªŒè¯åˆå§‹çŠ¶æ€: sink æœªæ£€æµ‹
    assert not evictor._first_chunk_recorded, "é¦– chunk å‰ä¸åº”å·²è®°å½•"
    report_lines.append(f"  [Before] first_chunk_recorded = False âœ…")

    cache_lens = []
    for i in range(NUM_CHUNKS):
        frames = create_test_frames(chunk_frames, 1920, 1080)
        result = engine.append_video_chunk(frames, fps=2.0)
        cache_len = engine.cache_manager.get_seq_length()
        cache_lens.append(cache_len)

        if i == 0:
            # é¦– chunk åéªŒè¯
            assert evictor._first_chunk_recorded, "é¦– chunk ååº”å·²è®°å½•"
            sink = evictor.effective_sink_size
            window = evictor.effective_window_size
            report_lines.append(f"  [Chunk 0] cache_len = {cache_len}")
            report_lines.append(f"  [Chunk 0] effective_sink_size = {sink}")
            report_lines.append(f"  [Chunk 0] effective_window_size = {window}")
            report_lines.append(f"  [Chunk 0] sink + window = {sink + window} (should â‰¤ {MAX_CACHE_TOKENS})")

            # æ ¸å¿ƒæ–­è¨€: sink = é¦– chunk cache é•¿åº¦
            assert sink == cache_len, f"sink ({sink}) != cache_len ({cache_len})"
            report_lines.append(f"  [Chunk 0] âœ… sink == cache_len")

            # window è‡ªåŠ¨è®¡ç®—
            assert window == MAX_CACHE_TOKENS - sink, \
                f"window ({window}) != max - sink ({MAX_CACHE_TOKENS - sink})"
            report_lines.append(f"  [Chunk 0] âœ… window == max_cache_tokens - sink")
        else:
            # åç»­ chunk: éªŒè¯ chunk ç»Ÿè®¡
            avg = evictor._avg_chunk_tokens
            report_lines.append(
                f"  [Chunk {i}] cache_len = {cache_len}, "
                f"avg_chunk_tokens = {avg:.0f}"
            )

    # è®¡ç®—å®é™…æ¯ chunk token æ•° (éé¦– chunk)
    per_chunk = []
    for j in range(1, len(cache_lens)):
        per_chunk.append(cache_lens[j] - cache_lens[j - 1])

    if per_chunk:
        actual_avg = sum(per_chunk) / len(per_chunk)
        recorded_avg = evictor._avg_chunk_tokens
        report_lines.append(f"  Actual per-chunk tokens: {per_chunk}")
        report_lines.append(f"  Actual average: {actual_avg:.0f}")
        report_lines.append(f"  Recorded average: {recorded_avg:.0f}")
        # å…è®¸å°è¯¯å·® (æµ®ç‚¹è¿è¡Œå¹³å‡)
        assert abs(recorded_avg - actual_avg) < 10, \
            f"avg mismatch: recorded={recorded_avg:.0f} vs actual={actual_avg:.0f}"
        report_lines.append(f"  âœ… Average chunk tokens match")

    # æ¸…ç†
    del engine
    gc.collect()
    torch.cuda.empty_cache()

    return cache_lens[0]  # è¿”å›é¦– chunk çš„ sink å€¼


def main():
    report_dir = os.path.dirname(REPORT_PATH)
    if report_dir:
        os.makedirs(report_dir, exist_ok=True)

    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        tee = TeeWriter(sys.stdout, f)
        original_stdout, original_stderr = sys.stdout, sys.stderr
        sys.stdout = tee
        sys.stderr = tee

        try:
            print("=" * 70)
            print("EXPERIMENT A: sink_size Auto-Detection Verification")
            print("=" * 70)
            print(f"Report time: {datetime.now().isoformat(timespec='seconds')}")
            print(f"max_cache_tokens = {MAX_CACHE_TOKENS}")
            print(f"Chunk frame configs to test: {CHUNK_FRAME_CONFIGS}")
            print()

            # åŠ è½½æ¨¡å‹
            print("[1] Loading model...")
            from transformers import AutoProcessor
            device = "cuda"
            dtype = torch.bfloat16
            processor = AutoProcessor.from_pretrained(MODEL_PATH)
            model = StreamQwenModel.from_pretrained(
                MODEL_PATH, torch_dtype=dtype
            ).to(device)
            model.eval()
            print(f"  VRAM after load: {get_vram_gb()}")
            print()

            # å¯¹æ¯ç§ chunk å¸§æ•°æµ‹è¯•
            results = {}
            report_lines = []
            for cf in CHUNK_FRAME_CONFIGS:
                sink_val = test_sink_detection(
                    model, processor, device, cf, report_lines
                )
                results[cf] = sink_val

            # æ‰“å°æ”¶é›†çš„æŠ¥å‘Š
            for line in report_lines:
                print(line)

            # æ€»ç»“
            print()
            print("=" * 70)
            print("SUMMARY")
            print("=" * 70)
            for cf, sink in results.items():
                print(f"  chunk_frames={cf}: sink_size = {sink} tokens")

            # éªŒè¯: ä¸åŒå¸§æ•° â†’ ä¸åŒ sink
            sinks = list(results.values())
            if len(set(sinks)) == len(sinks):
                print(f"  âœ… ä¸åŒ chunk_frames äº§ç”Ÿä¸åŒ sink_size")
            else:
                print(f"  âš ï¸ éƒ¨åˆ† chunk_frames äº§ç”Ÿç›¸åŒ sink_size (å¯èƒ½å¸§æ•°å·®å¼‚ä¸å¤Ÿå¤§)")

            # éªŒè¯: sink è¿œå¤§äºæ—§ç‰ˆç¡¬ç¼–ç çš„ 128
            for cf, sink in results.items():
                if sink > 128:
                    print(f"  âœ… chunk_frames={cf}: sink={sink} >> 128 (æ—§ç‰ˆç¡¬ç¼–ç å€¼)")
                else:
                    print(f"  âŒ chunk_frames={cf}: sink={sink} â‰¤ 128, ä¸åˆç†!")

            print()
            print("âœ… EXPERIMENT A COMPLETE")

        except Exception as e:
            print(f"\nâŒ EXPERIMENT A FAILED: {e}")
            import traceback
            traceback.print_exc()

        finally:
            sys.stdout = original_stdout
            sys.stderr = original_stderr

    print(f"\nReport saved to: {REPORT_PATH}")


if __name__ == "__main__":
    main()
```

### è¿è¡Œå‘½ä»¤

```bash
cd /root/autodl-tmp/diploma_project/temporal_encoding
python test_eviction_exp_a.py
```

### âœ… é€šè¿‡æ ‡å‡†

1. `effective_sink_size == é¦– chunk cache_len` (é¦– chunk åè‡ªåŠ¨æ£€æµ‹å‡†ç¡®)
2. `effective_window_size == max_cache_tokens - sink_size` (è‡ªåŠ¨è®¡ç®—æ­£ç¡®)
3. `avg_chunk_tokens` ä¸å®é™…å¢é‡å»åˆ (è¯¯å·® < 10)
4. `chunk_frames=2` å’Œ `chunk_frames=4` äº§ç”Ÿä¸åŒçš„ sink_size
5. æ‰€æœ‰ sink_size è¿œå¤§äº 128 (æ—§ç‰ˆç¡¬ç¼–ç å€¼)

### âŒ å¦‚æœå¤±è´¥

- **`sink != cache_len`**: æ£€æŸ¥ `video_stream_inference.py` ä¸­ `set_first_chunk_info()` çš„è°ƒç”¨æ—¶æœºæ˜¯å¦åœ¨ forward ä¹‹å
- **`avg ä¸åŒ¹é…`**: æ£€æŸ¥ `update_chunk_stats()` æ˜¯å¦æ­£ç¡®è®¡ç®—äº† `cache_len_after - _prev_cache_len`
- **`sink â‰¤ 128`**: æµ‹è¯•å¸§å¯èƒ½åˆ†è¾¨ç‡è¿‡ä½, æˆ– ViT ç¼–ç å¼‚å¸¸ â€” æ£€æŸ¥ ViT è¾“å‡º token æ•°
- **ä¿®å¤åé‡æ–°è¿è¡Œ**, ç›´è‡³æ‰€æœ‰æ–­è¨€é€šè¿‡

---

## å®éªŒ Bï¼šOOM-Free é•¿è§†é¢‘å¤„ç†

### ç›®æ ‡

ç”¨ Level 1 æ·˜æ±°ç­–ç•¥å¤„ç†å®Œæ•´ `1.mp4` è§†é¢‘ï¼ŒéªŒè¯ï¼š
1. æ˜¾å­˜ä¸æŒç»­å¢é•¿, ä¸ OOM
2. `cache_len` åœ¨è¾¾åˆ° `max_cache_tokens` åä¿æŒç¨³å®š
3. æ·˜æ±°ç»Ÿè®¡æ•°æ®æ­£ç¡® (æ€»æ·˜æ±°æ¬¡æ•°ã€token æ•°)
4. æœ€ç»ˆ `ask()` ä»å¯æ­£å¸¸å›ç­”

### èƒŒæ™¯

test_step10 å®æµ‹è¡¨æ˜: **æ— æ·˜æ±°æ—¶ 1920Ã—1080 æœ€å¤š ~120 å¸§ (30 chunks) å³è¾¾åˆ° 22.89 GB reserved, 40 chunks OOM**ã€‚
1.mp4 æ—¶é•¿çº¦ 200s, ä»¥ fps=2 é‡‡æ · â†’ ~400 å¸§ â†’ ~100 chunks (4å¸§/chunk) æˆ– ~200 chunks (2å¸§/chunk)ã€‚
æ— æ·˜æ±°ç»ä¸å¯èƒ½å¤„ç†å®Œã€‚

### æ­¥éª¤

è¯·åœ¨ `/root/autodl-tmp/diploma_project/temporal_encoding/` ç›®å½•ä¸‹åˆ›å»º `test_eviction_exp_b.py`:

```python
"""
å®éªŒ B: OOM-Free é•¿è§†é¢‘å¤„ç†

éªŒè¯:
  1) å¯ç”¨ Level 1 KV Cache æ·˜æ±° (Sink + Window, å…¨è‡ªåŠ¨å‚æ•°)
  2) ä»¥ 4 å¸§/chunkã€fps=2 çš„æ–¹å¼é€æ®µç¼–ç æ•´ä¸ª 1.mp4
  3) æ˜¾å­˜ä¿æŒç¨³å®šï¼Œä¸ OOM
  4) cache_len åœ¨è§¦å‘æ·˜æ±°åä¿æŒ â‰¤ max_cache_tokens
  5) æœ€åæä¸€ä¸ªé—®é¢˜éªŒè¯ cache å¯ç”¨æ€§
"""
import os
import sys
import gc
import time
import torch
from datetime import datetime
from PIL import Image

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from model import StreamQwenModel, VideoStreamingInference, EvictionConfig

MODEL_PATH = os.environ.get(
    "QWEN_MODEL_PATH",
    "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct",
)
VIDEO_PATH = os.environ.get(
    "VIDEO_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/1.mp4",
)
REPORT_PATH = os.environ.get(
    "REPORT_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/test_eviction_exp_b_report.txt",
)

# â”€â”€ æ·˜æ±°å‚æ•° (å…¨è‡ªåŠ¨) â”€â”€
# â¬‡ï¸ è¿™æ˜¯éœ€è¦å®éªŒè°ƒä¼˜çš„æ ¸å¿ƒè¶…å‚æ•°ã€‚
# 100K=ä¿å®ˆ(3.4GB cache), 130K=ä¸­ç­‰(4.5GB), 150K=æ¿€è¿›(5.2GB, æ¥è¿‘æé™)
# å³°å€¼ cache = max + 1 chunk (~5.4K), ä¸å¯è¶… ~155K (4090 24GB)
# è¿‡å°â†’windowä¸è¶³â†’è¿‘æœŸä¿¡æ¯ä¸¢å¤±â†’å›ç­”è´¨é‡ä¸‹é™; è¿‡å¤§â†’OOM
# å»ºè®®ä» 130K å¼€å§‹, è‹¥ç¨³å®šåˆ™å°è¯• 150K
MAX_CACHE_TOKENS = 130_000  # ä¸­ç­‰é…ç½®, ~4.5 GB cache, total ~11.6 GB

# â”€â”€ ç¼–ç å‚æ•° â”€â”€
CHUNK_FRAMES = 4      # æ¯æ¬¡è¿½åŠ  4 å¸§ (ä¸ test_step10 ä¸€è‡´)
SAMPLE_FPS = 2.0      # é‡‡æ ·å¸§ç‡
PRINT_INTERVAL = 10   # æ¯ 10 ä¸ª chunk æ‰“å°ä¸€æ¬¡


class TeeWriter:
    def __init__(self, *writers):
        self._writers = writers
    def write(self, text):
        for w in self._writers:
            w.write(text)
        self.flush()
    def flush(self):
        for w in self._writers:
            w.flush()


def get_vram_gb():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        return {
            "allocated": round(torch.cuda.memory_allocated() / (1024 ** 3), 3),
            "reserved": round(torch.cuda.memory_reserved() / (1024 ** 3), 3),
            "max_allocated": round(torch.cuda.max_memory_allocated() / (1024 ** 3), 3),
        }
    return {}


def extract_frames_from_video(video_path, fps=2.0):
    """ä»è§†é¢‘æ–‡ä»¶ä¸­æŒ‰æŒ‡å®š fps é‡‡æ ·å¸§ã€‚è¿”å› PIL Image åˆ—è¡¨ã€‚"""
    try:
        import decord
        from decord import VideoReader, cpu
        decord.bridge.set_bridge("native")
        vr = VideoReader(video_path, ctx=cpu(0))
        video_fps = vr.get_avg_fps()
        total_frames = len(vr)
        duration = total_frames / video_fps
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        print(f"  Video: {video_path}")
        print(f"  Duration: {duration:.1f}s, FPS: {video_fps:.1f}, Total frames: {total_frames}")
        print(f"  Sampling at {fps} fps â†’ {len(indices)} frames")
        frames = []
        for idx in indices:
            frame = vr[idx].asnumpy()
            frames.append(Image.fromarray(frame))
        return frames, duration
    except ImportError:
        import cv2
        cap = cv2.VideoCapture(video_path)
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / video_fps if video_fps > 0 else 0
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        print(f"  Video: {video_path}")
        print(f"  Duration: {duration:.1f}s, FPS: {video_fps:.1f}, Total frames: {total_frames}")
        print(f"  Sampling at {fps} fps â†’ {len(indices)} frames")
        frames = []
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
        cap.release()
        return frames, duration


def main():
    report_dir = os.path.dirname(REPORT_PATH)
    if report_dir:
        os.makedirs(report_dir, exist_ok=True)

    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        tee = TeeWriter(sys.stdout, f)
        original_stdout, original_stderr = sys.stdout, sys.stderr
        sys.stdout = tee
        sys.stderr = tee

        try:
            print("=" * 70)
            print("EXPERIMENT B: OOM-Free Long Video Processing with KV Cache Eviction")
            print("=" * 70)
            print(f"Report time: {datetime.now().isoformat(timespec='seconds')}")
            print(f"Eviction config: max_cache_tokens={MAX_CACHE_TOKENS}, "
                  f"sink=auto, window=auto")
            print(f"Expected: test_step10 shows OOM at 40 chunks (160 frames) without eviction.")
            print(f"With eviction, should process ALL chunks without OOM.")
            print()

            # â”€â”€ 0) æ£€æŸ¥æ–‡ä»¶ â”€â”€
            if not os.path.exists(MODEL_PATH):
                print(f"âŒ Model not found: {MODEL_PATH}")
                return
            if not os.path.exists(VIDEO_PATH):
                print(f"âŒ Video not found: {VIDEO_PATH}")
                return

            # â”€â”€ 1) åŠ è½½æ¨¡å‹ â”€â”€
            print("[1] Loading model...")
            from transformers import AutoProcessor
            device = "cuda"
            dtype = torch.bfloat16
            processor = AutoProcessor.from_pretrained(MODEL_PATH)
            model = StreamQwenModel.from_pretrained(
                MODEL_PATH, torch_dtype=dtype
            ).to(device)
            model.eval()
            vram_model = get_vram_gb()
            print(f"  VRAM after model load: {vram_model}")
            print()

            # â”€â”€ 2) æå–è§†é¢‘å¸§ â”€â”€
            print("[2] Extracting frames from video...")
            all_frames, duration = extract_frames_from_video(VIDEO_PATH, fps=SAMPLE_FPS)
            total_frame_count = len(all_frames)
            expected_chunks = (total_frame_count + CHUNK_FRAMES - 1) // CHUNK_FRAMES
            print(f"  Total frames extracted: {total_frame_count}")
            print(f"  Expected chunks (4 frames/chunk): {expected_chunks}")
            print(f"  âš ï¸ Without eviction, OOM at ~40 chunks ({40*CHUNK_FRAMES} frames).")
            print(f"  With eviction (max={MAX_CACHE_TOKENS}), should handle all {expected_chunks} chunks.")
            print()

            # â”€â”€ 3) åˆ›å»ºå¼•æ“ (å¯ç”¨ Level 1 æ·˜æ±°, å…¨è‡ªåŠ¨å‚æ•°) â”€â”€
            print("[3] Creating streaming inference engine with eviction...")
            eviction_config = EvictionConfig(
                max_cache_tokens=MAX_CACHE_TOKENS,
                # sink_size=0  â†’ è‡ªåŠ¨æ£€æµ‹é¦– chunk
                # window_size=0 â†’ è‡ªåŠ¨è®¡ç®—
            )
            engine = VideoStreamingInference(
                model, processor, device, eviction_config=eviction_config
            )
            print()

            # â”€â”€ 4) é€ chunk ç¼–ç  â”€â”€
            print("[4] Encoding video chunks...")
            t_start = time.time()
            vram_history = []
            cache_history = []
            chunk_count = 0
            first_eviction_chunk = None

            for i in range(0, total_frame_count, CHUNK_FRAMES):
                chunk = all_frames[i : i + CHUNK_FRAMES]
                if len(chunk) == 0:
                    continue
                # è¡¥é½å¶æ•°å¸§ (temporal_patch_size=2)
                if len(chunk) % 2 != 0:
                    chunk.append(chunk[-1])

                result = engine.append_video_chunk(chunk, fps=SAMPLE_FPS)
                chunk_count += 1

                info = engine.get_cache_info()
                cache_len = info["cache_seq_length"]

                # è®°å½•é¦–æ¬¡æ·˜æ±°
                if "eviction_stats" in info:
                    es = info["eviction_stats"]
                    if es.get("total_evictions", 0) > 0 and first_eviction_chunk is None:
                        first_eviction_chunk = chunk_count

                if chunk_count % PRINT_INTERVAL == 0 or chunk_count == 1:
                    vram = get_vram_gb()
                    vram_history.append({
                        "chunk": chunk_count,
                        "cache_len": cache_len,
                        "vram_alloc": vram.get("allocated", 0),
                        "vram_reserved": vram.get("reserved", 0),
                    })
                    cache_history.append(cache_len)

                    eviction_str = ""
                    if "eviction_stats" in info:
                        es = info["eviction_stats"]
                        eviction_str = (
                            f", evictions={es.get('total_evictions', 0)}, "
                            f"evicted={es.get('total_tokens_evicted', 0)}"
                        )

                    print(
                        f"  Chunk {chunk_count:>4d}/{expected_chunks}: "
                        f"cache_len={cache_len:>6d}, "
                        f"mem={info.get('cache_memory_gb', 0):.3f} GB, "
                        f"VRAM={vram.get('allocated', 0):.2f}/{vram.get('reserved', 0):.2f} GB"
                        f"{eviction_str}"
                    )

            t_encode = time.time() - t_start

            # æ±‡æ€»
            print(f"\n  âœ… Encoding completed: {chunk_count} chunks, "
                  f"{total_frame_count} frames in {t_encode:.1f}s")
            final_vram = get_vram_gb()
            print(f"  Final VRAM: {final_vram}")
            if first_eviction_chunk:
                print(f"  First eviction at chunk: {first_eviction_chunk}")

            # è·å– evictor çŠ¶æ€
            evictor = engine.cache_manager.evictor
            if evictor:
                print(f"  Effective sink_size: {evictor.effective_sink_size}")
                print(f"  Effective window_size: {evictor.effective_window_size}")
                print(f"  Avg chunk tokens: {evictor._avg_chunk_tokens:.0f}")
            print()

            # â”€â”€ 5) éªŒè¯ ask ä»å¯ç”¨ â”€â”€
            print("[5] Verification: asking a question...")
            final_info = engine.get_cache_info()
            print(f"  Pre-ask cache: len={final_info['cache_seq_length']}, "
                  f"mem={final_info.get('cache_memory_gb', 0):.3f} GB")

            answer, metrics = engine.ask(
                "Briefly describe what you saw in the entire video.",
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
            )
            print(f"  Answer: {answer}")
            print(f"  TTFT: {metrics['ttft']:.3f}s")

            # éªŒè¯ ask å cache æ¢å¤ (snapshot/restore)
            post_ask_info = engine.get_cache_info()
            print(f"  Post-ask cache: len={post_ask_info['cache_seq_length']}")
            assert post_ask_info['cache_seq_length'] == final_info['cache_seq_length'], \
                "ask() å cache é•¿åº¦åº”æ¢å¤ (snapshot/restore)"
            print(f"  âœ… Cache restored after ask()")
            print()

            # â”€â”€ 6) æ€»ç»“ â”€â”€
            print("=" * 70)
            print("SUMMARY")
            print("=" * 70)
            print(f"  Video: {VIDEO_PATH} ({duration:.0f}s)")
            print(f"  Total frames: {total_frame_count}")
            print(f"  Total chunks: {chunk_count}")
            print(f"  Encoding time: {t_encode:.1f}s "
                  f"({total_frame_count / t_encode:.2f} frames/sec)")
            print(f"  Max cache tokens: {MAX_CACHE_TOKENS}")
            print(f"  Final cache_len: {final_info['cache_seq_length']}")
            print(f"  Final VRAM: allocated={final_vram.get('allocated', 0):.2f} GB, "
                  f"reserved={final_vram.get('reserved', 0):.2f} GB, "
                  f"max_allocated={final_vram.get('max_allocated', 0):.2f} GB")

            if "eviction_stats" in final_info:
                es = final_info["eviction_stats"]
                print(f"  Total evictions: {es.get('total_evictions', 0)}")
                print(f"  Total tokens evicted: {es.get('total_tokens_evicted', 0)}")

            # é€šè¿‡åˆ¤å®š
            print()
            print("â”€" * 70)
            print("PASS/FAIL CRITERIA:")
            all_pass = True

            # 1) æ²¡ OOM (èµ°åˆ°è¿™é‡Œè¯´æ˜æ²¡ OOM)
            print(f"  âœ… [P1] No OOM â€” processed all {chunk_count} chunks "
                  f"(test_step10 OOM at 40 chunks without eviction)")

            # 2) cache_len â‰¤ max_cache_tokens
            if final_info['cache_seq_length'] <= MAX_CACHE_TOKENS:
                print(f"  âœ… [P2] cache_len ({final_info['cache_seq_length']}) "
                      f"â‰¤ max ({MAX_CACHE_TOKENS})")
            else:
                print(f"  âŒ [P2] cache_len ({final_info['cache_seq_length']}) "
                      f"> max ({MAX_CACHE_TOKENS})")
                all_pass = False

            # 3) æœ‰æ·˜æ±°å‘ç”Ÿ
            if "eviction_stats" in final_info:
                es = final_info["eviction_stats"]
                if es.get("total_evictions", 0) > 0:
                    print(f"  âœ… [P3] Eviction occurred "
                          f"({es['total_evictions']} times, "
                          f"{es['total_tokens_evicted']} tokens)")
                else:
                    print(f"  âŒ [P3] No eviction occurred â€” config may not have been applied")
                    all_pass = False

            # 4) VRAM æœªè¶… 23 GB
            max_alloc = final_vram.get("max_allocated", 0)
            if max_alloc < 23.0:
                print(f"  âœ… [P4] Max VRAM allocated ({max_alloc:.2f} GB) < 23 GB")
            else:
                print(f"  âš ï¸ [P4] Max VRAM allocated ({max_alloc:.2f} GB) â‰¥ 23 GB")
                all_pass = False

            # 5) ask æ­£å¸¸
            if answer and len(answer) > 5:
                print(f"  âœ… [P5] ask() returned valid answer ({len(answer)} chars)")
            else:
                print(f"  âŒ [P5] ask() returned empty/short answer")
                all_pass = False

            print()
            if all_pass:
                print("ğŸ‰ EXPERIMENT B: ALL PASSED")
            else:
                print("âš ï¸ EXPERIMENT B: SOME CHECKS FAILED â€” see above")

        except torch.cuda.OutOfMemoryError:
            print(f"\nâŒ EXPERIMENT B FAILED: CUDA OOM!")
            print(f"  This means eviction did not prevent OOM.")
            print(f"  Possible causes:")
            print(f"    1) Eviction not triggered â€” check EvictionConfig")
            print(f"    2) max_cache_tokens too large â€” try 50,000")
            print(f"    3) torch reserved memory fragmentation")
            import traceback
            traceback.print_exc()

        except Exception as e:
            print(f"\nâŒ EXPERIMENT B FAILED: {e}")
            import traceback
            traceback.print_exc()

        finally:
            sys.stdout = original_stdout
            sys.stderr = original_stderr

    print(f"\nReport saved to: {REPORT_PATH}")


if __name__ == "__main__":
    main()
```

### è¿è¡Œå‘½ä»¤

```bash
cd /root/autodl-tmp/diploma_project/temporal_encoding
python test_eviction_exp_b.py
```

### âœ… é€šè¿‡æ ‡å‡†

1. **[P1]** å¤„ç†å…¨éƒ¨ chunk, ä¸ OOM (æ— æ·˜æ±°æ—¶ 40 chunks å°± OOM)
2. **[P2]** æœ€ç»ˆ `cache_len â‰¤ max_cache_tokens`
3. **[P3]** æ·˜æ±°æ¬¡æ•° > 0 ä¸”æ·˜æ±° token æ•° > 0
4. **[P4]** VRAM max_allocated < 23 GB
5. **[P5]** `ask()` è¿”å›æœ‰æ•ˆå›ç­”

### âŒ å¦‚æœå¤±è´¥

- **CUDA OOM**: é™ä½ `MAX_CACHE_TOKENS` (å¦‚ 100,000 æˆ– 50,000), æˆ–é™ä½ `SAMPLE_FPS` (å¦‚ 1.0)ã€‚ä¹Ÿå¯èƒ½æ˜¯ `torch.cuda.memory_reserved` ç¢ç‰‡åŒ– â€” å°è¯•æ¯ N chunk è°ƒç”¨ `torch.cuda.empty_cache()`
- **cache_len ä¸ä¸‹é™**: æ·˜æ±°æœªè§¦å‘ â€” æ£€æŸ¥ `eviction_interval`ã€`should_evict()` é€»è¾‘ã€`video_stream_inference.py` ä¸­çš„ `_chunk_counter`
- **ask() å¤±è´¥**: snapshot/restore ä¸æ·˜æ±°ä¸å…¼å®¹ â€” æ£€æŸ¥ `cache_manager.py` çš„ snapshot æ˜¯å¦ä¿å­˜äº† tracker
- **ä¿®å¤åé‡æ–°è¿è¡Œ**, ç›´è‡³æ‰€æœ‰ P1-P5 é€šè¿‡

### ğŸ”§ max_cache_tokens è°ƒä¼˜æµç¨‹ (å®éªŒ B é€šè¿‡åæ‰§è¡Œ)

å®éªŒ B çš„å®Œæ•´ç›®æ ‡ä¸ä»…æ˜¯â€œä¸ OOMâ€ï¼Œè¿˜è¦æ‰¾åˆ° **å……åˆ†åˆ©ç”¨ 24GB æ˜¾å­˜çš„æœ€ä¼˜ max_cache_tokens**ã€‚

window = max_cache_tokens - sink (â‰ˆ 5.4K)ï¼Œæ‰€ä»¥ max è¶Šå¤§ â†’ window è¶Šå¤§ â†’ ä¿ç•™æ›´å¤šè¿‘æœŸè§†é¢‘å¸§ â†’ å›ç­”è´¨é‡æ›´å¥½ã€‚
ä½† max è¿‡å¤§ â†’ å³°å€¼ cache (max + 1 chunk) è¶…è¿‡ CUDA å®‰å…¨çº¿ â†’ OOMã€‚

**æ­¥éª¤:**

1. å…ˆç”¨ `MAX_CACHE_TOKENS = 130_000` è·‘å®Œå®éªŒ B, è®°å½• VRAM max_reserved
2. è‹¥ max_reserved < 21 GB: æé«˜åˆ° `150_000` é‡è·‘
3. è‹¥ max_reserved 21~23 GB: å½“å‰å€¼å³ä¸ºæœ€ä¼˜
4. è‹¥ OOM: é™ä½åˆ° `100_000` é‡è·‘
5. ç”¨æœ€ç»ˆç¡®å®šçš„å€¼æ›´æ–° `kv_cache_eviction.py` ä¸­çš„ `max_cache_tokens` é»˜è®¤å€¼

```python
# å‚è€ƒé…ç½®æ¢¯åº¦:
MAX_CACHE_TOKENS = 100_000  # ä¿å®ˆ: ~3.4 GB cache, total ~10.5 GB
MAX_CACHE_TOKENS = 130_000  # ä¸­ç­‰: ~4.5 GB cache, total ~11.6 GB (æ¨èèµ·ç‚¹)
MAX_CACHE_TOKENS = 150_000  # æ¿€è¿›: ~5.2 GB cache, total ~12.3 GB (æ¥è¿‘æé™)
MAX_CACHE_TOKENS = 50_000   # å®‰å…¨ç½‘: ~1.7 GB cache, total ~8.8 GB (ä»…åœ¨ OOM æ—¶ç”¨)
```

**å…³é”®è¾“å‡ºæŒ‡æ ‡**:
- `Final VRAM reserved`: å°½é‡æ¥è¿‘ 22-23 GB (å……åˆ†åˆ©ç”¨)
- `effective_window_size / avg_chunk_tokens`: = çª—å£å†…èƒ½ä¿ç•™å¤šå°‘ä¸ª chunk, è¶Šå¤šè¶Šå¥½
- `ask() å›ç­”è´¨é‡`: åœ¨å®éªŒ C ä¸­æ¯”è¾ƒä¸åŒ max å€¼çš„æ­Œè¯æå–æ•ˆæœ

---

## å®éªŒ Cï¼šæ»‘çª—é€æ®µå¤„ç† + å‘¨æœŸæ€§è‡ªåŠ¨æé—®

### ç›®æ ‡

å°†è§†é¢‘åˆ†æ®µå¤„ç†, æ¯ç¼–ç  N ä¸ª chunk åè‡ªåŠ¨æé—®ä¸€æ¬¡, éªŒè¯:
1. æ·˜æ±°ä¸å½±å“ `ask()` çš„ snapshot/restore æœºåˆ¶
2. æ»‘çª—è¦†ç›–ä¸åŒè§†é¢‘æ®µ, æ¯æ®µéƒ½èƒ½æå–æœ‰æ•ˆä¿¡æ¯
3. æœ€ç»ˆèƒ½æ‹¼æ¥å‡ºè§†é¢‘ä¸­çš„æ­Œè¯/å­—å¹•å†…å®¹
4. å…¨ç¨‹ä¸ OOM

### æ­¥éª¤

è¯·åœ¨ `/root/autodl-tmp/diploma_project/temporal_encoding/` ç›®å½•ä¸‹åˆ›å»º `test_eviction_exp_c.py`:

```python
"""
å®éªŒ C: æ»‘çª—é€æ®µ + å‘¨æœŸæ€§æé—®ï¼Œæå–è§†é¢‘æ­Œè¯/å­—å¹•

éªŒè¯:
  1) æ¯ç¼–ç  ASK_INTERVAL ä¸ª chunk åè‡ªåŠ¨æé—®ä¸€æ¬¡
  2) æ·˜æ±°ä¸å½±å“ ask() çš„ snapshot/restore
  3) æ”¶é›†æ‰€æœ‰å›ç­”ï¼Œå»é‡åæ‹¼æ¥ä¸ºå®Œæ•´æ­Œè¯
  4) å…¨ç¨‹ä¸ OOM
"""
import os
import sys
import gc
import time
import torch
from datetime import datetime
from PIL import Image

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from model import StreamQwenModel, VideoStreamingInference, EvictionConfig

MODEL_PATH = os.environ.get(
    "QWEN_MODEL_PATH",
    "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct",
)
VIDEO_PATH = os.environ.get(
    "VIDEO_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/1.mp4",
)
REPORT_PATH = os.environ.get(
    "REPORT_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/test_eviction_exp_c_report.txt",
)

# â”€â”€ æ·˜æ±°å‚æ•° (å…¨è‡ªåŠ¨) â”€â”€
# â¬‡ï¸ åº”ä¸å®éªŒ B æœ€ç»ˆè°ƒä¼˜å€¼ä¸€è‡´; æ›´å¤§ â†’ çª—å£æ›´å¤§ â†’ è¿‘æœŸå¸§æ›´å¤š â†’ å›ç­”æ›´å¥½
MAX_CACHE_TOKENS = 130_000  # ä¸å®éªŒ B è°ƒä¼˜åä¿æŒä¸€è‡´

# â”€â”€ ç¼–ç å‚æ•° â”€â”€
CHUNK_FRAMES = 4
SAMPLE_FPS = 2.0

# â”€â”€ æé—®å‚æ•° â”€â”€
ASK_INTERVAL = 25       # æ¯ 25 ä¸ª chunk (~50 ç§’è§†é¢‘) æé—®ä¸€æ¬¡
MAX_NEW_TOKENS = 200

QUESTION = (
    "Read all text, lyrics, subtitles, or captions currently visible on screen. "
    "Output them verbatim. If there is no text, say 'No text visible'. "
    "Do NOT repeat previously mentioned text."
)


class TeeWriter:
    def __init__(self, *writers):
        self._writers = writers
    def write(self, text):
        for w in self._writers:
            w.write(text)
        self.flush()
    def flush(self):
        for w in self._writers:
            w.flush()


def get_vram_gb():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        return {
            "allocated": round(torch.cuda.memory_allocated() / (1024 ** 3), 3),
            "reserved": round(torch.cuda.memory_reserved() / (1024 ** 3), 3),
        }
    return {}


def extract_frames_from_video(video_path, fps=2.0):
    """ä»è§†é¢‘ä¸­æŒ‰æŒ‡å®š fps é‡‡æ ·å¸§ã€‚"""
    try:
        import decord
        from decord import VideoReader, cpu
        decord.bridge.set_bridge("native")
        vr = VideoReader(video_path, ctx=cpu(0))
        video_fps = vr.get_avg_fps()
        total_frames = len(vr)
        duration = total_frames / video_fps
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        print(f"  Video: {video_path}")
        print(f"  Duration: {duration:.1f}s, Total: {total_frames} frames")
        print(f"  Sampling at {fps} fps â†’ {len(indices)} frames")
        frames = []
        for idx in indices:
            frame = vr[idx].asnumpy()
            frames.append(Image.fromarray(frame))
        return frames, duration
    except ImportError:
        import cv2
        cap = cv2.VideoCapture(video_path)
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / video_fps if video_fps > 0 else 0
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        print(f"  Video: {video_path}")
        print(f"  Duration: {duration:.1f}s, Total: {total_frames} frames")
        print(f"  Sampling at {fps} fps â†’ {len(indices)} frames")
        frames = []
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
        cap.release()
        return frames, duration


def main():
    report_dir = os.path.dirname(REPORT_PATH)
    if report_dir:
        os.makedirs(report_dir, exist_ok=True)

    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        tee = TeeWriter(sys.stdout, f)
        original_stdout, original_stderr = sys.stdout, sys.stderr
        sys.stdout = tee
        sys.stderr = tee

        try:
            print("=" * 70)
            print("EXPERIMENT C: Sliding Window + Periodic Auto-Questioning")
            print("=" * 70)
            print(f"Report time: {datetime.now().isoformat(timespec='seconds')}")
            print(f"Ask interval: every {ASK_INTERVAL} chunks (~{ASK_INTERVAL * CHUNK_FRAMES / SAMPLE_FPS:.0f}s)")
            print(f"Eviction: max_cache_tokens={MAX_CACHE_TOKENS}, sink/window=auto")
            print()

            # â”€â”€ 0) æ£€æŸ¥ â”€â”€
            if not os.path.exists(MODEL_PATH):
                print(f"âŒ Model not found: {MODEL_PATH}")
                return
            if not os.path.exists(VIDEO_PATH):
                print(f"âŒ Video not found: {VIDEO_PATH}")
                return

            # â”€â”€ 1) åŠ è½½æ¨¡å‹ â”€â”€
            print("[1] Loading model...")
            from transformers import AutoProcessor
            device = "cuda"
            dtype = torch.bfloat16
            processor = AutoProcessor.from_pretrained(MODEL_PATH)
            model = StreamQwenModel.from_pretrained(
                MODEL_PATH, torch_dtype=dtype
            ).to(device)
            model.eval()
            print(f"  VRAM: {get_vram_gb()}")
            print()

            # â”€â”€ 2) æå–å¸§ â”€â”€
            print("[2] Extracting frames...")
            all_frames, duration = extract_frames_from_video(VIDEO_PATH, fps=SAMPLE_FPS)
            total_frame_count = len(all_frames)
            print()

            # â”€â”€ 3) åˆ›å»ºå¼•æ“ â”€â”€
            print("[3] Creating engine with eviction...")
            eviction_config = EvictionConfig(
                max_cache_tokens=MAX_CACHE_TOKENS,
            )
            engine = VideoStreamingInference(
                model, processor, device, eviction_config=eviction_config
            )
            print()

            # â”€â”€ 4) ç¼–ç  + å‘¨æœŸæ€§æé—® â”€â”€
            print("[4] Encoding with periodic questioning...")
            all_answers = []
            chunk_count = 0
            t_start = time.time()

            for i in range(0, total_frame_count, CHUNK_FRAMES):
                chunk = all_frames[i : i + CHUNK_FRAMES]
                if len(chunk) == 0:
                    continue
                if len(chunk) % 2 != 0:
                    chunk.append(chunk[-1])

                engine.append_video_chunk(chunk, fps=SAMPLE_FPS)
                chunk_count += 1

                # å‘¨æœŸæ€§æé—®
                if chunk_count % ASK_INTERVAL == 0:
                    time_pos = (i + CHUNK_FRAMES) / SAMPLE_FPS
                    print(f"\n  â”€â”€â”€ Ask at chunk {chunk_count} "
                          f"(video ~{time_pos:.0f}s / {duration:.0f}s) â”€â”€â”€")

                    info = engine.get_cache_info()
                    vram = get_vram_gb()
                    eviction_str = ""
                    if "eviction_stats" in info:
                        es = info["eviction_stats"]
                        eviction_str = f", evictions={es.get('total_evictions', 0)}"
                    print(f"  Cache: len={info['cache_seq_length']}, "
                          f"mem={info.get('cache_memory_gb', 0):.3f} GB, "
                          f"VRAM={vram.get('allocated', 0):.2f} GB"
                          f"{eviction_str}")

                    # è®°å½• ask å‰ cache é•¿åº¦
                    pre_ask_len = info['cache_seq_length']

                    answer, metrics = engine.ask(
                        QUESTION,
                        max_new_tokens=MAX_NEW_TOKENS,
                        do_sample=True,
                        temperature=0.3,
                    )

                    # éªŒè¯ snapshot/restore
                    post_ask_len = engine.cache_manager.get_seq_length()
                    restored = (post_ask_len == pre_ask_len)

                    all_answers.append({
                        "chunk": chunk_count,
                        "time_pos": f"~{time_pos:.0f}s",
                        "answer": answer.strip(),
                        "ttft": metrics["ttft"],
                        "cache_restored": restored,
                    })
                    print(f"  Answer: {answer.strip()[:150]}...")
                    print(f"  TTFT: {metrics['ttft']:.3f}s, "
                          f"Cache restored: {'âœ…' if restored else 'âŒ'}")

            t_total = time.time() - t_start

            # æœ€åä¸€æ®µå¦‚æœè¿˜æ²¡é—®è¿‡ï¼Œè¡¥ä¸€æ¬¡
            if chunk_count % ASK_INTERVAL != 0:
                print(f"\n  â”€â”€â”€ Final ask at chunk {chunk_count} â”€â”€â”€")
                pre_ask_len = engine.cache_manager.get_seq_length()
                answer, metrics = engine.ask(
                    QUESTION,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=True,
                    temperature=0.3,
                )
                post_ask_len = engine.cache_manager.get_seq_length()
                restored = (post_ask_len == pre_ask_len)
                all_answers.append({
                    "chunk": chunk_count,
                    "time_pos": f"~{total_frame_count / SAMPLE_FPS:.0f}s",
                    "answer": answer.strip(),
                    "ttft": metrics["ttft"],
                    "cache_restored": restored,
                })
                print(f"  Answer: {answer.strip()[:150]}...")

            print(f"\n  âœ… Done: {chunk_count} chunks, {len(all_answers)} questions asked")
            print(f"  Total time: {t_total:.1f}s")
            print()

            # â”€â”€ 5) æ±‡æ€»æ‰€æœ‰æ­Œè¯ â”€â”€
            print("=" * 70)
            print("ALL COLLECTED LYRICS / SUBTITLES")
            print("=" * 70)

            seen_lines = set()
            unique_lyrics = []

            for entry in all_answers:
                print(f"\n[{entry['time_pos']}] (chunk {entry['chunk']}):")
                print(f"  {entry['answer']}")

                lines = entry["answer"].split("\n")
                for line in lines:
                    line_clean = line.strip().lower()
                    if (
                        line_clean
                        and line_clean not in seen_lines
                        and "no text" not in line_clean
                        and "no lyrics" not in line_clean
                        and "no subtitle" not in line_clean
                        and "no caption" not in line_clean
                        and "no visible" not in line_clean
                    ):
                        seen_lines.add(line_clean)
                        unique_lyrics.append(line.strip())

            print()
            print("=" * 70)
            print("DEDUPLICATED LYRICS (all unique lines)")
            print("=" * 70)
            for line in unique_lyrics:
                print(f"  {line}")
            print(f"\n  Total unique lines: {len(unique_lyrics)}")

            # â”€â”€ 6) æ€»ç»“ + é€šè¿‡åˆ¤å®š â”€â”€
            print()
            print("=" * 70)
            print("SUMMARY & PASS/FAIL")
            print("=" * 70)
            final_info = engine.get_cache_info()
            print(f"  Video duration: {duration:.0f}s")
            print(f"  Total chunks: {chunk_count}")
            print(f"  Questions asked: {len(all_answers)}")
            print(f"  Unique lyric lines: {len(unique_lyrics)}")
            print(f"  Final cache_len: {final_info['cache_seq_length']}")
            print(f"  Total time: {t_total:.1f}s")

            if "eviction_stats" in final_info:
                es = final_info["eviction_stats"]
                print(f"  Total evictions: {es.get('total_evictions', 0)}")
                print(f"  Total tokens evicted: {es.get('total_tokens_evicted', 0)}")

            avg_ttft = sum(a["ttft"] for a in all_answers) / max(len(all_answers), 1)
            print(f"  Average TTFT: {avg_ttft:.3f}s")

            # é€šè¿‡åˆ¤å®š
            print()
            all_pass = True

            # C1: ä¸ OOM
            print(f"  âœ… [C1] No OOM â€” processed all {chunk_count} chunks")

            # C2: æ‰€æœ‰ ask å cache æ¢å¤
            all_restored = all(a["cache_restored"] for a in all_answers)
            if all_restored:
                print(f"  âœ… [C2] All {len(all_answers)} ask() calls "
                      f"correctly restored cache (snapshot/restore)")
            else:
                failed = [a for a in all_answers if not a["cache_restored"]]
                print(f"  âŒ [C2] {len(failed)} ask() calls did not restore cache!")
                all_pass = False

            # C3: è‡³å°‘ N æ¬¡æé—®æœ‰éç©ºå›ç­”
            non_empty = [
                a for a in all_answers
                if a["answer"]
                and "no text" not in a["answer"].lower()
                and "no visible" not in a["answer"].lower()
            ]
            if len(non_empty) >= 1:
                print(f"  âœ… [C3] {len(non_empty)}/{len(all_answers)} answers "
                      f"contained text/lyrics")
            else:
                print(f"  âš ï¸ [C3] All answers were empty/no text â€” "
                      f"video may not contain visible text")

            # C4: æå–åˆ°æ­Œè¯è¡Œ
            if len(unique_lyrics) >= 1:
                print(f"  âœ… [C4] Extracted {len(unique_lyrics)} unique lyric lines")
            else:
                print(f"  âš ï¸ [C4] No lyrics extracted â€” may be expected if video has no text")

            # C5: TTFT åˆç† (< 10s)
            if avg_ttft < 10.0:
                print(f"  âœ… [C5] Average TTFT ({avg_ttft:.3f}s) < 10s")
            else:
                print(f"  âš ï¸ [C5] Average TTFT ({avg_ttft:.3f}s) â‰¥ 10s â€” may be slow")

            print()
            if all_pass:
                print("ğŸ‰ EXPERIMENT C: ALL PASSED")
            else:
                print("âš ï¸ EXPERIMENT C: SOME CHECKS FAILED â€” see above")

        except torch.cuda.OutOfMemoryError:
            print(f"\nâŒ EXPERIMENT C FAILED: CUDA OOM!")
            import traceback
            traceback.print_exc()

        except Exception as e:
            print(f"\nâŒ EXPERIMENT C FAILED: {e}")
            import traceback
            traceback.print_exc()

        finally:
            sys.stdout = original_stdout
            sys.stderr = original_stderr

    print(f"\nReport saved to: {REPORT_PATH}")


if __name__ == "__main__":
    main()
```

### è¿è¡Œå‘½ä»¤

```bash
cd /root/autodl-tmp/diploma_project/temporal_encoding
python test_eviction_exp_c.py
```

### âœ… é€šè¿‡æ ‡å‡†

1. **[C1]** å…¨ç¨‹ä¸ OOM
2. **[C2]** æ‰€æœ‰ `ask()` è°ƒç”¨å cache æ­£ç¡®æ¢å¤ (snapshot/restore åœ¨æ·˜æ±°åä»å·¥ä½œ)
3. **[C3]** è‡³å°‘æœ‰éƒ¨åˆ†æé—®è¿”å›äº†æœ‰æ•ˆæ–‡æœ¬å†…å®¹ (å–å†³äºè§†é¢‘)
4. **[C4]** å»é‡åæå–åˆ°æ­Œè¯/å­—å¹•è¡Œ (å–å†³äºè§†é¢‘)
5. **[C5]** å¹³å‡ TTFT < 10s

### âŒ å¦‚æœå¤±è´¥

- **OOM**: ä¸å®éªŒ B ç›¸åŒçš„æ’æŸ¥æ­¥éª¤
- **Cache ä¸æ¢å¤**: `cache_manager.py` çš„ `snapshot()` å¯èƒ½æœªåŒ…å« tracker çš„ eviction ç›¸å…³çŠ¶æ€
- **æ‰€æœ‰å›ç­”ä¸ºç©º**: å¯èƒ½è§†é¢‘ç¡®å®æ— æ–‡æœ¬, æˆ– `QUESTION` prompt éœ€è¦è°ƒæ•´, æˆ– `temperature` å¤ªä½
- **ä¿®å¤åé‡æ–°è¿è¡Œ**

### å‚æ•°è°ƒä¼˜å»ºè®®

```python
# æ›´é¢‘ç¹æé—®
ASK_INTERVAL = 15      # æ¯ 15 chunk (~30s)

# æ›´ä½æ¸©åº¦, æ›´ç²¾ç¡®
temperature = 0.1

# å¦‚æœæ­Œè¯ä¸ºä¸­æ–‡
QUESTION = "è¯»å–å±å¹•ä¸Šå½“å‰å¯è§çš„æ‰€æœ‰æ–‡å­—ã€æ­Œè¯ã€å­—å¹•ã€‚é€å­—è¾“å‡ºã€‚å¦‚æœæ²¡æœ‰æ–‡å­—ï¼Œè¯´'æ— æ–‡å­—'ã€‚"
```

---

## ç†è®ºé£é™©ä¸ Copilot æ¢ç´¢æŒ‡ä»¤

### âš ï¸ ç†è®ºé£é™©è¯´æ˜

æœ¬å®éªŒé‡‡ç”¨â€œåŸç”Ÿ Qwen2.5-VLâ€æ¨¡å‹ï¼ˆæœªé’ˆå¯¹å¤§è·¨åº¦ position gap/æ·˜æ±°åœºæ™¯å¾®è°ƒï¼‰ï¼Œå…¶æ³¨æ„åŠ›æœºåˆ¶åœ¨é‡åˆ°å¤§è§„æ¨¡ KV Cache æ·˜æ±°ï¼ˆå³ position index ä¸è¿ç»­ï¼Œgap å¾ˆå¤§ï¼‰æ—¶ï¼Œç†è®ºä¸Šå¯èƒ½å‡ºç°ï¼š

- æ³¨æ„åŠ›åˆ†å¸ƒå¼‚å¸¸ï¼ˆå¦‚ attention collapseã€å†å²å¸§ä¿¡æ¯ä¸¢å¤±ï¼‰
- é•¿æ—¶ä¾èµ–å»ºæ¨¡èƒ½åŠ›ä¸‹é™ï¼Œå¯¼è‡´é—®ç­”/å­—å¹•æå–ç­‰ä¸‹æ¸¸ä»»åŠ¡è´¨é‡å˜å·®

æ­¤é£é™©å±äºæ¨¡å‹é¢„è®­ç»ƒåˆ†å¸ƒå¤–ï¼ˆOODï¼‰è¡Œä¸ºï¼Œå·¥ç¨‹ä¸Šå¯è¿è¡Œï¼Œä½†æ¨ç†è´¨é‡éœ€å®éªŒéªŒè¯ã€‚

### ğŸš¦ Copilot è‡ªåŠ¨æ¢ç´¢æŒ‡ä»¤

å¦‚å®éªŒ C å‘ç°æ·˜æ±°åæ¨¡å‹è¾“å‡ºè´¨é‡æ˜æ˜¾ä¸‹é™ï¼ˆå¦‚å­—å¹•/é—®ç­”å†…å®¹ä¸¢å¤±ã€æ— å…³ã€é‡å¤ã€æç«¯ç®€åŒ–ç­‰ï¼‰ï¼ŒCopilot å¿…é¡»ï¼š

1. è‡ªåŠ¨åˆ†ææ—¥å¿—ä¸è¾“å‡ºï¼Œç¡®è®¤æ˜¯å¦ä¸º position gap å¯¼è‡´çš„æ³¨æ„åŠ›å¼‚å¸¸
2. åœ¨**ä¸å¾®è°ƒæ¨¡å‹**å‰æä¸‹ï¼Œæ¢ç´¢ä»¥ä¸‹å·¥ç¨‹/æ¨ç†å±‚è§£å†³æ€è·¯ï¼Œå¹¶é€ä¸€å®éªŒï¼š
    - **ä½ç½®é‡æ˜ å°„**ï¼šå°†æ–°å¸§ position index è¿ç»­åŒ–ï¼ˆå¦‚é‡ç½®ä¸º sink+1â€¦sink+Nï¼‰ï¼Œé¿å…å¤§ gap
    - **é™ä½åˆ†è¾¨ç‡/å¸§ç‡**ï¼šå‡å°‘å•å¸§ token æ•°ï¼Œå‡ç¼“æ·˜æ±°é¢‘ç‡
    - **å¢å¤§ max_cache_tokens**ï¼šæƒè¡¡æ˜¾å­˜ä¸å†å²ä¿ç•™é•¿åº¦
    - **å…¶ä»–æ¨ç†å±‚æŠ€å·§**ï¼šå¦‚åˆ†å—é—®ç­”ã€æ»‘åŠ¨çª—å£èåˆã€prompt å·¥ç¨‹ç­‰
3. è®°å½•æ¯ç§æ–¹æ¡ˆçš„å®éªŒç»“æœï¼Œä¼˜å…ˆæ¨èæ— éœ€æ¨¡å‹æ”¹åŠ¨å³å¯æ˜¾è‘—æå‡è´¨é‡çš„æ–¹æ¡ˆ

å¦‚æ‰€æœ‰å·¥ç¨‹æ–¹æ¡ˆå‡æ— æ•ˆï¼Œå†å»ºè®®å¾®è°ƒ/é€‚é…è®­ç»ƒã€‚

---

## ä¸‰ä¸ªå®éªŒçš„æ‰§è¡Œé¡ºåºä¸è¿­ä»£æŒ‡å¼•

### æ¨èæ‰§è¡Œé¡ºåº

```
å®éªŒ A (sink æ£€æµ‹) â†’ å®éªŒ B (OOM-Free) â†’ å®éªŒ C (ask è´¨é‡)
```

A æ˜¯åŸºç¡€: å¦‚æœ sink æ£€æµ‹ä¸å¯¹, B/C çš„æ·˜æ±°è¡Œä¸ºéƒ½ä¼šå¼‚å¸¸ã€‚
B æ˜¯æ ¸å¿ƒ: å¦‚æœ OOM, C è‡ªç„¶ä¹Ÿä¼š OOMã€‚
C æ˜¯æœ€ç»ˆéªŒè¯: æ·˜æ±°åæ¨¡å‹ä»ç„¶æœ‰ç”¨ã€‚

### è¿­ä»£ç­–ç•¥

æ¯ä¸ªå®éªŒå¦‚æœå¤±è´¥, æŒ‰ä»¥ä¸‹æµç¨‹:

1. **é˜…è¯»æŠ¥å‘Š**: å®Œæ•´é˜…è¯» `_report.txt`, ç¡®è®¤å“ªä¸ªåˆ¤å®šæ¡ä»¶å¤±è´¥
2. **å®šä½ä»£ç **: æ ¹æ®å¤±è´¥ç±»å‹, æŸ¥çœ‹å¯¹åº”æ–‡ä»¶:
   - sink ç›¸å…³ â†’ `kv_cache_eviction.py` çš„ `set_first_chunk_info()` + `video_stream_inference.py` çš„è°ƒç”¨æ—¶æœº
   - æ·˜æ±°æœªè§¦å‘ â†’ `kv_cache_eviction.py` çš„ `should_evict()` + `evict()` + `video_stream_inference.py` çš„ `_chunk_counter`
   - OOM â†’ `max_cache_tokens` å€¼, æˆ– `torch.cuda.empty_cache()` ç¼ºå¤±
   - snapshot/restore â†’ `cache_manager.py` çš„ `take_snapshot()` / `restore_snapshot()`
3. **ä¿®æ”¹ä»£ç **: åªæ”¹å¿…è¦çš„éƒ¨åˆ†
4. **é‡æ–°è¿è¡Œ**: åŒä¸€å®éªŒ, ç›´è‡³é€šè¿‡
5. **è¿›å…¥ä¸‹ä¸€ä¸ªå®éªŒ**

### è°ƒè¯•å»ºè®®

```python
# åœ¨ video_stream_inference.py çš„ append_frame ä¸­åŠ å…¥ verbose è¾“å‡º:
print(f"  [DEBUG] chunk={self.frame_count}, "
      f"cache_before={prev_len}, cache_after={cache_len_after}, "
      f"sink={evictor.effective_sink_size if evictor else 'N/A'}")
```

### æ–‡ä»¶å˜æ›´æ¸…å•

ä»¥ä¸‹æ–‡ä»¶å·²æ–°å¢/ä¿®æ”¹, è¯·ç¡®è®¤å‡å·²åŒæ­¥åˆ°è¿œç¨‹æœºå™¨:

| æ–‡ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| `model/kv_cache_eviction.py` | ğŸ†• é‡å†™ | sink è‡ªåŠ¨æ£€æµ‹, å‡åŒ€æ—¶åºé‡‡æ ·, å¸§çº§é‡è¦æ€§ |
| `model/cache_manager.py` | âœï¸ ä¿®æ”¹ | é›†æˆ `set_first_chunk_info()`, `track_tokens(is_new_chunk=)` |
| `model/video_stream_inference.py` | âœï¸ ä¿®æ”¹ | é¦– chunk auto-detect, chunk stats æ›´æ–° |
| `model/__init__.py` | âœï¸ å·²æœ‰ | å¯¼å‡º EvictionConfig, KVCacheEvictor ç­‰ |
| `PROJECT_STRUCTURE_V2.md` | âœï¸ é‡å†™ | ä¿®æ­£å®¹é‡è¡¨ã€å‚æ•°è¯´æ˜ã€ç­–ç•¥æè¿° |
| `EVICTION_EXPERIMENT_PROMPT.md` | ğŸ†• é‡å†™ | æœ¬æ–‡ä»¶: 3 ä¸ªå®éªŒ + è¿­ä»£æŒ‡å¼• |
