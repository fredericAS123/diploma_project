"""
å®éªŒ D: å…¨è§†é¢‘æ»‘åŠ¨ + æˆªè‡³å½“å‰æ—¶åˆ»ç´¯è®¡æ­Œè¯æŠ½å–

ç›®æ ‡:
1) ä½¿ç”¨ç°æœ‰æµå¼æ»‘åŠ¨ç¼“å­˜æ–¹æ¡ˆå¤„ç†å®Œæ•´è§†é¢‘ã€‚
2) å‘¨æœŸæ€§æé—®â€œæˆªè‡³ç›®å‰å‡ºç°è¿‡çš„æ‰€æœ‰æ­Œè¯/å­—å¹•â€ï¼Œè€Œéåªé—®å½“å‰ç”»é¢ã€‚
3) è‡ªåŠ¨æ±‡æ€»ã€å»é‡ã€æŒ‰é¦–æ¬¡å‡ºç°æ—¶é—´æ’åºï¼Œå¾—åˆ°å°½å¯èƒ½å®Œæ•´çš„æ­Œè¯æ¸…å•ã€‚
4) è‹¥ç»“æœä¸è¶³ï¼Œä¾¿äºè¿­ä»£ prompt / æé—®é¢‘ç‡ã€‚
"""

import os
import re
import sys
import time
import gc
from datetime import datetime
from typing import List, Dict

import torch
from PIL import Image

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from model import StreamQwenModel, VideoStreamingInference, EvictionConfig

MODEL_PATH = os.environ.get("QWEN_MODEL_PATH", "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct")
VIDEO_PATH = os.environ.get("VIDEO_PATH", "/root/autodl-tmp/diploma_project/temporal_encoding/202208312002.mp4")
REPORT_PATH = os.environ.get(
    "REPORT_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/test_eviction_exp_d_cumulative_lyrics_report.txt",
)

MAX_CACHE_TOKENS = int(os.environ.get("MAX_CACHE_TOKENS", "150000"))
CHUNK_FRAMES = int(os.environ.get("CHUNK_FRAMES", "4"))
SAMPLE_FPS = float(os.environ.get("SAMPLE_FPS", "2.0"))
ASK_INTERVAL = int(os.environ.get("ASK_INTERVAL", "5"))
MAX_NEW_TOKENS = int(os.environ.get("MAX_NEW_TOKENS", "256"))

QUESTION_CUMULATIVE = (
    "è¯·åŸºäºä½ æˆªè‡³å½“å‰æ—¶åˆ»åœ¨è§†é¢‘ä¸­çœ‹è§è¿‡çš„æ‰€æœ‰ç”»é¢ï¼Œ"
    "åªæ±‡æ€»æ­Œè¯/å­—å¹•çš„æ­£æ–‡å¥å­ï¼ˆä¸è¦æ ‡é¢˜ã€æ­Œæ‰‹åã€å“ç‰Œè¯ã€åˆ¶ä½œä¿¡æ¯ï¼‰ã€‚"
    "è¦æ±‚ï¼š\n"
    "1) åªè¾“å‡ºä½ ç¡®è®¤çœ‹è§è¿‡çš„æ­Œè¯æ­£æ–‡ï¼Œä¸è¦çŒœæµ‹ï¼›\n"
    "2) æ¯è¡Œä¸€æ¡ï¼Œå°½é‡å®Œæ•´å¥å­ï¼›\n"
    "3) ä¸è¦è¾“å‡ºï¼šå®‰æ…•å¸Œã€æ­Œåã€äººåã€è¯/æ›²/ç¼–æ›²/åŸå”±ç­‰ä¿¡æ¯ï¼›\n"
    "5) å°½é‡è¡¥å…¨æ­¤å‰æ¼æ‰çš„æ­Œè¯å¥å­ã€‚"
    "4) è‹¥å½“å‰è¿˜æ— æ³•ç¡®è®¤ä»»ä½•æ­Œè¯æ­£æ–‡ï¼Œåªè¾“å‡ºï¼šæ— æ–‡å­—ã€‚"
)

QUESTION_CURRENT = (
    "è¯·è¯»å–å½“å‰ç”»é¢å¯è§çš„æ­Œè¯/å­—å¹•ã€‚"
    "å¦‚æœçœ‹åˆ°ä¸¤è¡Œæ­Œè¯ï¼Œè¯·ä¸¤è¡Œéƒ½è¾“å‡ºï¼ˆæ¯è¡Œä¸€æ¡ï¼‰ï¼›"
    "å¦‚æœåªçœ‹åˆ°ä¸€è¡Œï¼Œå°±è¾“å‡ºä¸€è¡Œï¼›"
    "å¦‚æœæ²¡æœ‰å¯è§æ­Œè¯ï¼Œè¾“å‡ºï¼šæ— æ–‡å­—ã€‚"
)



class TeeWriter:
    def __init__(self, *writers):
        self._writers = writers

    def write(self, text):
        for w in self._writers:
            w.write(text)
        self.flush()

    def flush(self):
        for w in self._writers:
            w.flush()


def get_vram_gb():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        return {
            "allocated": round(torch.cuda.memory_allocated() / (1024 ** 3), 3),
            "reserved": round(torch.cuda.memory_reserved() / (1024 ** 3), 3),
            "max_allocated": round(torch.cuda.max_memory_allocated() / (1024 ** 3), 3),
        }
    return {}


def extract_frames_from_video(video_path, fps=2.0):
    try:
        import decord
        from decord import VideoReader, cpu

        decord.bridge.set_bridge("native")
        vr = VideoReader(video_path, ctx=cpu(0))
        video_fps = vr.get_avg_fps()
        total_frames = len(vr)
        duration = total_frames / video_fps
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        frames = [Image.fromarray(vr[idx].asnumpy()) for idx in indices]
        return frames, duration, total_frames, video_fps
    except ImportError:
        import cv2

        cap = cv2.VideoCapture(video_path)
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / video_fps if video_fps > 0 else 0
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        frames = []
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
        cap.release()
        return frames, duration, total_frames, video_fps


def normalize_line(line: str) -> str:
    s = line.strip()
    s = re.sub(r"^[\-\*\d\.)ã€\s]+", "", s)
    s = s.strip(" \t\r\n\"'â€œâ€â€˜â€™")
    return s


def parse_lines(answer: str) -> List[str]:
    bad = ["æ— æ–‡å­—", "no text", "çœ‹ä¸æ¸…", "æ— æ³•ç¡®è®¤", "none", "æ²¡æœ‰"]
    bad_meta = ["å®‰æ…•å¸Œ", "è‘£ä¹¦å«", "è¯ ", "æ›² ", "ç¼–æ›²", "åŸå”±", "æ­Œæ‰‹", "æ¼”å”±"]

    lines = []
    for raw in answer.split("\n"):
        s = normalize_line(raw)
        if not s:
            continue
        ls = s.lower()
        if any(k in ls for k in bad):
            continue
        if any(m in s for m in bad_meta):
            continue
        # è¿‡æ»¤è¿‡çŸ­ç¢ç‰‡ï¼ˆå¦‚â€œå®‰â€â€œè‘£ä¹¦â€ï¼‰
        if len(s) < 6:
            continue
        # è¿‡æ»¤â€œxxå¯ä»¥â€è¿™ç±»æ˜æ˜¾éæ­Œè¯ç¢å¥
        if s.endswith("å¯ä»¥") and len(s) <= 8:
            continue
        lines.append(s)
    return lines


def main():
    report_dir = os.path.dirname(REPORT_PATH)
    if report_dir:
        os.makedirs(report_dir, exist_ok=True)

    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        tee = TeeWriter(sys.stdout, f)
        old_stdout, old_stderr = sys.stdout, sys.stderr
        sys.stdout = tee
        sys.stderr = tee

        try:
            print("=" * 78)
            print("EXPERIMENT D: CUMULATIVE LYRICS OVER FULL VIDEO")
            print("=" * 78)
            print(f"Report time: {datetime.now().isoformat(timespec='seconds')}")
            print(f"VIDEO_PATH={VIDEO_PATH}")
            print(f"MAX_CACHE_TOKENS={MAX_CACHE_TOKENS}, CHUNK_FRAMES={CHUNK_FRAMES}, SAMPLE_FPS={SAMPLE_FPS}")
            print(f"ASK_INTERVAL={ASK_INTERVAL}, MAX_NEW_TOKENS={MAX_NEW_TOKENS}")
            print()

            if not os.path.exists(MODEL_PATH):
                print(f"âŒ Model not found: {MODEL_PATH}")
                return
            if not os.path.exists(VIDEO_PATH):
                print(f"âŒ Video not found: {VIDEO_PATH}")
                return

            print("[1] Loading model...")
            from transformers import AutoProcessor

            processor = AutoProcessor.from_pretrained(MODEL_PATH)
            model = StreamQwenModel.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16).to("cuda")
            model.eval()
            print(f"  VRAM after load: {get_vram_gb()}")
            print()

            print("[2] Extracting sampled frames...")
            frames, duration, total_frames, video_fps = extract_frames_from_video(VIDEO_PATH, fps=SAMPLE_FPS)
            total_sampled = len(frames)
            expected_chunks = (total_sampled + CHUNK_FRAMES - 1) // CHUNK_FRAMES
            print(f"  Raw video: duration={duration:.1f}s, fps={video_fps:.2f}, frames={total_frames}")
            print(f"  Sampled: {total_sampled} frames => {expected_chunks} chunks")
            print()

            print("[3] Running streaming inference with sliding window...")
            config = EvictionConfig(max_cache_tokens=MAX_CACHE_TOKENS)
            engine = VideoStreamingInference(model, processor, "cuda", eviction_config=config)

            seen_map: Dict[str, Dict[str, str]] = {}
            ask_records = []
            chunk_count = 0
            t_start = time.time()

            for i in range(0, total_sampled, CHUNK_FRAMES):
                chunk = frames[i: i + CHUNK_FRAMES]
                if not chunk:
                    continue
                if len(chunk) % 2 != 0:
                    chunk.append(chunk[-1])

                engine.append_video_chunk(chunk, fps=SAMPLE_FPS)
                chunk_count += 1

                should_ask = (chunk_count % ASK_INTERVAL == 0) or (i + CHUNK_FRAMES >= total_sampled)
                if not should_ask:
                    continue

                time_pos = (i + CHUNK_FRAMES) / SAMPLE_FPS
                info = engine.get_cache_info()
                pre_len = info["cache_seq_length"]

                ans_cum, m1 = engine.ask(
                    QUESTION_CUMULATIVE,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=False,
                    temperature=0.1,
                )
                post_len1 = engine.cache_manager.get_seq_length()

                ans_cur, m2 = engine.ask(
                    QUESTION_CURRENT,
                    max_new_tokens=96,
                    do_sample=False,
                    temperature=0.1,
                )
                post_len2 = engine.cache_manager.get_seq_length()

                restore_ok = (post_len1 == pre_len) and (post_len2 == pre_len)

                lines_cum = parse_lines(ans_cum)
                lines_cur = parse_lines(ans_cur)
                merged = lines_cum + [x for x in lines_cur if x not in lines_cum]

                new_lines = []
                for ln in merged:
                    key = ln.lower()
                    if key not in seen_map:
                        seen_map[key] = {
                            "line": ln,
                            "first_time": f"~{time_pos:.0f}s",
                            "first_chunk": str(chunk_count),
                        }
                        new_lines.append(ln)

                ask_records.append(
                    {
                        "chunk": chunk_count,
                        "time": f"~{time_pos:.0f}s",
                        "cache_len": pre_len,
                        "ttft_cum": m1["ttft"],
                        "ttft_cur": m2["ttft"],
                        "restore_ok": restore_ok,
                        "ans_cum": ans_cum.strip(),
                        "ans_cur": ans_cur.strip(),
                        "new_lines": new_lines,
                    }
                )

                print(f"  Ask@chunk={chunk_count:>3d}, t~{time_pos:>5.0f}s, cache={pre_len}, restore={'âœ…' if restore_ok else 'âŒ'}")
                print(f"    cumulative: {ans_cum.strip()[:140]}")
                print(f"    current   : {ans_cur.strip()[:140]}")
                if new_lines:
                    print(f"    + New lines ({len(new_lines)}): {new_lines}")
                else:
                    print("    + New lines: 0")

            total_time = time.time() - t_start
            final_info = engine.get_cache_info()

            print("\n" + "=" * 78)
            print("FINAL DEDUPLICATED LYRICS (ORDERED BY FIRST APPEARANCE)")
            print("=" * 78)
            ordered = sorted(seen_map.values(), key=lambda x: int(x["first_chunk"]))
            for row in ordered:
                print(f"[{row['first_time']}] {row['line']}")
            print(f"\nTotal unique lines: {len(ordered)}")

            print("\n" + "=" * 78)
            print("ANALYSIS")
            print("=" * 78)
            restored_all = all(r["restore_ok"] for r in ask_records)
            avg_ttft_cum = sum(r["ttft_cum"] for r in ask_records) / max(len(ask_records), 1)
            avg_ttft_cur = sum(r["ttft_cur"] for r in ask_records) / max(len(ask_records), 1)

            print(f"Total chunks processed: {chunk_count}/{expected_chunks}")
            print(f"Total asks: {len(ask_records)}")
            print(f"All snapshot/restore valid: {restored_all}")
            print(f"Avg TTFT cumulative ask: {avg_ttft_cum:.3f}s")
            print(f"Avg TTFT current ask: {avg_ttft_cur:.3f}s")
            print(f"Final cache_len: {final_info['cache_seq_length']}")
            if "eviction_stats" in final_info:
                es = final_info["eviction_stats"]
                print(f"Total evictions: {es.get('total_evictions', 0)}")
                print(f"Total tokens evicted: {es.get('total_tokens_evicted', 0)}")
            print(f"Total runtime: {total_time:.1f}s")

            print("\nPass/Fail checks:")
            ok1 = chunk_count == expected_chunks
            ok2 = restored_all
            ok3 = len(ordered) >= 6
            ok4 = final_info["cache_seq_length"] <= MAX_CACHE_TOKENS
            print(f"  [D1] Full-video sliding completed: {'âœ…' if ok1 else 'âŒ'}")
            print(f"  [D2] All ask restore cache: {'âœ…' if ok2 else 'âŒ'}")
            print(f"  [D3] Extracted enough unique lyric lines (>=6): {'âœ…' if ok3 else 'âŒ'}")
            print(f"  [D4] cache_len <= max_cache_tokens: {'âœ…' if ok4 else 'âŒ'}")

            if ok1 and ok2 and ok3 and ok4:
                print("\nğŸ‰ EXPERIMENT D PASSED")
            else:
                print("\nâš ï¸ EXPERIMENT D NOT FULLY PASSED â€” please iterate prompt/frequency.")

        except Exception as e:
            print(f"\nâŒ EXPERIMENT D FAILED: {e}")
            import traceback
            traceback.print_exc()

        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

    print(f"\nReport saved to: {REPORT_PATH}")


if __name__ == "__main__":
    main()
