"""
æµ‹è¯•æ–°æ–¹æ¡ˆï¼šå»¶è¿Ÿæ‰¹é‡ç¼–ç  (Delayed Batch Inference)

æµ‹è¯•æµç¨‹ï¼š
1. æµå¼æ·»åŠ å¸§
2. åœ¨ä¸åŒæ—¶åˆ»æé—®
3. å¯¹æ¯”åŸç”Ÿè§†é¢‘æ¨ç†çš„ç»“æœ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from PIL import Image
import time
from pathlib import Path

from temporal_encoding.model.delayed_batch_inference import DelayedBatchInferenceEngine


def load_test_video_frames(video_source: str, max_frames: int = 50):
    """åŠ è½½æµ‹è¯•è§†é¢‘å¸§ï¼ˆæ”¯æŒå¸§ç›®å½•æˆ–è§†é¢‘æ–‡ä»¶ï¼‰"""
    source_path = Path(video_source)
    frames: list[Image.Image] = []

    if source_path.is_dir():
        frame_files = sorted(source_path.glob("*.jpg"))[:max_frames]
        for f in frame_files:
            frames.append(Image.open(f).convert("RGB"))
    elif source_path.is_file():
        try:
            import cv2
        except Exception as exc:
            raise RuntimeError("ç¼ºå°‘ OpenCVï¼Œæ— æ³•ä»è§†é¢‘æ–‡ä»¶æå–å¸§") from exc

        cap = cv2.VideoCapture(str(source_path))
        if not cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {source_path}")

        try:
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if frame_count <= 0:
                raise RuntimeError("è§†é¢‘å¸§æ•°ä¸º 0ï¼Œæ— æ³•æå–")

            if max_frames >= frame_count:
                indices = list(range(frame_count))
            else:
                step = frame_count / max_frames
                indices = [int(i * step) for i in range(max_frames)]

            for idx in indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret and frame is not None:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(Image.fromarray(frame_rgb))
        finally:
            cap.release()
    else:
        raise RuntimeError(f"è§†é¢‘æºä¸å­˜åœ¨: {source_path}")

    print(f"âœ… åŠ è½½ {len(frames)} å¸§")
    return frames


def test_delayed_batch_inference():
    """æµ‹è¯•å»¶è¿Ÿæ‰¹é‡ç¼–ç æ–¹æ¡ˆ"""
    print("="*80)
    print("æµ‹è¯•ï¼šå»¶è¿Ÿæ‰¹é‡ç¼–ç æ–¹æ¡ˆ (Delayed Batch Inference)")
    print("="*80)
    
    # 1. åŠ è½½æ¨¡å‹
    model_path = "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct"
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda",
        trust_remote_code=True,
    )
    
    processor = AutoProcessor.from_pretrained(
        model_path,
        trust_remote_code=True,
    )
    
    # 2. åˆ›å»ºå¼•æ“
    print("\nğŸš€ åˆå§‹åŒ– DelayedBatchInferenceEngine")
    engine = DelayedBatchInferenceEngine(
        model=model,
        processor=processor,
        device="cuda",
        star_memory_size=20,      # Star Memory å®¹é‡
        stream_window_size=20,    # Stream Memory çª—å£å¤§å°
        max_pixels=2 * 224 * 224, # ä½åˆ†è¾¨ç‡ç­–ç•¥
    )
    
    # 3. åŠ è½½æµ‹è¯•è§†é¢‘
    video_source = "/root/autodl-tmp/diploma/temporal_encoding/202208312002.mp4"
    frames = load_test_video_frames(video_source, max_frames=50)
    if not frames:
        raise RuntimeError("æœªåŠ è½½åˆ°ä»»ä½•å¸§ï¼Œè¯·æ£€æŸ¥è§†é¢‘æº")
    
    # 4. æµå¼æ·»åŠ å¸§
    print("\n" + "="*80)
    print("é˜¶æ®µ 1ï¼šæµå¼æ·»åŠ å¸§")
    print("="*80)
    
    for i, frame in enumerate(frames):
        timestamp = i * 0.5  # å‡è®¾æ¯å¸§é—´éš” 0.5 ç§’
        status = engine.add_frame(frame, timestamp)
        
        # æ¯ 10 å¸§æ‰“å°ä¸€æ¬¡çŠ¶æ€
        if (i + 1) % 10 == 0:
            print(f"  [{i+1}/{len(frames)}] {status}")
    
    print(f"\nâœ… æ‰€æœ‰å¸§å·²æ·»åŠ ")
    
    # 5. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = engine.get_statistics()
    print("\nğŸ“Š å¸§ç®¡ç†ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # 6. æé—®æµ‹è¯•ï¼ˆç¬¬ä¸€æ¬¡ä¼šè§¦å‘ç¼–ç ï¼‰
    print("\n" + "="*80)
    print("é˜¶æ®µ 2ï¼šæé—®æµ‹è¯•")
    print("="*80)
    
    questions = [
        "è¯·æè¿°è§†é¢‘ä¸­çš„ä¸»è¦å†…å®¹ã€‚",
        "è§†é¢‘ä¸­æœ‰ä»€ä¹ˆäººç‰©æˆ–ç‰©ä½“ï¼Ÿ",
        "è§†é¢‘ä¸­æœ‰ä»€ä¹ˆåœºæ™¯å˜åŒ–ï¼Ÿ",
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\nâ“ é—®é¢˜ {i}: {question}")
        answer, metrics = engine.ask(question, max_new_tokens=256)
        
        print(f"ğŸ’¬ å›ç­”: {answer}")
        print(f"ğŸ“Š æŒ‡æ ‡:")
        for key, value in metrics.items():
            if 'latency' in key or 'time' in key:
                print(f"  {key}: {value:.2f}s")
            else:
                print(f"  {key}: {value}")
    
    # 7. æµ‹è¯•å¤šæ¬¡æé—®ï¼ˆcacheå¤ç”¨ï¼‰
    print("\n" + "="*80)
    print("é˜¶æ®µ 3ï¼šCache å¤ç”¨æµ‹è¯•")
    print("="*80)
    
    for i in range(3):
        print(f"\nğŸ”„ ç¬¬ {i+1} æ¬¡æé—®ï¼ˆåº”è¯¥å¤ç”¨ cacheï¼‰")
        question = f"è¿™æ˜¯ç¬¬ {i+1} ä¸ªé—®é¢˜ï¼Œè¯·ç®€è¦å›ç­”è§†é¢‘å†…å®¹ã€‚"
        
        t_start = time.time()
        answer, metrics = engine.ask(question, max_new_tokens=128)
        t_end = time.time()
        
        print(f"ğŸ’¬ å›ç­”: {answer}")
        print(f"â±ï¸  æ€»è€—æ—¶: {t_end - t_start:.2f}s")
        print(f"ğŸ“Š ç¼–ç è€—æ—¶: {metrics.get('encoding_latency', 'N/A (cacheå¤ç”¨)')}")
    
    # 8. æ·»åŠ æ–°å¸§åå†æé—®
    print("\n" + "="*80)
    print("é˜¶æ®µ 4ï¼šæ·»åŠ æ–°å¸§ + é‡æ–°ç¼–ç ")
    print("="*80)
    
    # æ·»åŠ  10 ä¸ªæ–°å¸§
    print("\nâ• æ·»åŠ  10 ä¸ªæ–°å¸§...")
    for i in range(10):
        frame = frames[i % len(frames)]  # å¤ç”¨å·²æœ‰å¸§
        timestamp = len(frames) * 0.5 + i * 0.5
        status = engine.add_frame(frame, timestamp)
    
    print(f"\nâœ… æ–°å¸§å·²æ·»åŠ ")
    
    # å†æ¬¡æé—®ï¼ˆä¼šè§¦å‘é‡æ–°ç¼–ç ï¼‰
    print(f"\nâ“ æ·»åŠ æ–°å¸§åæé—®:")
    question = "ç°åœ¨è§†é¢‘æœ‰æ›´æ–°ï¼Œè¯·æè¿°æœ€æ–°çš„å†…å®¹ã€‚"
    answer, metrics = engine.ask(question, max_new_tokens=256)
    
    print(f"ğŸ’¬ å›ç­”: {answer}")
    print(f"ğŸ“Š æŒ‡æ ‡:")
    for key, value in metrics.items():
        if 'latency' in key or 'time' in key:
            print(f"  {key}: {value:.2f}s")
        else:
            print(f"  {key}: {value}")
    
    # 9. æœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*80)
    print("æœ€ç»ˆç»Ÿè®¡")
    print("="*80)
    
    final_stats = engine.get_statistics()
    print(f"\nğŸ“Š æœ€ç»ˆå¸§ç®¡ç†ç»Ÿè®¡:")
    for key, value in final_stats.items():
        print(f"  {key}: {value}")
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("="*80)


def test_native_vs_delayed():
    """å¯¹æ¯”åŸç”Ÿæ¨ç†ä¸å»¶è¿Ÿæ‰¹é‡ç¼–ç """
    print("="*80)
    print("å¯¹æ¯”æµ‹è¯•ï¼šåŸç”Ÿæ¨ç† vs å»¶è¿Ÿæ‰¹é‡ç¼–ç ")
    print("="*80)
    
    model_path = "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct"
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="cuda",
        trust_remote_code=True,
    )
    
    processor = AutoProcessor.from_pretrained(
        model_path,
        trust_remote_code=True,
    )
    
    # åŠ è½½æµ‹è¯•è§†é¢‘
    video_source = "/root/autodl-tmp/diploma/temporal_encoding/202208312002.mp4"
    frames = load_test_video_frames(video_source, max_frames=30)
    if not frames:
        raise RuntimeError("æœªåŠ è½½åˆ°ä»»ä½•å¸§ï¼Œè¯·æ£€æŸ¥è§†é¢‘æº")
    
    question = "è¯·è¯¦ç»†æè¿°è§†é¢‘ä¸­çš„ä¸»è¦å†…å®¹å’Œåœºæ™¯ã€‚"
    
    # 1. åŸç”Ÿæ¨ç†
    print("\n" + "="*80)
    print("æ–¹æ³• 1ï¼šåŸç”Ÿè§†é¢‘æ¨ç†")
    print("="*80)
    
    messages = [{
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": frames,
                "max_pixels": 4 * 224 * 224,
            },
            {"type": "text", "text": question},
        ],
    }]
    
    text_prompt = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    t_start = time.time()
    inputs = processor(
        text=[text_prompt],
        videos=[frames],
        padding=True,
        return_tensors="pt",
    ).to("cuda")
    
    with torch.inference_mode():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
        )
    
    native_answer = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0].split("assistant\n")[-1]
    
    t_end = time.time()
    
    print(f"ğŸ’¬ åŸç”Ÿå›ç­”: {native_answer}")
    print(f"â±ï¸  è€—æ—¶: {t_end - t_start:.2f}s")
    
    # 2. å»¶è¿Ÿæ‰¹é‡ç¼–ç 
    print("\n" + "="*80)
    print("æ–¹æ³• 2ï¼šå»¶è¿Ÿæ‰¹é‡ç¼–ç ")
    print("="*80)
    
    engine = DelayedBatchInferenceEngine(
        model=model,
        processor=processor,
        device="cuda",
        star_memory_size=20,
        stream_window_size=20,
        max_pixels=2 * 224 * 224,
    )
    
    # æ·»åŠ æ‰€æœ‰å¸§
    for i, frame in enumerate(frames):
        engine.add_frame(frame, i * 0.5)
    
    # æé—®
    t_start = time.time()
    delayed_answer, metrics = engine.ask(question, max_new_tokens=256)
    t_end = time.time()
    
    print(f"ğŸ’¬ å»¶è¿Ÿç¼–ç å›ç­”: {delayed_answer}")
    print(f"â±ï¸  è€—æ—¶: {t_end - t_start:.2f}s")
    print(f"ğŸ“Š è¯¦ç»†æŒ‡æ ‡:")
    for key, value in metrics.items():
        if 'latency' in key or 'time' in key:
            print(f"  {key}: {value:.2f}s")
        else:
            print(f"  {key}: {value}")
    
    # 3. ç›¸ä¼¼åº¦æ¯”è¾ƒ
    print("\n" + "="*80)
    print("ç»“æœå¯¹æ¯”")
    print("="*80)
    
    print(f"\nåŸç”Ÿå›ç­”:\n{native_answer}\n")
    print(f"å»¶è¿Ÿç¼–ç å›ç­”:\n{delayed_answer}\n")
    
    # ç®€å•çš„ç›¸ä¼¼åº¦ä¼°è®¡ï¼ˆåŸºäºé•¿åº¦ï¼‰
    len_ratio = len(delayed_answer) / len(native_answer) if len(native_answer) > 0 else 0
    print(f"ğŸ“ é•¿åº¦æ¯”: {len_ratio:.2f}")
    
    print("\nâœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•å»¶è¿Ÿæ‰¹é‡ç¼–ç æ–¹æ¡ˆ")
    parser.add_argument(
        "--mode",
        choices=["basic", "compare"],
        default="basic",
        help="æµ‹è¯•æ¨¡å¼ï¼šbasicï¼ˆåŸºç¡€æµ‹è¯•ï¼‰, compareï¼ˆå¯¹æ¯”æµ‹è¯•ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.mode == "basic":
        test_delayed_batch_inference()
    elif args.mode == "compare":
        test_native_vs_delayed()
