"""
VideoStreamingInference â€” Streaming VLM Inference (Chunk-Local / Append æ¨¡å¼)

å…³é”®è®¾è®¡ï¼š
  1) é¦–å¸§åŒ…å« system+user+visionï¼Œåç»­å¸§ä»…è¿½åŠ  vision tokens
  2) Position ç”± StreamQwenModel å†…éƒ¨è‡ªåŠ¨è·Ÿè¸ªï¼ˆappend æ¨¡å¼ 3 åˆ†æ”¯ï¼‰
  3) ask()/ask_choice() ä½¿ç”¨ KVCacheManager snapshot/restoreï¼Œ
     åŒæ—¶ä¿å­˜/æ¢å¤æ¨¡å‹çš„ stream_stateï¼Œé˜²æ­¢æ±¡æŸ“è§†é¢‘ç¼“å­˜
  4) ç´¯ç§¯æ‰€æœ‰å†å² KV Cacheï¼Œä¸åš sliding window æˆ– eviction

Chunk-Local å‡è®¾ï¼š
  - ViT åªåœ¨ chunk å†…å»ºæ¨¡ï¼Œè·¨ chunk æ—¶åºç”± LLM+KV+RoPE è´Ÿè´£
  - temporal_patch_size=2ï¼Œæ¯ä¸ª temporal chunk èåˆ 2 å¸§

æ¨è chunk å¤§å°ï¼š
  - 2 å¸§ (as_video=True, fps=1-2): æœ€ä½å»¶è¿Ÿï¼ŒT=1 temporal grid
  - 4 å¸§ (as_video=True, fps=2-4): å»¶è¿Ÿ/è´¨é‡å‡è¡¡æ¨èï¼ŒT=2
  - 6-8 å¸§ (as_video=True, fps=2-4): æ›´é«˜ååï¼Œé€‚åˆå‡†å®æ—¶
  - å•å¸§ image æ¨¡å¼: æœ€ç®€å•ä½†æ•ˆç‡è¾ƒä½ï¼ˆ1å¸§è¢«å¤åˆ¶ä¸º2å¸§å‡‘å¯¹ temporal_patch_sizeï¼‰

æ³¨æ„ï¼š
  - ä¸å†ä½¿ç”¨ manual_time / VideoMetaCalculator
  - è‹¥è¦è¾“å…¥å¤šå¸§ chunkï¼Œè¯·ä½¿ç”¨ as_video=Trueï¼Œå¹¶ä¼ å…¥å¸§åˆ—è¡¨
  - å¤šå¸§ chunk çš„å¸§æ•°å»ºè®®ä¸º temporal_patch_size(2) çš„å€æ•°ï¼Œé¿å…è¢«å¸§å¡«å……
"""

import gc
import time
from typing import List, Optional

import torch

from .cache_manager import KVCacheManager


class VideoStreamingInference:
    def __init__(self, model, processor, device: str = "cuda"):
        self.model = model
        self.processor = processor
        self.device = device

        self.cache_manager = KVCacheManager()
        self.frame_count = 0      # chunk è®¡æ•°
        self.total_frames = 0     # å®é™…å¸§æ•°ç´¯è®¡
        self._system_prompt_added = False

        # ç»Ÿä¸€çš„ç³»ç»Ÿæç¤º
        self.system_prompt = (
            "You are a concise video analyst. Answer briefly and directly. "
            "Focus on visible facts only. Avoid speculation, avoid repetition. "
            "Strictly limit the response to at most 60 tokens."
        )

        print("âœ… VideoStreamingInference Engine Initialized (Chunk-Local / Append Mode).")

    # â”€â”€ Prompt å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _extract_vision_segment(text_prompt: str) -> str:
        """ä» chat template ä¸­è£å‰ªå‡º <|vision_start|>...<|vision_end|> ç‰‡æ®µã€‚"""
        start_tok = "<|vision_start|>"
        end_tok = "<|vision_end|>"
        if start_tok in text_prompt and end_tok in text_prompt:
            head = text_prompt.split(start_tok, 1)[1]
            body = head.split(end_tok, 1)[0]
            return f"{start_tok}{body}{end_tok}"
        return text_prompt

    def _build_frame_prompt(self, as_video: bool, vision_payload, text_content: str) -> str:
        if not self._system_prompt_added:
            messages = [
                {"role": "system", "content": [{"type": "text", "text": self.system_prompt}]},
            ]
            if as_video:
                messages.append(
                    {"role": "user", "content": [
                        {"type": "video", "video": vision_payload},
                        {"type": "text", "text": text_content},
                    ]}
                )
            else:
                messages.append(
                    {"role": "user", "content": [
                        {"type": "image", "image": vision_payload},
                        {"type": "text", "text": text_content},
                    ]}
                )

            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            self._system_prompt_added = True
            return text_prompt

        # åç»­å¸§ï¼šä»…è¿½åŠ è§†è§‰ tokenï¼ˆé¿å…é‡å¤ç³»ç»Ÿ/æ–‡æœ¬ï¼‰
        if as_video:
            messages = [
                {"role": "user", "content": [{"type": "video", "video": vision_payload}]}
            ]
        else:
            messages = [
                {"role": "user", "content": [{"type": "image", "image": vision_payload}]}
            ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        return self._extract_vision_segment(text_prompt)

    # â”€â”€ Reset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def reset(self):
        self.cache_manager.clear()
        self.frame_count = 0
        self.total_frames = 0
        self._system_prompt_added = False
        if hasattr(self.model, "reset_stream_state"):
            self.model.reset_stream_state()
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        print("ğŸ”„ Memory Reset.")

    # â”€â”€ è¿½åŠ å¸§ / Chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def append_frame(
        self,
        image,
        text_content: str = "Frame processed.",
        as_video: bool = False,
        fps: Optional[float] = None,
    ) -> str:
        """
        Phase 1: Stream Encoding (Append)

        Args:
            image: å•å¸§ PIL Imageï¼›æˆ–å½“ as_video=True æ—¶ä¸ºå¸§åˆ—è¡¨ (List[PIL.Image])
            text_content: é¦–å¸§é™„å¸¦çš„æ–‡æœ¬æè¿°ï¼ˆåç»­å¸§è¢«å¿½ç•¥ï¼‰
            as_video: True â†’ ä½¿ç”¨è§†é¢‘ tokenï¼ˆæ¨èå¤šå¸§ chunkï¼‰
            fps: é‡‡æ ·å¸§ç‡ï¼ˆä»… as_video=True æ—¶æœ‰æ•ˆï¼‰
        """
        if as_video and not isinstance(image, (list, tuple)):
            # å…è®¸å•å¸§è§†é¢‘ä½œä¸ºç‰¹ä¾‹
            image = [image]
        if (not as_video) and isinstance(image, (list, tuple)):
            raise ValueError("When passing multiple frames, set as_video=True.")

        # 1) æ„é€  prompt
        text_prompt = self._build_frame_prompt(as_video, image, text_content)

        # 2) Processor è¾“å…¥
        if as_video:
            videos_kwargs = {"fps": fps} if fps is not None else None
            inputs = self.processor(
                text=[text_prompt],
                videos=[image],
                padding=True,
                return_tensors="pt",
                **({"videos_kwargs": videos_kwargs} if videos_kwargs is not None else {}),
            ).to(self.device)
        else:
            inputs = self.processor(
                text=[text_prompt],
                images=[image],
                padding=True,
                return_tensors="pt",
            ).to(self.device)

        # 3) æ„é€  Attention Mask (åŒ…å« past KV cache é•¿åº¦)
        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )
        model_inputs = {k: v for k, v in inputs.items()}
        model_inputs["attention_mask"] = full_mask

        # 4) Forwardï¼ˆposition ç”±æ¨¡å‹å†…éƒ¨è‡ªåŠ¨è®¡ç®—ï¼‰
        with torch.inference_mode():
            outputs = self.model(
                **model_inputs,
                past_key_values=self.cache_manager.cache,
                use_cache=True,
            )
            self.cache_manager.cache = self.cache_manager.detach(outputs.past_key_values)
            del outputs

        self.frame_count += 1
        n_frames = len(image) if as_video and isinstance(image, (list, tuple)) else 1
        self.total_frames += n_frames
        cache_len = self.cache_manager.get_seq_length()
        return f"Chunk {self.frame_count - 1} encoded ({n_frames} frame(s), cache_len={cache_len})"

    # â”€â”€ Ask â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def ask(
        self,
        question: str,
        max_new_tokens: int = 256,
        min_new_tokens: int = 1,
        update_state: bool = False,
        do_sample: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ):
        """
        Phase 2: Interaction (Chunk Prefill + Decode)

        - é—®é¢˜ Prefill â†’ Branch 2 (chunk prefill + offset)
        - é€ token Decode â†’ Branch 3 (last_cache_position + 1)
        """
        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_start = time.time()

        # Snapshot: ä¿æŠ¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.snapshot(self.model)

        # 1) æ„é€ é—®é¢˜ Promptï¼ˆä¸é‡å¤ system promptï¼‰
        messages = [
            {"role": "user", "content": [{"type": "text", "text": question}]}
        ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text_prompt], images=None, padding=True, return_tensors="pt"
        ).to(self.device)

        input_ids = inputs.input_ids

        # 2) æ„é€  Attention Mask (åŒ…å« Video å†å²)
        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )

        current_cache = self.cache_manager.cache

        def _select_token(logits):
            if not do_sample:
                return torch.argmax(logits, dim=-1).unsqueeze(-1)
            temp = max(1e-5, float(temperature))
            scaled = logits / temp
            probs = torch.softmax(scaled, dim=-1)
            if top_p is not None and 0 < top_p < 1:
                sorted_probs, sorted_idx = torch.sort(probs, descending=True)
                cum = torch.cumsum(sorted_probs, dim=-1)
                mask = cum > top_p
                mask[..., 0] = False
                sorted_probs = sorted_probs.masked_fill(mask, 0.0)
                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)
                next_idx = torch.multinomial(sorted_probs, num_samples=1)
                return sorted_idx.gather(-1, next_idx)
            return torch.multinomial(probs, num_samples=1)

        # 3) Prefill
        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=full_mask,
                past_key_values=current_cache,
                use_cache=True,
            )
            current_cache = self.cache_manager.detach(outputs.past_key_values)
            next_token_logits = outputs.logits[:, -1, :]
            next_token = _select_token(next_token_logits)

        # TTFT: é¦– token åœ¨ prefill å®Œæˆåå³å¯ç¡®å®š
        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_first_token = time.time()

        # 4) Decode loop
        generated_ids: List[int] = []
        max_new_tokens = max(1, int(max_new_tokens))
        min_new_tokens = max(1, int(min_new_tokens))
        min_new_tokens = min(min_new_tokens, max_new_tokens)
        eos_token_id = self.processor.tokenizer.eos_token_id

        curr_input = next_token
        last_next_token_logits = next_token_logits
        curr_mask = torch.cat([full_mask, torch.ones((1, 1), device=self.device)], dim=1)

        with torch.inference_mode():
            if curr_input.item() == eos_token_id and min_new_tokens > 0:
                tmp_logits = last_next_token_logits.clone()
                tmp_logits[0, eos_token_id] = -1e9
                curr_input = _select_token(tmp_logits)

            if curr_input.item() != eos_token_id:
                generated_ids.append(curr_input.item())

                outputs = self.model(
                    input_ids=curr_input,
                    attention_mask=curr_mask,
                    past_key_values=current_cache,
                    use_cache=True,
                )

                current_cache = self.cache_manager.detach(outputs.past_key_values)
                next_token_logits = outputs.logits[:, -1, :]
                curr_input = _select_token(next_token_logits)
                last_next_token_logits = next_token_logits

                curr_mask = torch.cat([curr_mask, torch.ones((1, 1), device=self.device)], dim=1)

            for _ in range(max_new_tokens - 1):
                if curr_input.item() == eos_token_id:
                    if len(generated_ids) >= min_new_tokens:
                        break
                    tmp_logits = last_next_token_logits.clone()
                    tmp_logits[0, eos_token_id] = -1e9
                    curr_input = _select_token(tmp_logits)
                generated_ids.append(curr_input.item())

                outputs = self.model(
                    input_ids=curr_input,
                    attention_mask=curr_mask,
                    past_key_values=current_cache,
                    use_cache=True,
                )

                current_cache = self.cache_manager.detach(outputs.past_key_values)
                next_token_logits = outputs.logits[:, -1, :]
                curr_input = _select_token(next_token_logits)
                last_next_token_logits = next_token_logits

                curr_mask = torch.cat([curr_mask, torch.ones((1, 1), device=self.device)], dim=1)

        output_text = self.processor.decode(generated_ids, skip_special_tokens=True)

        if update_state:
            self.cache_manager.cache = current_cache
            self.cache_manager.discard_snapshot()
        else:
            # æ¢å¤ KV Cache + æ¨¡å‹ stream_state
            self.cache_manager.restore(self.model)

        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_end = time.time()

        metrics = {
            "ttft": t_first_token - t_start,
            "total_latency": t_end - t_start,
        }
        return output_text, metrics

    # â”€â”€ Ask Choice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def ask_choice(self, question: str, choices: List[str]):
        """
        Multiple-choice querying with log-prob scoring.

        å¯¹é—®é¢˜åšä¸€æ¬¡ prefillï¼Œç„¶åå¯¹æ¯ä¸ªé€‰é¡¹é€ token ç´¯åŠ  log-probã€‚
        å¤š token é€‰é¡¹ä½¿ç”¨ç‹¬ç«‹ cache å‰¯æœ¬ + ç‹¬ç«‹æ¨¡å‹çŠ¶æ€ã€‚
        """
        # Snapshot: ä¿æŠ¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.snapshot(self.model)

        messages = [
            {"role": "user", "content": [{"type": "text", "text": question}]}
        ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text_prompt], images=None, padding=True, return_tensors="pt"
        ).to(self.device)
        input_ids = inputs.input_ids

        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )

        base_cache = self.cache_manager.cache

        # Prefill é—®é¢˜éƒ¨åˆ† â†’ Branch 2
        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=full_mask,
                past_key_values=base_cache,
                use_cache=True,
            )
            base_cache = self.cache_manager.detach(outputs.past_key_values)
            next_token_logits = outputs.logits[:, -1, :]

        # ä¿å­˜ prefill åçš„æ¨¡å‹çŠ¶æ€ï¼ˆæ¯ä¸ªé€‰é¡¹ä»æ­¤åˆ†å‰ï¼‰
        post_prefill_state = self.model.stream_state

        tokenizer = self.processor.tokenizer
        log_probs = torch.log_softmax(next_token_logits, dim=-1)

        best_choice = None
        best_score = None

        for choice in choices:
            token_ids = tokenizer(choice, add_special_tokens=False).input_ids
            if len(token_ids) == 0:
                continue

            score = log_probs[0, token_ids[0]].item()

            if len(token_ids) > 1:
                # ç‹¬ç«‹ cache å‰¯æœ¬ + ç‹¬ç«‹æ¨¡å‹çŠ¶æ€
                temp_cache = self.cache_manager.clone(base_cache)
                self.model.stream_state = post_prefill_state
                curr_mask = torch.cat([full_mask, torch.ones((1, 1), device=self.device)], dim=1)
                curr_input = torch.tensor([[token_ids[0]]], device=self.device)

                with torch.inference_mode():
                    for tid in token_ids[1:]:
                        outputs = self.model(
                            input_ids=curr_input,
                            attention_mask=curr_mask,
                            past_key_values=temp_cache,
                            use_cache=True,
                        )
                        temp_cache = self.cache_manager.detach(outputs.past_key_values)
                        logits = outputs.logits[:, -1, :]
                        lp = torch.log_softmax(logits, dim=-1)
                        score += lp[0, tid].item()

                        curr_input = torch.tensor([[tid]], device=self.device)
                        curr_mask = torch.cat(
                            [curr_mask, torch.ones((1, 1), device=self.device)], dim=1
                        )

            if best_score is None or score > best_score:
                best_score = score
                best_choice = choice

        # æ¢å¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.restore(self.model)

        return best_choice if best_choice is not None else ""

    # â”€â”€ ä¾¿æ·æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def append_video_chunk(
        self,
        frames: List,
        fps: float = 2.0,
        text_content: str = "Video chunk processed.",
    ) -> str:
        """
        è¿½åŠ å¤šå¸§è§†é¢‘ chunk çš„ä¾¿æ·æ–¹æ³•ã€‚

        Args:
            frames: PIL Image åˆ—è¡¨ï¼Œå»ºè®®å¸§æ•°ä¸º temporal_patch_size(2) çš„å€æ•°
            fps: å¸§ç‡ï¼ˆå½±å“ LLM M-RoPE ä¸­çš„æ—¶é—´ä½ç½®ç¼–ç é—´è·ï¼‰
            text_content: é¦–å¸§é™„å¸¦çš„æ–‡æœ¬æè¿°

        Returns:
            ç¼–ç çŠ¶æ€å­—ç¬¦ä¸²

        æ¨èç”¨æ³•:
            # 2 å¸§ chunk (T=1, æœ€ä½å»¶è¿Ÿ)
            engine.append_video_chunk([frame0, frame1], fps=2.0)

            # 4 å¸§ chunk (T=2, å»¶è¿Ÿ/è´¨é‡å‡è¡¡)
            engine.append_video_chunk([f0, f1, f2, f3], fps=4.0)
        """
        if not isinstance(frames, (list, tuple)) or len(frames) == 0:
            raise ValueError("frames must be a non-empty list of PIL Images.")
        if len(frames) % 2 != 0:
            print(
                f"âš ï¸ {len(frames)} frames is not a multiple of temporal_patch_size=2. "
                f"Last frame will be duplicated by the processor."
            )
        return self.append_frame(frames, text_content=text_content, as_video=True, fps=fps)

    def get_cache_info(self) -> dict:
        """è¿”å›å½“å‰ KV Cache çŠ¶æ€ä¿¡æ¯ã€‚"""
        cache_len = self.cache_manager.get_seq_length()
        mem_mb = 0.0
        cache = self.cache_manager.cache
        if cache is not None:
            if hasattr(cache, "get_seq_length"):
                # DynamicCache: ä¼°ç®— = n_layers Ã— 2(K+V) Ã— seq Ã— heads Ã— dim Ã— dtype_bytes
                try:
                    for kv in cache.key_cache:
                        mem_mb += kv.nelement() * kv.element_size()
                    for kv in cache.value_cache:
                        mem_mb += kv.nelement() * kv.element_size()
                except Exception:
                    pass
            elif isinstance(cache, (tuple, list)):
                for layer in cache:
                    for t in layer:
                        mem_mb += t.nelement() * t.element_size()
            mem_mb /= (1024 * 1024)

        stream_state = None
        if hasattr(self.model, "stream_state"):
            stream_state = {
                "last_cache_position": self.model.stream_state["last_cache_position"],
                "rope_deltas": (
                    self.model.stream_state["rope_deltas"].tolist()
                    if self.model.stream_state["rope_deltas"] is not None
                    else None
                ),
            }

        return {
            "chunks_encoded": self.frame_count,
            "total_frames": self.total_frames,
            "cache_seq_length": cache_len,
            "cache_memory_mb": round(mem_mb, 2),
            "stream_state": stream_state,
        }
