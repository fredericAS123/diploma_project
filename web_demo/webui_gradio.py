import gradio as gr
import threading
import time
import torch
import numpy as np
from PIL import Image
from Qwen_inference import QwenInferenceWrapper

try:
    from decord import VideoReader, cpu
except ImportError:
    VideoReader = None
    cpu = None

class HistorySynchronizer:
    def __init__(self):
        self.chat_history = []

    def get_chat_history(self):
        return self.chat_history

    def update(self, role, content):
        self.chat_history.append(gr.ChatMessage(role=role, content=str(content)))

    def add_message(self, role, content):
        self.chat_history.append(gr.ChatMessage(role=role, content=str(content)))
        return len(self.chat_history) - 1

    def update_message(self, idx, role, content):
        if 0 <= idx < len(self.chat_history):
            self.chat_history[idx] = gr.ChatMessage(role=role, content=str(content))

    def reset(self):
        self.chat_history = []

class VideoChatWebUI:
    def __init__(self, inference_engine: QwenInferenceWrapper):
        """
        åˆå§‹åŒ– UI é€»è¾‘ï¼Œä¼ å…¥æ¨ç†å¼•æ“å®ä¾‹ã€‚
        å·²é€‚é…æ–°çš„æµå¼å¼•æ“ APIï¼ˆæ—  manual_timeï¼Œæ”¯æŒ chunk ç¼–ç ï¼‰ã€‚
        """
        self.inference_engine = inference_engine
        self.history_synchronizer = HistorySynchronizer()
        
        # [æ§åˆ¶æ ‡å¿—]
        self.pause_event = threading.Event()
        self.pause_event.set() 
        self.stop_signal = False 
        self.is_streaming = False            
        self.current_streaming_time = 0.0
        
        # [æ•°æ®ç¼“å­˜]
        self.cached_video_path = None 
        self.cached_video_data = None 

        # [åŸç”Ÿæ¨ç†æ¨¡å¼]
        self.mode = "streaming"  # "streaming" or "native"
        self.native_frame_buffer = []
        self.native_fps = 2.0
        self.last_streaming_metrics = {}
        self.last_native_metrics = {}
        self.last_single_frame_metrics = {}
        self.last_frame_pil = None
        self._last_eviction_notified = 0
        self.stream_frame_interval = 1.0
        self.stream_target_fps = 1.0
        self.stream_chunk_size = 4
        self._chunk_token_history = []
        self._prev_cache_len = None
        self._prev_total_evicted_tokens = 0
        self._mem_history = []
        self._mem_t0 = time.perf_counter()

    def _format_cache_status(self, cache_info):
        """æ ¼å¼åŒ– KV Cache çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…å«æ·˜æ±°ç»Ÿè®¡ã€‚"""
        parts = [
            f"Seq: {cache_info['cache_seq_length']}",
            f"Mem: {cache_info['cache_memory_gb']:.2f}GB",
            f"Frames: {cache_info['total_frames']}",
        ]
        evict = cache_info.get('eviction_stats', {})
        if evict:
            n_evictions = evict.get('total_evictions', 0)
            if n_evictions > 0:
                total_evicted = evict.get('total_tokens_evicted', 0)
                evicted_str = f"{total_evicted // 1000}K" if total_evicted >= 1000 else str(total_evicted)
                parts.append(f"âœ‚ï¸ {n_evictions}æ¬¡(-{evicted_str})")
            sink = evict.get('effective_sink_size')
            window = evict.get('effective_window_size')
            if sink and sink > 0:
                parts.append(f"Sink:{sink}")
            if window and window > 0:
                parts.append(f"Win:{window}")

            time_span_str = self._estimate_cached_time_span(cache_info)
            if time_span_str:
                parts.append(time_span_str)

        return " | ".join(parts)

    def _reset_stream_estimation_state(self):
        self._chunk_token_history = []
        self._prev_cache_len = None
        self._prev_total_evicted_tokens = 0

    def _record_chunk_token_delta(self, cache_info, chunk_start_t: float, chunk_end_t: float):
        """è®°å½•æ¯ä¸ªchunkçš„è¿‘ä¼¼tokenå¢é‡ï¼Œç”¨äºæ›´å‡†ç¡®ä¼°ç®—çª—å£æ—¶é—´è¦†ç›–ã€‚"""
        cache_len = int(cache_info.get('cache_seq_length', 0))
        evict = cache_info.get('eviction_stats', {})
        total_evicted = int(evict.get('total_tokens_evicted', 0)) if evict else 0

        evicted_delta = max(0, total_evicted - self._prev_total_evicted_tokens)
        if self._prev_cache_len is None:
            new_chunk_tokens = cache_len
        else:
            # cache_after = cache_before + new_chunk - evicted_delta
            new_chunk_tokens = cache_len - self._prev_cache_len + evicted_delta

        new_chunk_tokens = max(1, int(new_chunk_tokens))
        self._chunk_token_history.append(
            {
                "start_t": float(chunk_start_t),
                "end_t": float(chunk_end_t),
                "tokens": new_chunk_tokens,
            }
        )

        self._prev_cache_len = cache_len
        self._prev_total_evicted_tokens = total_evicted

    def _estimate_cached_time_span(self, cache_info) -> str:
        """åŸºäºchunk tokenå†å²ä¼°ç®—å½“å‰cacheè¦†ç›–çš„è§†é¢‘æ—¶é—´åŒºé—´ã€‚"""
        if not self._chunk_token_history:
            return ""

        evict = cache_info.get('eviction_stats', {})
        if not evict:
            return ""

        cache_seq = float(cache_info.get('cache_seq_length', 0))
        if cache_seq <= 0:
            return ""

        sink_tokens = float(evict.get('effective_sink_size') or 0)
        sink_tokens_in_mem = min(sink_tokens, cache_seq)
        window_tokens_in_mem = max(0.0, cache_seq - sink_tokens_in_mem)

        # Sink æ—¶é—´èŒƒå›´ï¼šä¸¥æ ¼æŒ‰â€œé¦–chunkæ—¶é—´èŒƒå›´â€å±•ç¤ºï¼Œé¿å…tokenå‡å€¼æ¢ç®—è¯¯å·®
        first_chunk = self._chunk_token_history[0]
        sink_start_t = max(0.0, float(first_chunk['start_t']))
        sink_end_t = max(sink_start_t, float(first_chunk['end_t']))

        # Window èµ·ç‚¹ï¼šæŒ‰â€œå°¾éƒ¨ token é¢„ç®—â€ä»æœ€è¿‘chunkå‘å‰å›æ¨
        now_t = max(0.0, float(self.current_streaming_time))
        if window_tokens_in_mem <= 0:
            return f"Timeâ‰ˆSink[{sink_start_t:.1f}-{sink_end_t:.1f}s]"

        acc = 0.0
        win_start_t = now_t
        for rec in reversed(self._chunk_token_history):
            tok = max(1.0, float(rec['tokens']))
            st = float(rec['start_t'])
            ed = float(rec['end_t'])

            if acc + tok >= window_tokens_in_mem:
                need = window_tokens_in_mem - acc
                ratio = max(0.0, min(1.0, need / tok))
                win_start_t = ed - (ed - st) * ratio
                break

            acc += tok
            win_start_t = st

        win_start_t = max(0.0, min(win_start_t, now_t))
        return f"Timeâ‰ˆSink[{sink_start_t:.1f}-{sink_end_t:.1f}s]+Win[{win_start_t:.1f}-{now_t:.1f}s]"

    def _format_fps_display(self, throughput_fps: float) -> str:
        """æ ¼å¼åŒ–ç¼–ç ååæ˜¾ç¤ºï¼šframes/s + ç›¸å¯¹é‡‡æ ·è§†é¢‘çš„å®æ—¶å€ç‡ã€‚"""
        throughput_fps = max(0.0, float(throughput_fps))
        if self.stream_target_fps > 0:
            rt_factor = throughput_fps / self.stream_target_fps
            return f"{throughput_fps:.2f} (x{rt_factor:.2f} realtime)"
        return f"{throughput_fps:.2f}"

    def _reset_memory_monitor(self):
        self._mem_history = []
        self._mem_t0 = time.perf_counter()

    def _build_memory_wave_html(self):
        if not self._mem_history:
            return "<div style='color:#999;'>ç­‰å¾…æ˜¾å­˜æ•°æ®...</div>"

        data = self._mem_history[-120:]
        w, h = 520, 160
        pad_l, pad_r, pad_t, pad_b = 38, 8, 8, 20
        pw = w - pad_l - pad_r
        ph = h - pad_t - pad_b

        max_y = max(max(d["allocated"], d["model"], d["kv"], d["runtime"]) for d in data)
        max_y = max(0.1, max_y * 1.1)

        def xy(i, y):
            x = pad_l + (i / max(1, len(data) - 1)) * pw
            yy = pad_t + (1 - y / max_y) * ph
            return x, yy

        def poly(points, color, width=2):
            return f"<polyline fill='none' stroke='{color}' stroke-width='{width}' points='{points}'/>"

        def make_points(key):
            pts = [xy(i, d[key]) for i, d in enumerate(data)]
            return " ".join([f"{x:.1f},{y:.1f}" for x, y in pts])

        grid = []
        for v in [0.25, 0.5, 0.75, 1.0]:
            yy = pad_t + (1 - v) * ph
            val = max_y * v
            grid.append(f"<line x1='{pad_l}' y1='{yy:.1f}' x2='{w-pad_r}' y2='{yy:.1f}' stroke='#2a2a2a' stroke-width='1' />")
            grid.append(f"<text x='2' y='{yy+4:.1f}' fill='#888' font-size='10'>{val:.1f}GB</text>")

        svg = [
            f"<svg width='{w}' height='{h}' viewBox='0 0 {w} {h}' style='background:#111;border:1px solid #333;border-radius:6px;'>",
            "".join(grid),
            poly(make_points("allocated"), "#f59e0b", 2),
            poly(make_points("model"), "#38bdf8", 2),
            poly(make_points("kv"), "#22c55e", 2),
            poly(make_points("runtime"), "#ef4444", 2),
            "</svg>",
            "<div style='font-size:12px;margin-top:4px;'>"
            "<span style='color:#f59e0b'>â–  Alloc</span> "
            "<span style='color:#38bdf8'>â–  Model</span> "
            "<span style='color:#22c55e'>â–  KV</span> "
            "<span style='color:#ef4444'>â–  Runtime/Act</span>"
            "</div>",
        ]
        return "".join(svg)

    def _update_memory_monitor(self, cache_info=None):
        if cache_info is None:
            cache_info = self.inference_engine.get_cache_info()
        mem = self.inference_engine.get_memory_breakdown(cache_info)
        t_rel = time.perf_counter() - self._mem_t0
        self._mem_history.append(
            {
                "t": t_rel,
                "model": float(mem.get("model_gb", 0.0)),
                "kv": float(mem.get("kv_cache_gb", 0.0)),
                "runtime": float(mem.get("runtime_gb", 0.0)),
                "allocated": float(mem.get("allocated_gb", 0.0)),
                "reserved": float(mem.get("reserved_gb", 0.0)),
            }
        )
        if len(self._mem_history) > 240:
            self._mem_history = self._mem_history[-240:]

        mem_str = (
            f"Model: {mem.get('model_gb', 0.0):.2f}GB | "
            f"KV: {mem.get('kv_cache_gb', 0.0):.2f}GB | "
            f"Runtime/Act: {mem.get('runtime_gb', 0.0):.2f}GB | "
            f"Alloc: {mem.get('allocated_gb', 0.0):.2f}GB | "
            f"Reserved: {mem.get('reserved_gb', 0.0):.2f}GB"
        )
        return mem_str, self._build_memory_wave_html()

    def generate_answer(self, question):
        """æ‰‹åŠ¨æé—®ï¼šæ”¯æŒæµå¼ã€åŸç”Ÿã€å•å¸§ä¸‰æ¨¡å¼å¯¹æ¯”"""

        # â”€â”€â”€ åŸç”Ÿæ¨ç†æ¨¡å¼ â”€â”€â”€
        if self.mode == "native":
            if not self.native_frame_buffer:
                yield (self.history_synchronizer.get_chat_history() + [
                    gr.ChatMessage(role="assistant", content="è¯·å…ˆåŠ è½½è§†é¢‘å¸§ï¼ˆç‚¹å‡» Start Nativeï¼‰")
                ], "", "", "", "", "")
                return

            auto_paused = False
            if self.is_streaming and self.pause_event.is_set():
                self.pause_event.clear()
                auto_paused = True
                time.sleep(0.1)

            self.history_synchronizer.update("user", question)
            n_frames = len(self.native_frame_buffer)
            self._reset_memory_monitor()
            mem_str, mem_wave = self._update_memory_monitor()
            yield (self.history_synchronizer.get_chat_history() + [
                gr.ChatMessage(role="assistant",
                    content=f"ğŸ”„ æ­£åœ¨è¿›è¡ŒåŒæ¨¡å¼å¯¹æ¯”æ¨ç† ({n_frames} å¸§ vs å•å¸§)...")
            ], "", "", "", mem_str, mem_wave)

            # 1) åŸç”Ÿæ¨ç†ï¼ˆå…¨éƒ¨å¸§ï¼‰
            try:
                native_response, native_metrics = self.inference_engine.native_video_inference(
                    frames=self.native_frame_buffer,
                    question=question,
                    fps=self.native_fps,
                    max_new_tokens=256,
                    min_new_tokens=8,
                    do_sample=False,
                )
                self.last_native_metrics = native_metrics
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                native_response = (
                    "âš ï¸ OOM: åŸç”Ÿæ¨¡å¼æ˜¾å­˜ä¸è¶³ï¼è¿™æ­£è¯´æ˜æµå¼æ¨ç†çš„ä¼˜åŠ¿â€”â€”"
                    "åŸç”Ÿæ¨¡å¼éœ€è¦ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å¸§ï¼Œæ˜¾å­˜éœ€æ±‚è¿œé«˜äºæµå¼æ¨¡å¼ã€‚"
                )
                native_metrics = {"ttft": 0.0, "vram_peak_gb": 0.0, "num_frames": n_frames, "input_tokens": 0}
                self.last_native_metrics = native_metrics

            # 2) å•å¸§æ¨ç†ï¼ˆä»…æœ€åä¸€å¸§ï¼‰
            last_frame = self.native_frame_buffer[-1]
            try:
                sf_response, sf_metrics = self.inference_engine.single_frame_inference(
                    image=last_frame,
                    question=question,
                    max_new_tokens=256,
                    min_new_tokens=8,
                    do_sample=False,
                )
                self.last_single_frame_metrics = sf_metrics
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                sf_response = "âš ï¸ å•å¸§æ¨ç† OOM"
                sf_metrics = {"ttft": 0.0, "vram_peak_gb": 0.0, "input_tokens": 0}
                self.last_single_frame_metrics = sf_metrics

            self.history_synchronizer.update(
                "assistant",
                f"ğŸ“¦ [åŸç”Ÿæ¨ç† - å…¨éƒ¨ {n_frames} å¸§]\n{native_response}"
            )
            self.history_synchronizer.update(
                "assistant",
                f"ğŸ–¼ï¸ [å•å¸§æ¨ç† - ä»…æœ€åä¸€å¸§]\n{sf_response}"
            )

            native_info = (
                f"[Native] Frames: {native_metrics.get('num_frames', 0)} | "
                f"Tokens: {native_metrics.get('input_tokens', 0)} | "
                f"VRAM Peak: {native_metrics.get('vram_peak_gb', 0):.2f}GB"
            )
            comparison = self._build_comparison_text()
            mem_str, mem_wave = self._update_memory_monitor()
            if auto_paused:
                self.pause_event.set()
            yield (self.history_synchronizer.get_chat_history(),
                   f"{native_metrics.get('ttft', 0):.3f}", native_info, comparison, mem_str, mem_wave)
            return

        # â”€â”€â”€ æµå¼æ¨ç†æ¨¡å¼ â”€â”€â”€
        if not self.is_streaming and self.current_streaming_time <= 0:
            yield (self.history_synchronizer.get_chat_history() + [
                gr.ChatMessage(role="assistant", content="è¯·å…ˆæ’­æ”¾è§†é¢‘")
            ], "", "", "", "", "")
            return

        # è‡ªåŠ¨æš‚åœæœºåˆ¶
        auto_paused = False
        if self.is_streaming and self.pause_event.is_set():
            print("âš ï¸ Detected streaming active. Auto-pausing for manual question...")
            self.pause_event.clear()
            auto_paused = True
            time.sleep(0.1)

        self.history_synchronizer.update("user", question)
        self._reset_memory_monitor()
        mem_str, mem_wave = self._update_memory_monitor()
        
        status_msg = "ğŸ”„ æ­£åœ¨è¿›è¡ŒåŒæ¨¡å¼å¯¹æ¯”æ¨ç† (æµå¼ + å•å¸§)..."
        if auto_paused:
            status_msg += " (è§†é¢‘å·²è‡ªåŠ¨æš‚åœ)"
            
        yield self.history_synchronizer.get_chat_history() + [gr.ChatMessage(role="assistant", content=status_msg)], "", "", "", mem_str, mem_wave

        try:
            # é‡ç½® VRAM å³°å€¼ç»Ÿè®¡
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()

            assistant_prefix = "ğŸ“¡ [æµå¼æ¨ç† - KV Cache å¤ç”¨]\n"
            assistant_idx = self.history_synchronizer.add_message("assistant", assistant_prefix)
            final_text = ""
            ttft_stream = 0.0
            metrics = {"ttft": 0.0, "total_latency": 0.0}

            for event in self.inference_engine.ask_question_stream(
                question,
                max_new_tokens=256,
                min_new_tokens=8,
                do_sample=False,
                temperature=0.7,
                top_p=0.9,
            ):
                if event.get("type") == "token":
                    final_text = event.get("text", final_text)
                    if event.get("ttft") is not None:
                        ttft_stream = float(event["ttft"])
                    self.history_synchronizer.update_message(
                        assistant_idx,
                        "assistant",
                        assistant_prefix + final_text,
                    )
                    cache_info = self.inference_engine.get_cache_info()
                    cache_str = self._format_cache_status(cache_info)
                    mem_str, mem_wave = self._update_memory_monitor(cache_info)
                    yield self.history_synchronizer.get_chat_history(), f"{ttft_stream:.3f}", cache_str, "", mem_str, mem_wave
                elif event.get("type") == "final":
                    final_text = event.get("text", final_text)
                    metrics = event.get("metrics", metrics)

            if torch.cuda.is_available():
                metrics["vram_peak_gb"] = round(
                    torch.cuda.max_memory_allocated() / (1024 ** 3), 3
                )
            self.last_streaming_metrics = metrics
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            response = "Error: OOM (æ˜¾å­˜ä¸è¶³)ã€‚è¯·å°è¯•å‡å°‘è§†é¢‘å¸§æ•°ã€‚"
            metrics = {"ttft": 0.0}
            self.history_synchronizer.update("assistant", f"ğŸ“¡ [æµå¼æ¨ç† - KV Cache å¤ç”¨]\n{response}")
            mem_str, mem_wave = self._update_memory_monitor()
            yield self.history_synchronizer.get_chat_history(), "0.000", "", "", mem_str, mem_wave
            return

        # å•å¸§æ¨ç†å¯¹æ¯”
        sf_response = ""
        sf_metrics = {}
        if self.last_frame_pil is not None:
            try:
                sf_response, sf_metrics = self.inference_engine.single_frame_inference(
                    image=self.last_frame_pil,
                    question=question,
                    max_new_tokens=256,
                    min_new_tokens=8,
                    do_sample=False,
                )
                self.last_single_frame_metrics = sf_metrics
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                sf_response = "âš ï¸ å•å¸§æ¨ç† OOM"
                sf_metrics = {"ttft": 0.0, "vram_peak_gb": 0.0, "input_tokens": 0}
                self.last_single_frame_metrics = sf_metrics

        if sf_response:
            self.history_synchronizer.update("assistant", f"ğŸ–¼ï¸ [å•å¸§æ¨ç† - ä»…æœ€åä¸€å¸§]\n{sf_response}")

        ttft_val = metrics.get("ttft", 0.0)

        # è·å–ç¼“å­˜ä¿¡æ¯
        cache_info = self.inference_engine.get_cache_info()
        cache_str = self._format_cache_status(cache_info)
        comparison = self._build_comparison_text()
        mem_str, mem_wave = self._update_memory_monitor(cache_info)
        yield self.history_synchronizer.get_chat_history(), f"{ttft_val:.3f}", cache_str, comparison, mem_str, mem_wave

    def start_chat(self, video_path, frame_interval, chunk_size, enable_eviction, max_cache_tokens, current_history):
        """
        æ™ºèƒ½å¯åŠ¨ï¼šæµå¼ç¼–ç è§†é¢‘å¸§ã€‚
        
        æ”¯æŒ chunk ç¼–ç æ¨¡å¼ï¼š
          - chunk_size=1: é€å¸§ image æ¨¡å¼
          - chunk_size=2,4,6...: å¤šå¸§ video chunk æ¨¡å¼ï¼ˆæ¨èï¼‰
        """
        if not video_path:
            raise gr.Error("Please upload a video file.")

        if VideoReader is None:
            raise gr.Error("Missing dependency: decord. Please install it first.")

        # 1. å¼ºåˆ¶é‡ç½®æ ‡å¿—ä½ï¼ˆåˆ‡æ¢åˆ°æµå¼æ¨¡å¼ï¼‰
        self.mode = "streaming"
        self.native_frame_buffer = []
        self.pause_event.set()
        self.stop_signal = False 
        self.current_streaming_time = 0.0

        if self.inference_engine is None:
            self.inference_engine = QwenInferenceWrapper()

        self.inference_engine.reset_with_eviction(
            enable_eviction=bool(enable_eviction),
            max_cache_tokens=int(max_cache_tokens),
        )
        self._last_eviction_notified = 0
        self._reset_stream_estimation_state()
        self._reset_memory_monitor()

        print(f"ğŸ”„ Loading video for streaming encode: {video_path} "
              f"(eviction={'ON, max=' + str(int(max_cache_tokens)) if enable_eviction else 'OFF'})")
        torch.cuda.empty_cache()

        vr = VideoReader(video_path, ctx=cpu(0))
        avg_fps = vr.get_avg_fps()
        target_fps = 1.0 / frame_interval
        step = max(1, int(avg_fps / target_fps))
        frame_indices = np.arange(0, len(vr), step)

        self.stream_frame_interval = float(frame_interval)
        self.stream_target_fps = float(target_fps)
        self.stream_chunk_size = int(chunk_size)

        self.history_synchronizer.reset()
        fps_display = 0.0
        chunk_size = max(1, int(chunk_size))
        
        print(f"ğŸš€ å¼€å§‹æµå¼ç¼–ç ï¼Œæ€»å¸§æ•°: {len(frame_indices)}, chunk_size={chunk_size}")
        self.inference_engine.log_vram("Start-Encode")
        self.is_streaming = True
        stopped_early = False
        
        try:
            last_timestamp = 0.0
            last_frame = None
            frame_buffer = []  # ç´¯ç§¯å¸§ç”¨äº chunk ç¼–ç 
            time_buffer = []

            for idx, frame_index in enumerate(frame_indices):
                if self.stop_signal:
                    print("ğŸ›‘ Inference loop stopped by user.")
                    stopped_early = True
                    break

                if not self.pause_event.is_set():
                    self.pause_event.wait()
                    if self.stop_signal:
                        stopped_early = True
                        break
                
                img_np = vr[frame_index].asnumpy()
                img_pil = Image.fromarray(img_np)
                self.last_frame_pil = img_pil
                timestamp = frame_index / avg_fps

                frame_buffer.append(img_pil)
                time_buffer.append(timestamp)

                # å½“ buffer æ»¡æ—¶è¿›è¡Œ chunk ç¼–ç 
                if len(frame_buffer) >= chunk_size:
                    inference_start = time.perf_counter()

                    if chunk_size == 1:
                        # å•å¸§ image æ¨¡å¼
                        response = self.inference_engine.process_frame(frame_buffer[0])
                    else:
                        # å¤šå¸§ video chunk æ¨¡å¼
                        chunk_fps = target_fps  # ä½¿ç”¨é‡‡æ ·åçš„å¸§ç‡
                        response = self.inference_engine.process_video_chunk(
                            frame_buffer, fps=chunk_fps
                        )

                    encoded_count = len(frame_buffer)  # è®°å½•ç¼–ç å¸§æ•°ï¼ˆæ¸…ç©ºå‰ï¼‰
                    chunk_start_t = time_buffer[0]
                    chunk_end_t = time_buffer[-1]
                    frame_buffer = []
                    time_buffer = []
                    self.current_streaming_time = timestamp

                    if idx % 10 == 0:
                        self.inference_engine.log_vram(f"Encode-{idx}")

                    self.history_synchronizer.update(
                        "assistant",
                        f"System: [T={timestamp:.1f}s] {response}"
                    )

                    # æ£€æŸ¥æ·˜æ±°äº‹ä»¶å¹¶åœ¨é¦–æ¬¡è§¦å‘æ—¶é€šçŸ¥
                    evict_info = self.inference_engine.get_cache_info().get('eviction_stats', {})
                    cur_evict_n = evict_info.get('total_evictions', 0)
                    if cur_evict_n > 0 and self._last_eviction_notified == 0:
                        self._last_eviction_notified = cur_evict_n
                        self.history_synchronizer.update(
                            "assistant",
                            f"System: âœ‚ï¸ KV Cache æ·˜æ±°é¦–æ¬¡è§¦å‘ï¼"
                            f"Sink={evict_info.get('effective_sink_size', '?')}, "
                            f"Window={evict_info.get('effective_window_size', '?')}"
                        )

                    # è®°å½•chunk tokenå¢é‡ç”¨äºæ—¶é—´èŒƒå›´ä¼°ç®—
                    cache_info_after_chunk = self.inference_engine.get_cache_info()
                    self._record_chunk_token_delta(
                        cache_info_after_chunk,
                        chunk_start_t=chunk_start_t,
                        chunk_end_t=chunk_end_t,
                    )

                    cost_time = time.perf_counter() - inference_start
                    if cost_time > 0:
                        fps_display = encoded_count / cost_time

                last_timestamp = timestamp
                last_frame = img_np

                # è·å–ç¼“å­˜ä¿¡æ¯ï¼ˆå«æ·˜æ±°ç»Ÿè®¡ï¼‰
                cache_info = self.inference_engine.get_cache_info()
                cache_str = self._format_cache_status(cache_info)
                mem_str, mem_wave = self._update_memory_monitor(cache_info)
                yield timestamp, img_np, None, self.history_synchronizer.get_chat_history(), self._format_fps_display(fps_display), cache_str, mem_str, mem_wave

            # å¤„ç†å‰©ä½™å¸§
            if frame_buffer and not self.stop_signal:
                if chunk_size == 1:
                    self.inference_engine.process_frame(frame_buffer[0])
                else:
                    self.inference_engine.process_video_chunk(frame_buffer, fps=target_fps)
                cache_info_after_chunk = self.inference_engine.get_cache_info()
                self._record_chunk_token_delta(
                    cache_info_after_chunk,
                    chunk_start_t=time_buffer[0],
                    chunk_end_t=time_buffer[-1],
                )
                frame_buffer = []
                time_buffer = []

        except Exception as e:
            print(f"Runtime Error: {e}")
            raise e
        finally:
            self.is_streaming = False
            self.stop_signal = False 
            self.inference_engine.log_vram("Finished")
            
        if not stopped_early and last_frame is not None:
            cache_info = self.inference_engine.get_cache_info()
            cache_str = self._format_cache_status(cache_info)
            mem_str, mem_wave = self._update_memory_monitor(cache_info)
            yield last_timestamp, last_frame, None, self.history_synchronizer.get_chat_history(), self._format_fps_display(fps_display), cache_str, mem_str, mem_wave

    def _build_comparison_text(self):
        """æ„å»ºæµå¼ vs åŸç”Ÿ vs å•å¸§ä¸‰æ¨¡å¼å¯¹æ¯”æ–‡æœ¬"""
        s = self.last_streaming_metrics
        n = self.last_native_metrics
        f = self.last_single_frame_metrics

        if not s and not n and not f:
            return ""

        lines = ["â•â•â•â•â•â• ä¸‰æ¨¡å¼æ¨ç†å¯¹æ¯” â•â•â•â•â•â•"]

        if s:
            s_vram_str = f"{s['vram_peak_gb']}GB" if 'vram_peak_gb' in s else "N/A"
            lines.append(f"ğŸ“¡ æµå¼: TTFT={s.get('ttft', 0):.3f}s | VRAM Peak={s_vram_str}")
        else:
            lines.append("ğŸ“¡ æµå¼: (å°šæœªæµ‹è¯•ï¼Œè¯·å…ˆè¿è¡Œ Start Streaming)")

        if n:
            lines.append(
                f"ğŸ“¦ åŸç”Ÿ: TTFT={n.get('ttft', 0):.3f}s | "
                f"VRAM Peak={n.get('vram_peak_gb', 0):.2f}GB | "
                f"Input Tok={n.get('input_tokens', 0)}"
            )
        else:
            lines.append("ğŸ“¦ åŸç”Ÿ: (å°šæœªæµ‹è¯•ï¼Œè¯·å…ˆè¿è¡Œ Start Native)")

        if f:
            lines.append(
                f"ğŸ–¼ï¸ å•å¸§: TTFT={f.get('ttft', 0):.3f}s | "
                f"VRAM Peak={f.get('vram_peak_gb', 0):.2f}GB | "
                f"Input Tok={f.get('input_tokens', 0)}"
            )
        else:
            lines.append("ğŸ–¼ï¸ å•å¸§: (å°šæœªæµ‹è¯•)")

        lines.append("â”€â”€â”€ å¯¹æ¯”åˆ†æ â”€â”€â”€")

        # æµå¼ vs åŸç”Ÿ
        if s and n and s.get('ttft', 0) > 0 and n.get('ttft', 0) > 0:
            speedup = n['ttft'] / s['ttft']
            lines.append(f"ğŸš€ æµå¼ vs åŸç”Ÿ TTFT åŠ é€Ÿ: {speedup:.1f}x")

            s_vram = s.get('vram_peak_gb')
            n_vram = n.get('vram_peak_gb')
            if (isinstance(s_vram, (int, float)) and isinstance(n_vram, (int, float))
                    and s_vram > 0 and n_vram > 0):
                savings = (1 - s_vram / n_vram) * 100
                lines.append(f"ğŸ’¾ æµå¼ vs åŸç”Ÿ VRAM èŠ‚çœ: {savings:.1f}% ({n_vram:.2f}GB â†’ {s_vram:.2f}GB)")

        # è§†é¢‘ç†è§£ vs å•å¸§
        if f and f.get('ttft', 0) > 0:
            if s and s.get('ttft', 0) > 0:
                ratio = f['ttft'] / s['ttft']
                lines.append(f"âš¡ æµå¼ vs å•å¸§ TTFT: Ã—{ratio:.1f} (æµå¼ç•¥æ…¢ä½†æ‹¥æœ‰å®Œæ•´æ—¶åºç†è§£)")
            if n and n.get('ttft', 0) > 0:
                ratio = n['ttft'] / f['ttft']
                lines.append(f"ğŸ¢ åŸç”Ÿ vs å•å¸§ TTFT: Ã—{ratio:.1f} (åŸç”Ÿå¤„ç†å…¨å¸§ï¼Œå»¶è¿Ÿè¿œé«˜äºå•å¸§)")

        if (s or n) and f:
            lines.append("â”€â”€â”€ ç»“è®º â”€â”€â”€")
            lines.append("ğŸ¬ è§†é¢‘ç†è§£(æµå¼/åŸç”Ÿ)åˆ©ç”¨å¤šå¸§æ—¶åºä¿¡æ¯ï¼Œå›ç­”æ›´å‡†ç¡®å…¨é¢")
            lines.append("ğŸ–¼ï¸ å•å¸§ä»…çœ‹åˆ°ä¸€å¸§ç”»é¢ï¼Œç¼ºä¹æ—¶åºåŠ¨æ€ç†è§£èƒ½åŠ›")
            if s:
                lines.append("âœ… æµå¼æ¨ç†ï¼šæ¥è¿‘å•å¸§çš„é€Ÿåº¦ + å®Œæ•´è§†é¢‘çš„ç†è§£åŠ› = æœ€ä½³æ–¹æ¡ˆ")

        return "\n".join(lines)

    def start_native_chat(self, video_path, frame_interval, current_history):
        """
        åŸç”Ÿæ¨ç†æ¨¡å¼å¯åŠ¨ï¼šæå–å¸§å¹¶ç¼“å­˜ï¼Œä¸åšå®æ—¶ç¼–ç ã€‚
        ç”¨æˆ·æé—®æ—¶ä¸€æ¬¡æ€§å°†æ‰€æœ‰å¸§é€å…¥æ¨¡å‹æ¨ç†ã€‚
        """
        if not video_path:
            raise gr.Error("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶ã€‚")
        if VideoReader is None:
            raise gr.Error("ç¼ºå°‘ä¾èµ–: decordã€‚è¯·å…ˆå®‰è£…ã€‚")

        # åˆ‡æ¢åˆ°åŸç”Ÿæ¨¡å¼
        self.mode = "native"
        self.native_frame_buffer = []
        self.pause_event.set()
        self.stop_signal = False
        self.current_streaming_time = 0.0
        self.history_synchronizer.reset()
        self._reset_stream_estimation_state()
        self._reset_memory_monitor()

        # é‡ç½®æµå¼å¼•æ“çŠ¶æ€
        if self.inference_engine is not None:
            self.inference_engine.reset()
        torch.cuda.empty_cache()

        vr = VideoReader(video_path, ctx=cpu(0))
        avg_fps = vr.get_avg_fps()
        target_fps = 1.0 / frame_interval
        step = max(1, int(avg_fps / target_fps))
        frame_indices = np.arange(0, len(vr), step)

        self.stream_frame_interval = float(frame_interval)
        self.stream_target_fps = float(target_fps)
        self.stream_chunk_size = 1
        self.native_fps = target_fps
        self.is_streaming = True
        stopped_early = False

        print(f"ğŸ“¹ [Native Mode] æå– {len(frame_indices)} å¸§ (ä»…ç¼“å­˜ï¼Œä¸ç¼–ç )")

        last_frame = None
        fps_display = 0.0

        try:
            for idx, frame_index in enumerate(frame_indices):
                if self.stop_signal:
                    print("ğŸ›‘ Native frame extraction stopped by user.")
                    stopped_early = True
                    break
                if not self.pause_event.is_set():
                    self.pause_event.wait()
                    if self.stop_signal:
                        stopped_early = True
                        break

                extract_start = time.perf_counter()
                img_np = vr[frame_index].asnumpy()
                img_pil = Image.fromarray(img_np)
                timestamp = frame_index / avg_fps

                self.native_frame_buffer.append(img_pil)
                self.current_streaming_time = timestamp
                last_frame = img_np

                extract_cost = time.perf_counter() - extract_start
                if extract_cost > 0:
                    fps_display = 1.0 / extract_cost

                if idx == 0:
                    self.history_synchronizer.update(
                        "assistant",
                        "System: [Native Mode] ğŸ“¦ å¼€å§‹æå–å¸§... (ä¸è¿›è¡Œæ¨¡å‹ç¼–ç )"
                    )

                status = (
                    f"[Native] å·²ç¼“å­˜: {len(self.native_frame_buffer)}/{len(frame_indices)} å¸§ | "
                    f"T={timestamp:.1f}s | æ— æ¨¡å‹ç¼–ç "
                )
                mem_str, mem_wave = self._update_memory_monitor()
                yield (timestamp, img_np, None,
                       self.history_synchronizer.get_chat_history(),
                      self._format_fps_display(fps_display), status, mem_str, mem_wave)

        except Exception as e:
            print(f"Native extraction error: {e}")
            raise e
        finally:
            self.is_streaming = False
            self.stop_signal = False

        if stopped_early:
            self.history_synchronizer.update(
                "assistant",
                "System: â¹ï¸ [Native] å·²åœæ­¢å¸§æå–ã€‚"
            )
            if last_frame is not None:
                final_status = f"[Native] å·²åœæ­¢: å·²ç¼“å­˜ {len(self.native_frame_buffer)} å¸§"
                mem_str, mem_wave = self._update_memory_monitor()
                yield (self.current_streaming_time, last_frame, None,
                       self.history_synchronizer.get_chat_history(),
                                             self._format_fps_display(fps_display), final_status, mem_str, mem_wave)
            return

        # å®Œæˆæ¶ˆæ¯
        n_frames = len(self.native_frame_buffer)
        self.history_synchronizer.update(
            "assistant",
            f"System: âœ… [Native] å¸§ç¼“å­˜å®Œæˆ: {n_frames} å¸§ | FPS={self.native_fps:.1f}\n"
            f"ğŸ’¡ è¯·åœ¨å³ä¾§è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹å°†ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ {n_frames} å¸§è¿›è¡Œæ¨ç†ã€‚"
        )

        if last_frame is not None:
            final_status = (
                f"[Native] âœ… Ready: {n_frames} frames | "
                f"FPS={self.native_fps:.1f} | ç­‰å¾…æé—®"
            )
            mem_str, mem_wave = self._update_memory_monitor()
            yield (self.current_streaming_time, last_frame, None,
                   self.history_synchronizer.get_chat_history(),
                   self._format_fps_display(fps_display), final_status, mem_str, mem_wave)

    def toggle_pause(self):
        if self.pause_event.is_set():
            self.pause_event.clear()
            return "Resume Video", self.history_synchronizer.get_chat_history()
        else:
            self.pause_event.set()
            return "Pause Video", self.history_synchronizer.get_chat_history()

    def stop_chat(self):
        print("ğŸ›‘ Stop command received.")
        self.stop_signal = True 
        self.pause_event.set() 
        time.sleep(0.1)
        self.is_streaming = False
        self.current_streaming_time = 0.0
        self._last_eviction_notified = 0
        self._reset_stream_estimation_state()
        self._reset_memory_monitor()
        self.native_frame_buffer = []
        self.last_frame_pil = None
        self.last_single_frame_metrics = {}
        self.mode = "streaming"
        if self.inference_engine is not None:
            self.inference_engine.reset()
        return 0, None, None, [], "0.00", "", "", "", "", "<div style='color:#999;'>ç­‰å¾…æ˜¾å­˜æ•°æ®...</div>"

    def create_interface(self):
        with gr.Blocks(title="Qwen2.5-VL Streaming Video Chat") as demo:
            pil_frames_state = gr.State()

            gr.Markdown("# ğŸ¬ Qwen2.5-VL Streaming Video Chat\næµå¼è§†é¢‘ç†è§£ + å®æ—¶é—®ç­” | æ”¯æŒæµå¼ vs åŸç”Ÿæ¨ç†å¯¹æ¯”")

            with gr.Row():
                with gr.Column(scale=3):
                    gr_frame_display = gr.Image(label="Current Frame", interactive=False, height=400)
                    with gr.Row():
                        gr_time_display = gr.Number(label="Video Time (s)", value=0)
                        gr_fps_display = gr.Textbox(label="Encode Throughput (frames/s)", value="0.00")
                        gr_ttft_display = gr.Textbox(label="Ask TTFT (s)", value="")
                    with gr.Row():
                        gr_cache_display = gr.Textbox(label="KV Cache Status", value="", interactive=False)
                    with gr.Row():
                        gr_mem_display = gr.Textbox(label="GPU Memory Breakdown (GB)", value="", interactive=False)
                    with gr.Row():
                        gr_mem_wave = gr.HTML(value="<div style='color:#999;'>ç­‰å¾…æ˜¾å­˜æ•°æ®...</div>")
                    with gr.Row():
                        gr_pause_button = gr.Button("Pause Video")
                        gr_stop_button = gr.Button("Stop Video", variant="stop")

                with gr.Column(scale=2):
                    gr_chat_interface = gr.Chatbot(label="Chat History", height=500)
                    gr_question_input = gr.Textbox(label="Manual Question (Auto-pauses video)")

            with gr.Row():
                with gr.Column():
                    gr_video_upload = gr.Video(label="1. Upload Video")
                with gr.Column():
                    gr_frame_interval = gr.Slider(
                        minimum=0.1, maximum=5.0, step=0.1, value=1.0,
                        interactive=True, label="2. Frame Interval (s)"
                    )
                    gr_chunk_size = gr.Slider(
                        minimum=1, maximum=8, step=1, value=4,
                        interactive=True, label="3. Chunk Size (frames, æ¨è 2/4/6)"
                    )
                    gr.Markdown("ç¼–ç åå=æ¨¡å‹ç¼–ç é€Ÿåº¦ï¼ˆframes/sï¼‰ï¼Œæ‹¬å·å†…ä¸ºç›¸å¯¹é‡‡æ ·è§†é¢‘é€Ÿç‡çš„å®æ—¶å€ç‡ã€‚å°äº 1 é€šå¸¸è¡¨ç¤ºç¼–ç é€Ÿåº¦ä½äºå®æ—¶æ’­æ”¾é€Ÿåº¦ã€‚")
                    gr.Markdown("#### âœ‚ï¸ KV Cache æ·˜æ±° (Sink + Sliding Window)")
                    with gr.Row():
                        gr_eviction_enable = gr.Checkbox(
                            label="å¯ç”¨æ·˜æ±° (é˜² OOMï¼Œæ”¯æŒæ— é™é•¿è§†é¢‘)",
                            value=True,
                            interactive=True,
                        )
                        gr_max_cache_tokens = gr.Slider(
                            minimum=50000, maximum=200000, step=10000, value=150000,
                            interactive=True,
                            label="Max Cache Tokens (è¶Šå¤§ä¿ç•™è¶Šå¤šå†å²ï¼ŒVRAMè¶Šé«˜)",
                        )
                    with gr.Row():
                        gr_start_button = gr.Button("4a. Start Streaming â–¶ï¸", variant="primary")
                        gr_start_native_button = gr.Button("4b. Start Native ğŸ“¦", variant="secondary")

            with gr.Row():
                gr_comparison_display = gr.Textbox(
                    label="ğŸ“Š ä¸‰æ¨¡å¼å¯¹æ¯”: Streaming vs Native vs Single-Frame",
                    value="åˆ†åˆ«è¿è¡Œæµå¼å’ŒåŸç”Ÿæ¨¡å¼åï¼Œæé—®æ—¶è‡ªåŠ¨å¯¹æ¯”ä¸‰ç§æ¨ç†æ¨¡å¼",
                    lines=8,
                    interactive=False,
                )

            gr_question_input.submit(
                self.generate_answer,
                inputs=[gr_question_input],
                outputs=[gr_chat_interface, gr_ttft_display, gr_cache_display, gr_comparison_display, gr_mem_display, gr_mem_wave],
                queue=True,
            )
            gr_start_button.click(
                self.start_chat,
                inputs=[gr_video_upload, gr_frame_interval, gr_chunk_size,
                        gr_eviction_enable, gr_max_cache_tokens, gr_chat_interface],
                outputs=[gr_time_display, gr_frame_display, pil_frames_state,
                         gr_chat_interface, gr_fps_display, gr_cache_display, gr_mem_display, gr_mem_wave],
            )
            gr_start_native_button.click(
                self.start_native_chat,
                inputs=[gr_video_upload, gr_frame_interval, gr_chat_interface],
                outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display, gr_cache_display, gr_mem_display, gr_mem_wave],
            )
            gr_pause_button.click(
                self.toggle_pause,
                inputs=[],
                outputs=[gr_pause_button, gr_chat_interface],
            )
            gr_stop_button.click(
                self.stop_chat,
                inputs=[],
                outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display, gr_ttft_display, gr_cache_display, gr_comparison_display, gr_mem_display, gr_mem_wave],
            )
        
        return demo
