"""
VideoStreamingInference â€” Streaming VLM Inference (Chunk-Local / Append æ¨¡å¼)

å…³é”®è®¾è®¡ï¼š
  1) é¦–å¸§åŒ…å« system+user+visionï¼Œåç»­å¸§ä»…è¿½åŠ  vision tokens
  2) Position ç”± StreamQwenModel å†…éƒ¨è‡ªåŠ¨è·Ÿè¸ªï¼ˆappend æ¨¡å¼ 3 åˆ†æ”¯ï¼‰
  3) ask()/ask_choice() ä½¿ç”¨ KVCacheManager snapshot/restoreï¼Œ
     åŒæ—¶ä¿å­˜/æ¢å¤æ¨¡å‹çš„ stream_stateï¼Œé˜²æ­¢æ±¡æŸ“è§†é¢‘ç¼“å­˜
  4) (v2) æ”¯æŒ KV Cache æ·˜æ±°ç­–ç•¥ï¼Œæ§åˆ¶æ˜¾å­˜å¢é•¿ï¼Œå®ç°æ— é™é•¿åº¦è§†é¢‘æµ
     å‚è€ƒ StreamingVLM (MIT-HAN-Lab) + StreamingLLM + LOOK-M

Chunk-Local å‡è®¾ï¼š
  - ViT åªåœ¨ chunk å†…å»ºæ¨¡ï¼Œè·¨ chunk æ—¶åºç”± LLM+KV+RoPE è´Ÿè´£
  - temporal_patch_size=2ï¼Œæ¯ä¸ª temporal chunk èåˆ 2 å¸§

æ¨è chunk å¤§å°ï¼š
  - 2 å¸§ (as_video=True, fps=1-2): æœ€ä½å»¶è¿Ÿï¼ŒT=1 temporal grid
  - 4 å¸§ (as_video=True, fps=2-4): å»¶è¿Ÿ/è´¨é‡å‡è¡¡æ¨èï¼ŒT=2
  - 6-8 å¸§ (as_video=True, fps=2-4): æ›´é«˜ååï¼Œé€‚åˆå‡†å®æ—¶
  - å•å¸§ image æ¨¡å¼: æœ€ç®€å•ä½†æ•ˆç‡è¾ƒä½ï¼ˆ1å¸§è¢«å¤åˆ¶ä¸º2å¸§å‡‘å¯¹ temporal_patch_sizeï¼‰

æ³¨æ„ï¼š
  - ä¸å†ä½¿ç”¨ manual_time / VideoMetaCalculator
  - è‹¥è¦è¾“å…¥å¤šå¸§ chunkï¼Œè¯·ä½¿ç”¨ as_video=Trueï¼Œå¹¶ä¼ å…¥å¸§åˆ—è¡¨
  - å¤šå¸§ chunk çš„å¸§æ•°å»ºè®®ä¸º temporal_patch_size(2) çš„å€æ•°ï¼Œé¿å…è¢«å¸§å¡«å……
"""

import gc
import time
from typing import List, Optional

import torch

from .cache_manager import KVCacheManager
from .kv_cache_eviction import EvictionConfig


class VideoStreamingInference:
    def __init__(
        self,
        model,
        processor,
        device: str = "cuda",
        eviction_config: Optional[EvictionConfig] = None,
    ):
        self.model = model
        self.processor = processor
        self.device = device

        self.cache_manager = KVCacheManager(eviction_config=eviction_config)
        self.frame_count = 0      # chunk è®¡æ•°
        self.total_frames = 0     # å®é™…å¸§æ•°ç´¯è®¡
        self._system_prompt_added = False
        self._chunk_counter = 0   # æ·˜æ±°é—´éš”è®¡æ•°å™¨

        # ç»Ÿä¸€çš„ç³»ç»Ÿæç¤º
        self.system_prompt = (
            "You are a concise video analyst. Answer briefly and directly. "
            "Focus on visible facts only. Avoid speculation, avoid repetition. "
            "Strictly limit the response to at most 60 tokens."
        )

        eviction_str = "OFF"
        if eviction_config is not None:
            sink_str = "auto" if eviction_config.sink_size == 0 else str(eviction_config.sink_size)
            win_str = "auto" if eviction_config.window_size == 0 else str(eviction_config.window_size)
            eviction_str = (
                f"ON (max={eviction_config.max_cache_tokens}, "
                f"sink={sink_str}, window={win_str})"
            )
        print(f"âœ… VideoStreamingInference Engine Initialized (Chunk-Local / Append Mode).")
        print(f"   KV Cache Eviction: {eviction_str}")

    # â”€â”€ Prompt å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _extract_vision_segment(text_prompt: str) -> str:
        """ä» chat template ä¸­è£å‰ªå‡º <|vision_start|>...<|vision_end|> ç‰‡æ®µã€‚"""
        start_tok = "<|vision_start|>"
        end_tok = "<|vision_end|>"
        if start_tok in text_prompt and end_tok in text_prompt:
            head = text_prompt.split(start_tok, 1)[1]
            body = head.split(end_tok, 1)[0]
            return f"{start_tok}{body}{end_tok}"
        return text_prompt

    @staticmethod
    def _extract_user_vision_turn(text_prompt: str) -> str:
        """
        ä» chat template ä¸­æå–ç”¨æˆ· turn ä¸­çš„è§†è§‰å†…å®¹ï¼Œä¿ç•™å¯¹è¯ç»“æ„æ ‡è®°ã€‚

        è¿”å›: <|im_start|>user\n<|vision_start|>...<|vision_end|><|im_end|>\n
        è¿™æ ·åç»­ chunk çš„ token åˆ†å¸ƒä¸é¦–å¸§ç›¸ä¼¼ï¼Œå‡å°‘ OOD é™è´¨ã€‚
        """
        im_start = "<|im_start|>"
        im_end = "<|im_end|>"
        vision_start = "<|vision_start|>"
        vision_end = "<|vision_end|>"

        # æ‰¾åˆ°åŒ…å« vision çš„ user turn
        if vision_start in text_prompt and vision_end in text_prompt:
            # æå– vision segment
            head = text_prompt.split(vision_start, 1)[1]
            body = head.split(vision_end, 1)[0]
            vision_seg = f"{vision_start}{body}{vision_end}"
            # åŒ…è£¹åœ¨ user turn ç»“æ„ä¸­
            return f"{im_start}user\n{vision_seg}{im_end}\n"

        return text_prompt

    def _build_frame_prompt(self, as_video: bool, vision_payload, text_content: str) -> str:
        if not self._system_prompt_added:
            messages = [
                {"role": "system", "content": [{"type": "text", "text": self.system_prompt}]},
            ]
            if as_video:
                messages.append(
                    {"role": "user", "content": [
                        {"type": "video", "video": vision_payload},
                        {"type": "text", "text": text_content},
                    ]}
                )
            else:
                messages.append(
                    {"role": "user", "content": [
                        {"type": "image", "image": vision_payload},
                        {"type": "text", "text": text_content},
                    ]}
                )

            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            self._system_prompt_added = True
            return text_prompt

        # åç»­å¸§ï¼šä¿ç•™ <|im_start|>user\n...<|im_end|> å¯¹è¯ç»“æ„åŒ…è£¹
        # å‡å°‘è£¸ vision token å¸¦æ¥çš„ OOD æ•ˆåº”
        if as_video:
            messages = [
                {"role": "user", "content": [{"type": "video", "video": vision_payload}]}
            ]
        else:
            messages = [
                {"role": "user", "content": [{"type": "image", "image": vision_payload}]}
            ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        return self._extract_user_vision_turn(text_prompt)

    # â”€â”€ Reset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def reset(self):
        self.cache_manager.clear()
        self.frame_count = 0
        self.total_frames = 0
        self._system_prompt_added = False
        self._chunk_counter = 0
        if hasattr(self.model, "reset_stream_state"):
            self.model.reset_stream_state()
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        print("ğŸ”„ Memory Reset.")

    # â”€â”€ è¿½åŠ å¸§ / Chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def append_frame(
        self,
        image,
        text_content: str = "Frame processed.",
        as_video: bool = False,
        fps: Optional[float] = None,
    ) -> str:
        """
        Phase 1: Stream Encoding (Append)

        Args:
            image: å•å¸§ PIL Imageï¼›æˆ–å½“ as_video=True æ—¶ä¸ºå¸§åˆ—è¡¨ (List[PIL.Image])
            text_content: é¦–å¸§é™„å¸¦çš„æ–‡æœ¬æè¿°ï¼ˆåç»­å¸§è¢«å¿½ç•¥ï¼‰
            as_video: True â†’ ä½¿ç”¨è§†é¢‘ tokenï¼ˆæ¨èå¤šå¸§ chunkï¼‰
            fps: é‡‡æ ·å¸§ç‡ï¼ˆä»… as_video=True æ—¶æœ‰æ•ˆï¼‰
        """
        if as_video and not isinstance(image, (list, tuple)):
            # å…è®¸å•å¸§è§†é¢‘ä½œä¸ºç‰¹ä¾‹
            image = [image]
        if (not as_video) and isinstance(image, (list, tuple)):
            raise ValueError("When passing multiple frames, set as_video=True.")

        # 1) æ„é€  prompt
        text_prompt = self._build_frame_prompt(as_video, image, text_content)

        # 2) Processor è¾“å…¥
        if as_video:
            videos_kwargs = {"fps": fps} if fps is not None else None
            inputs = self.processor(
                text=[text_prompt],
                videos=[image],
                padding=True,
                return_tensors="pt",
                **({"videos_kwargs": videos_kwargs} if videos_kwargs is not None else {}),
            ).to(self.device)
        else:
            inputs = self.processor(
                text=[text_prompt],
                images=[image],
                padding=True,
                return_tensors="pt",
            ).to(self.device)

        # 3) æ„é€  Attention Mask (åŒ…å« past KV cache é•¿åº¦)
        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )
        model_inputs = {k: v for k, v in inputs.items()}
        model_inputs["attention_mask"] = full_mask

        # 4) Forwardï¼ˆposition ç”±æ¨¡å‹å†…éƒ¨è‡ªåŠ¨è®¡ç®—ï¼‰
        with torch.inference_mode():
            outputs = self.model(
                **model_inputs,
                past_key_values=self.cache_manager.cache,
                use_cache=True,
            )
            self.cache_manager.cache = self.cache_manager.detach(outputs.past_key_values)
            del outputs

        # 5) Token Tracking (Level 2/3 éœ€è¦)
        if "input_ids" in inputs:
            self.cache_manager.track_tokens(inputs["input_ids"], is_new_chunk=True)

        # 6) é¦– chunk è‡ªåŠ¨æ£€æµ‹ sink_size
        cache_len_after = self.cache_manager.get_seq_length()
        if self.cache_manager.eviction_enabled and self.frame_count == 0:
            self.cache_manager.set_first_chunk_info(cache_len_after)
            evictor = self.cache_manager.evictor
            if evictor is not None:
                print(
                    f"   \U0001f4cd Auto-sink detected: first chunk = {cache_len_after} tokens"
                    f" (sink={evictor.effective_sink_size}, window={evictor.effective_window_size})"
                )
            self._prev_cache_len = cache_len_after
        elif self.cache_manager.eviction_enabled:
            # æ›´æ–° chunk ç»Ÿè®¡
            chunk_tokens = cache_len_after - getattr(self, '_prev_cache_len', 0)
            if chunk_tokens > 0 and self.cache_manager.evictor is not None:
                self.cache_manager.evictor.update_chunk_stats(chunk_tokens)
            self._prev_cache_len = cache_len_after

        # 7) KV Cache Eviction (å¦‚æœå¯ç”¨)
        eviction_info = {"evicted": False}
        if self.cache_manager.eviction_enabled:
            self._chunk_counter += 1
            evictor = self.cache_manager.evictor
            if evictor is not None:
                interval = evictor.config.eviction_interval
                if self._chunk_counter % interval == 0:
                    eviction_info = self.cache_manager.evict_if_needed()
                    if eviction_info.get("evicted"):
                        print(
                            f"  âœ‚ï¸ Eviction: {eviction_info['tokens_before']} â†’ "
                            f"{eviction_info['tokens_after']} tokens "
                            f"(-{eviction_info['tokens_removed']})"
                        )

        self.frame_count += 1
        n_frames = len(image) if as_video and isinstance(image, (list, tuple)) else 1
        self.total_frames += n_frames
        cache_len = self.cache_manager.get_seq_length()
        return f"Chunk {self.frame_count - 1} encoded ({n_frames} frame(s), cache_len={cache_len})"

    # â”€â”€ Ask â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def ask(
        self,
        question: str,
        max_new_tokens: int = 256,
        min_new_tokens: int = 1,
        update_state: bool = False,
        do_sample: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ):
        """
        Phase 2: Interaction (Chunk Prefill + Decode)

        - é—®é¢˜ Prefill â†’ Branch 2 (chunk prefill + offset)
        - é€ token Decode â†’ Branch 3 (last_cache_position + 1)
        """
        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_start = time.time()

        # Snapshot: ä¿æŠ¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.snapshot(self.model)

        # 1) æ„é€ é—®é¢˜ Promptï¼ˆä¸é‡å¤ system promptï¼‰
        messages = [
            {"role": "user", "content": [{"type": "text", "text": question}]}
        ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text_prompt], images=None, padding=True, return_tensors="pt"
        ).to(self.device)

        input_ids = inputs.input_ids

        # 2) æ„é€  Attention Mask (åŒ…å« Video å†å²)
        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )

        current_cache = self.cache_manager.cache

        def _select_token(logits):
            if not do_sample:
                return torch.argmax(logits, dim=-1).unsqueeze(-1)
            temp = max(1e-5, float(temperature))
            scaled = logits / temp
            probs = torch.softmax(scaled, dim=-1)
            if top_p is not None and 0 < top_p < 1:
                sorted_probs, sorted_idx = torch.sort(probs, descending=True)
                cum = torch.cumsum(sorted_probs, dim=-1)
                mask = cum > top_p
                mask[..., 0] = False
                sorted_probs = sorted_probs.masked_fill(mask, 0.0)
                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)
                next_idx = torch.multinomial(sorted_probs, num_samples=1)
                return sorted_idx.gather(-1, next_idx)
            return torch.multinomial(probs, num_samples=1)

        # 3) Prefill
        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=full_mask,
                past_key_values=current_cache,
                use_cache=True,
            )
            current_cache = self.cache_manager.detach(outputs.past_key_values)
            next_token_logits = outputs.logits[:, -1, :]
            next_token = _select_token(next_token_logits)

        # TTFT: é¦– token åœ¨ prefill å®Œæˆåå³å¯ç¡®å®š
        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_first_token = time.time()

        # 4) Decode loop
        generated_ids: List[int] = []
        max_new_tokens = max(1, int(max_new_tokens))
        min_new_tokens = max(1, int(min_new_tokens))
        min_new_tokens = min(min_new_tokens, max_new_tokens)
        eos_token_id = self.processor.tokenizer.eos_token_id

        curr_input = next_token
        last_next_token_logits = next_token_logits
        curr_mask = torch.cat([full_mask, torch.ones((1, 1), device=self.device)], dim=1)

        with torch.inference_mode():
            if curr_input.item() == eos_token_id and min_new_tokens > 0:
                tmp_logits = last_next_token_logits.clone()
                tmp_logits[0, eos_token_id] = -1e9
                curr_input = _select_token(tmp_logits)

            if curr_input.item() != eos_token_id:
                generated_ids.append(curr_input.item())

                outputs = self.model(
                    input_ids=curr_input,
                    attention_mask=curr_mask,
                    past_key_values=current_cache,
                    use_cache=True,
                )

                current_cache = self.cache_manager.detach(outputs.past_key_values)
                next_token_logits = outputs.logits[:, -1, :]
                curr_input = _select_token(next_token_logits)
                last_next_token_logits = next_token_logits

                curr_mask = torch.cat([curr_mask, torch.ones((1, 1), device=self.device)], dim=1)

            for _ in range(max_new_tokens - 1):
                if curr_input.item() == eos_token_id:
                    if len(generated_ids) >= min_new_tokens:
                        break
                    tmp_logits = last_next_token_logits.clone()
                    tmp_logits[0, eos_token_id] = -1e9
                    curr_input = _select_token(tmp_logits)
                generated_ids.append(curr_input.item())

                outputs = self.model(
                    input_ids=curr_input,
                    attention_mask=curr_mask,
                    past_key_values=current_cache,
                    use_cache=True,
                )

                current_cache = self.cache_manager.detach(outputs.past_key_values)
                next_token_logits = outputs.logits[:, -1, :]
                curr_input = _select_token(next_token_logits)
                last_next_token_logits = next_token_logits

                curr_mask = torch.cat([curr_mask, torch.ones((1, 1), device=self.device)], dim=1)

        output_text = self.processor.decode(generated_ids, skip_special_tokens=True)

        if update_state:
            self.cache_manager.cache = current_cache
            self.cache_manager.discard_snapshot()
        else:
            # æ¢å¤ KV Cache + æ¨¡å‹ stream_state
            self.cache_manager.restore(self.model)

        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_end = time.time()

        metrics = {
            "ttft": t_first_token - t_start,
            "total_latency": t_end - t_start,
        }
        return output_text, metrics

    def ask_stream(
        self,
        question: str,
        max_new_tokens: int = 256,
        min_new_tokens: int = 1,
        update_state: bool = False,
        do_sample: bool = False,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ):
        """
        æµå¼é—®ç­”ï¼šæŒ‰ token å¢é‡äº§å‡ºæ–‡æœ¬ï¼Œæœ€åè¿”å›å®Œæ•´ metricsã€‚

        Yields:
            {
              "type": "token", "delta": str, "text": str, "ttft": float|None
            }
            {
              "type": "final", "text": str, "metrics": {...}
            }
        """
        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_start = time.time()

        # Snapshot: ä¿æŠ¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.snapshot(self.model)

        messages = [{"role": "user", "content": [{"type": "text", "text": question}]}]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text_prompt], images=None, padding=True, return_tensors="pt"
        ).to(self.device)

        input_ids = inputs.input_ids
        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )
        current_cache = self.cache_manager.cache

        def _select_token(logits):
            if not do_sample:
                return torch.argmax(logits, dim=-1).unsqueeze(-1)
            temp = max(1e-5, float(temperature))
            scaled = logits / temp
            probs = torch.softmax(scaled, dim=-1)
            if top_p is not None and 0 < top_p < 1:
                sorted_probs, sorted_idx = torch.sort(probs, descending=True)
                cum = torch.cumsum(sorted_probs, dim=-1)
                mask = cum > top_p
                mask[..., 0] = False
                sorted_probs = sorted_probs.masked_fill(mask, 0.0)
                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)
                next_idx = torch.multinomial(sorted_probs, num_samples=1)
                return sorted_idx.gather(-1, next_idx)
            return torch.multinomial(probs, num_samples=1)

        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=full_mask,
                past_key_values=current_cache,
                use_cache=True,
            )
            current_cache = self.cache_manager.detach(outputs.past_key_values)
            next_token_logits = outputs.logits[:, -1, :]
            next_token = _select_token(next_token_logits)

        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_first_token = time.time()

        generated_ids: List[int] = []
        max_new_tokens = max(1, int(max_new_tokens))
        min_new_tokens = max(1, int(min_new_tokens))
        min_new_tokens = min(min_new_tokens, max_new_tokens)
        eos_token_id = self.processor.tokenizer.eos_token_id

        curr_input = next_token
        last_next_token_logits = next_token_logits
        curr_mask = torch.cat([full_mask, torch.ones((1, 1), device=self.device)], dim=1)

        streamed_text = ""
        ttft_sent = False

        with torch.inference_mode():
            if curr_input.item() == eos_token_id and min_new_tokens > 0:
                tmp_logits = last_next_token_logits.clone()
                tmp_logits[0, eos_token_id] = -1e9
                curr_input = _select_token(tmp_logits)

            if curr_input.item() != eos_token_id:
                generated_ids.append(curr_input.item())
                new_text = self.processor.decode(generated_ids, skip_special_tokens=True)
                delta = new_text[len(streamed_text):]
                streamed_text = new_text
                if delta:
                    yield {
                        "type": "token",
                        "delta": delta,
                        "text": streamed_text,
                        "ttft": (t_first_token - t_start) if not ttft_sent else None,
                    }
                    ttft_sent = True

                outputs = self.model(
                    input_ids=curr_input,
                    attention_mask=curr_mask,
                    past_key_values=current_cache,
                    use_cache=True,
                )

                current_cache = self.cache_manager.detach(outputs.past_key_values)
                next_token_logits = outputs.logits[:, -1, :]
                curr_input = _select_token(next_token_logits)
                last_next_token_logits = next_token_logits
                curr_mask = torch.cat([curr_mask, torch.ones((1, 1), device=self.device)], dim=1)

            for _ in range(max_new_tokens - 1):
                if curr_input.item() == eos_token_id:
                    if len(generated_ids) >= min_new_tokens:
                        break
                    tmp_logits = last_next_token_logits.clone()
                    tmp_logits[0, eos_token_id] = -1e9
                    curr_input = _select_token(tmp_logits)

                generated_ids.append(curr_input.item())
                new_text = self.processor.decode(generated_ids, skip_special_tokens=True)
                delta = new_text[len(streamed_text):]
                streamed_text = new_text
                if delta:
                    yield {
                        "type": "token",
                        "delta": delta,
                        "text": streamed_text,
                        "ttft": None,
                    }

                outputs = self.model(
                    input_ids=curr_input,
                    attention_mask=curr_mask,
                    past_key_values=current_cache,
                    use_cache=True,
                )

                current_cache = self.cache_manager.detach(outputs.past_key_values)
                next_token_logits = outputs.logits[:, -1, :]
                curr_input = _select_token(next_token_logits)
                last_next_token_logits = next_token_logits
                curr_mask = torch.cat([curr_mask, torch.ones((1, 1), device=self.device)], dim=1)

        output_text = self.processor.decode(generated_ids, skip_special_tokens=True)

        if update_state:
            self.cache_manager.cache = current_cache
            self.cache_manager.discard_snapshot()
        else:
            self.cache_manager.restore(self.model)

        if self.device.startswith("cuda"):
            torch.cuda.synchronize()
        t_end = time.time()

        metrics = {
            "ttft": t_first_token - t_start,
            "total_latency": t_end - t_start,
        }
        yield {
            "type": "final",
            "text": output_text,
            "metrics": metrics,
        }

    # â”€â”€ Ask Choice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def ask_choice(self, question: str, choices: List[str]):
        """
        Multiple-choice querying with log-prob scoring.

        å¯¹é—®é¢˜åšä¸€æ¬¡ prefillï¼Œç„¶åå¯¹æ¯ä¸ªé€‰é¡¹é€ token ç´¯åŠ  log-probã€‚
        å¤š token é€‰é¡¹ä½¿ç”¨ç‹¬ç«‹ cache å‰¯æœ¬ + ç‹¬ç«‹æ¨¡å‹çŠ¶æ€ã€‚
        """
        # Snapshot: ä¿æŠ¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.snapshot(self.model)

        messages = [
            {"role": "user", "content": [{"type": "text", "text": question}]}
        ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text_prompt], images=None, padding=True, return_tensors="pt"
        ).to(self.device)
        input_ids = inputs.input_ids

        full_mask = self.cache_manager.build_full_attention_mask(
            inputs.attention_mask,
            cache_override=self.cache_manager.cache,
        )

        base_cache = self.cache_manager.cache

        # Prefill é—®é¢˜éƒ¨åˆ† â†’ Branch 2
        with torch.inference_mode():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=full_mask,
                past_key_values=base_cache,
                use_cache=True,
            )
            base_cache = self.cache_manager.detach(outputs.past_key_values)
            next_token_logits = outputs.logits[:, -1, :]

        # ä¿å­˜ prefill åçš„æ¨¡å‹çŠ¶æ€ï¼ˆæ¯ä¸ªé€‰é¡¹ä»æ­¤åˆ†å‰ï¼‰
        post_prefill_state = self.model.stream_state

        tokenizer = self.processor.tokenizer
        log_probs = torch.log_softmax(next_token_logits, dim=-1)

        best_choice = None
        best_score = None

        for choice in choices:
            token_ids = tokenizer(choice, add_special_tokens=False).input_ids
            if len(token_ids) == 0:
                continue

            score = log_probs[0, token_ids[0]].item()

            if len(token_ids) > 1:
                # ç‹¬ç«‹ cache å‰¯æœ¬ + ç‹¬ç«‹æ¨¡å‹çŠ¶æ€
                temp_cache = self.cache_manager.clone(base_cache)
                self.model.stream_state = post_prefill_state
                curr_mask = torch.cat([full_mask, torch.ones((1, 1), device=self.device)], dim=1)
                curr_input = torch.tensor([[token_ids[0]]], device=self.device)

                with torch.inference_mode():
                    for tid in token_ids[1:]:
                        outputs = self.model(
                            input_ids=curr_input,
                            attention_mask=curr_mask,
                            past_key_values=temp_cache,
                            use_cache=True,
                        )
                        temp_cache = self.cache_manager.detach(outputs.past_key_values)
                        logits = outputs.logits[:, -1, :]
                        lp = torch.log_softmax(logits, dim=-1)
                        score += lp[0, tid].item()

                        curr_input = torch.tensor([[tid]], device=self.device)
                        curr_mask = torch.cat(
                            [curr_mask, torch.ones((1, 1), device=self.device)], dim=1
                        )

            if best_score is None or score > best_score:
                best_score = score
                best_choice = choice

        # æ¢å¤è§†é¢‘ KV Cache + æ¨¡å‹ stream_state
        self.cache_manager.restore(self.model)

        return best_choice if best_choice is not None else ""

    # â”€â”€ ä¾¿æ·æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def append_video_chunk(
        self,
        frames: List,
        fps: float = 2.0,
        text_content: str = "Video chunk processed.",
    ) -> str:
        """
        è¿½åŠ å¤šå¸§è§†é¢‘ chunk çš„ä¾¿æ·æ–¹æ³•ã€‚

        Args:
            frames: PIL Image åˆ—è¡¨ï¼Œå»ºè®®å¸§æ•°ä¸º temporal_patch_size(2) çš„å€æ•°
            fps: å¸§ç‡ï¼ˆå½±å“ LLM M-RoPE ä¸­çš„æ—¶é—´ä½ç½®ç¼–ç é—´è·ï¼‰
            text_content: é¦–å¸§é™„å¸¦çš„æ–‡æœ¬æè¿°

        Returns:
            ç¼–ç çŠ¶æ€å­—ç¬¦ä¸²

        æ¨èç”¨æ³•:
            # 2 å¸§ chunk (T=1, æœ€ä½å»¶è¿Ÿ)
            engine.append_video_chunk([frame0, frame1], fps=2.0)

            # 4 å¸§ chunk (T=2, å»¶è¿Ÿ/è´¨é‡å‡è¡¡)
            engine.append_video_chunk([f0, f1, f2, f3], fps=4.0)
        """
        if not isinstance(frames, (list, tuple)) or len(frames) == 0:
            raise ValueError("frames must be a non-empty list of PIL Images.")
        if len(frames) % 2 != 0:
            print(
                f"âš ï¸ {len(frames)} frames is not a multiple of temporal_patch_size=2. "
                f"Last frame will be duplicated by the processor."
            )
        return self.append_frame(frames, text_content=text_content, as_video=True, fps=fps)

    @staticmethod
    def _measure_cache_bytes(cache) -> int:
        """
        è®¡ç®— KV Cache çš„æ€»æ˜¾å­˜å ç”¨ï¼ˆå­—èŠ‚ï¼‰ã€‚

        å…¼å®¹ 3 ç§ DynamicCache å†…éƒ¨ç»“æ„:
          â‘  transformers â‰¥ 4.50: cache.layers â†’ list[DynamicLayer]
             æ¯å±‚æœ‰ .key_state / .value_state (Tensor)
          â‘¡ transformers < 4.50: cache.key_cache / cache.value_cache â†’ list[Tensor]
          â‘¢ Tuple-of-tuples: ((K, V), (K, V), ...)
        """
        if cache is None:
            return 0

        total = 0

        # â”€â”€â”€ DynamicCache è·¯å¾„ â”€â”€â”€
        if hasattr(cache, "get_seq_length"):
            try:
                # ç­–ç•¥ 1: æ–°ç‰ˆ .layers å±æ€§ï¼ˆæ¯å±‚æ˜¯ DynamicLayer å¯¹è±¡ï¼‰
                if hasattr(cache, "layers") and len(getattr(cache, "layers", [])) > 0:
                    for layer in cache.layers:
                        for attr in ("key_state", "value_state"):
                            t = getattr(layer, attr, None)
                            if t is not None and hasattr(t, "nelement"):
                                total += t.nelement() * t.element_size()
                    if total > 0:
                        return total

                # ç­–ç•¥ 2: æ—§ç‰ˆ key_cache / value_cache åˆ—è¡¨
                for attr in ("key_cache", "value_cache"):
                    lst = getattr(cache, attr, None)
                    if lst is not None:
                        for t in lst:
                            if hasattr(t, "nelement"):
                                total += t.nelement() * t.element_size()
                if total > 0:
                    return total

                # ç­–ç•¥ 3: é€šç”¨å›é€€ â€” é€šè¿‡ __getitem__ é€å±‚æå–
                try:
                    n = len(cache)
                    for i in range(n):
                        layer_kv = cache[i]
                        if isinstance(layer_kv, (tuple, list)):
                            for t in layer_kv:
                                if hasattr(t, "nelement"):
                                    total += t.nelement() * t.element_size()
                except (TypeError, IndexError):
                    pass
            except Exception:
                pass
            return total

        # â”€â”€â”€ Tuple-of-tuples è·¯å¾„ â”€â”€â”€
        if isinstance(cache, (tuple, list)):
            for layer in cache:
                if isinstance(layer, (tuple, list)):
                    for t in layer:
                        if hasattr(t, "nelement"):
                            total += t.nelement() * t.element_size()
        return total

    def get_cache_info(self) -> dict:
        """è¿”å›å½“å‰ KV Cache çŠ¶æ€ä¿¡æ¯ï¼ˆå«æ·˜æ±°ç»Ÿè®¡ï¼‰ã€‚"""
        cache_len = self.cache_manager.get_seq_length()
        cache = self.cache_manager.cache
        mem_bytes = self._measure_cache_bytes(cache)
        mem_gb = mem_bytes / (1024 ** 3)

        stream_state = None
        if hasattr(self.model, "stream_state"):
            stream_state = {
                "last_cache_position": self.model.stream_state["last_cache_position"],
                "rope_deltas": (
                    self.model.stream_state["rope_deltas"].tolist()
                    if self.model.stream_state["rope_deltas"] is not None
                    else None
                ),
            }

        info = {
            "chunks_encoded": self.frame_count,
            "total_frames": self.total_frames,
            "cache_seq_length": cache_len,
            "cache_memory_gb": round(mem_gb, 4),
            "stream_state": stream_state,
        }

        # æ·˜æ±°ç»Ÿè®¡
        eviction_stats = self.cache_manager.get_eviction_stats()
        if eviction_stats:
            info["eviction_stats"] = eviction_stats

        return info
