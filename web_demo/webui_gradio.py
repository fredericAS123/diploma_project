import gradio as gr
import threading
import time
import torch
import numpy as np
from PIL import Image
from Qwen_inference import QwenInferenceWrapper

try:
    from decord import VideoReader, cpu
except ImportError:
    VideoReader = None
    cpu = None

class HistorySynchronizer:
    def __init__(self):
        self.chat_history = []

    def get_chat_history(self):
        return self.chat_history

    def update(self, role, content):
        self.chat_history.append(gr.ChatMessage(role=role, content=str(content)))

    def reset(self):
        self.chat_history = []

class VideoChatWebUI:
    def __init__(self, inference_engine: QwenInferenceWrapper):
        """
        åˆå§‹åŒ– UI é€»è¾‘ï¼Œä¼ å…¥æ¨ç†å¼•æ“å®ä¾‹ã€‚
        å·²é€‚é…æ–°çš„æµå¼å¼•æ“ APIï¼ˆæ—  manual_timeï¼Œæ”¯æŒ chunk ç¼–ç ï¼‰ã€‚
        """
        self.inference_engine = inference_engine
        self.history_synchronizer = HistorySynchronizer()
        
        # [æ§åˆ¶æ ‡å¿—]
        self.pause_event = threading.Event()
        self.pause_event.set() 
        self.stop_signal = False 
        self.is_streaming = False            
        self.current_streaming_time = 0.0
        
        # [æ•°æ®ç¼“å­˜]
        self.cached_video_path = None 
        self.cached_video_data = None 

        # [åŸç”Ÿæ¨ç†æ¨¡å¼]
        self.mode = "streaming"  # "streaming" or "native"
        self.native_frame_buffer = []
        self.native_fps = 2.0
        self.last_streaming_metrics = {}
        self.last_native_metrics = {}
        self.last_single_frame_metrics = {}
        self.last_frame_pil = None

    def generate_answer(self, question):
        """æ‰‹åŠ¨æé—®ï¼šæ”¯æŒæµå¼ã€åŸç”Ÿã€å•å¸§ä¸‰æ¨¡å¼å¯¹æ¯”"""

        # â”€â”€â”€ åŸç”Ÿæ¨ç†æ¨¡å¼ â”€â”€â”€
        if self.mode == "native":
            if not self.native_frame_buffer:
                yield (self.history_synchronizer.get_chat_history() + [
                    gr.ChatMessage(role="assistant", content="è¯·å…ˆåŠ è½½è§†é¢‘å¸§ï¼ˆç‚¹å‡» Start Nativeï¼‰")
                ], "", "", "")
                return

            auto_paused = False
            if self.is_streaming and self.pause_event.is_set():
                self.pause_event.clear()
                auto_paused = True
                time.sleep(0.1)

            self.history_synchronizer.update("user", question)
            n_frames = len(self.native_frame_buffer)
            yield (self.history_synchronizer.get_chat_history() + [
                gr.ChatMessage(role="assistant",
                    content=f"ğŸ”„ æ­£åœ¨è¿›è¡ŒåŒæ¨¡å¼å¯¹æ¯”æ¨ç† ({n_frames} å¸§ vs å•å¸§)...")
            ], "", "", "")

            # 1) åŸç”Ÿæ¨ç†ï¼ˆå…¨éƒ¨å¸§ï¼‰
            try:
                native_response, native_metrics = self.inference_engine.native_video_inference(
                    frames=self.native_frame_buffer,
                    question=question,
                    fps=self.native_fps,
                    max_new_tokens=256,
                    min_new_tokens=8,
                    do_sample=False,
                )
                self.last_native_metrics = native_metrics
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                native_response = (
                    "âš ï¸ OOM: åŸç”Ÿæ¨¡å¼æ˜¾å­˜ä¸è¶³ï¼è¿™æ­£è¯´æ˜æµå¼æ¨ç†çš„ä¼˜åŠ¿â€”â€”"
                    "åŸç”Ÿæ¨¡å¼éœ€è¦ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å¸§ï¼Œæ˜¾å­˜éœ€æ±‚è¿œé«˜äºæµå¼æ¨¡å¼ã€‚"
                )
                native_metrics = {"ttft": 0.0, "vram_peak_gb": 0.0, "num_frames": n_frames, "input_tokens": 0}
                self.last_native_metrics = native_metrics

            # 2) å•å¸§æ¨ç†ï¼ˆä»…æœ€åä¸€å¸§ï¼‰
            last_frame = self.native_frame_buffer[-1]
            try:
                sf_response, sf_metrics = self.inference_engine.single_frame_inference(
                    image=last_frame,
                    question=question,
                    max_new_tokens=256,
                    min_new_tokens=8,
                    do_sample=False,
                )
                self.last_single_frame_metrics = sf_metrics
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                sf_response = "âš ï¸ å•å¸§æ¨ç† OOM"
                sf_metrics = {"ttft": 0.0, "vram_peak_gb": 0.0, "input_tokens": 0}
                self.last_single_frame_metrics = sf_metrics

            self.history_synchronizer.update(
                "assistant",
                f"ğŸ“¦ [åŸç”Ÿæ¨ç† - å…¨éƒ¨ {n_frames} å¸§]\n{native_response}"
            )
            self.history_synchronizer.update(
                "assistant",
                f"ğŸ–¼ï¸ [å•å¸§æ¨ç† - ä»…æœ€åä¸€å¸§]\n{sf_response}"
            )

            native_info = (
                f"[Native] Frames: {native_metrics.get('num_frames', 0)} | "
                f"Tokens: {native_metrics.get('input_tokens', 0)} | "
                f"VRAM Peak: {native_metrics.get('vram_peak_gb', 0):.2f}GB"
            )
            comparison = self._build_comparison_text()
            if auto_paused:
                self.pause_event.set()
            yield (self.history_synchronizer.get_chat_history(),
                   f"{native_metrics.get('ttft', 0):.3f}", native_info, comparison)
            return

        # â”€â”€â”€ æµå¼æ¨ç†æ¨¡å¼ â”€â”€â”€
        if not self.is_streaming and self.current_streaming_time <= 0:
            yield (self.history_synchronizer.get_chat_history() + [
                gr.ChatMessage(role="assistant", content="è¯·å…ˆæ’­æ”¾è§†é¢‘")
            ], "", "", "")
            return

        # è‡ªåŠ¨æš‚åœæœºåˆ¶
        auto_paused = False
        if self.is_streaming and self.pause_event.is_set():
            print("âš ï¸ Detected streaming active. Auto-pausing for manual question...")
            self.pause_event.clear()
            auto_paused = True
            time.sleep(0.1)

        self.history_synchronizer.update("user", question)
        
        status_msg = "ğŸ”„ æ­£åœ¨è¿›è¡ŒåŒæ¨¡å¼å¯¹æ¯”æ¨ç† (æµå¼ + å•å¸§)..."
        if auto_paused:
            status_msg += " (è§†é¢‘å·²è‡ªåŠ¨æš‚åœ)"
            
        yield self.history_synchronizer.get_chat_history() + [gr.ChatMessage(role="assistant", content=status_msg)], "", "", ""

        try:
            # é‡ç½® VRAM å³°å€¼ç»Ÿè®¡
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()

            start_t = time.perf_counter()
            response, metrics = self.inference_engine.ask_question(
                question,
                max_new_tokens=256,
                min_new_tokens=8,
                do_sample=False,
                temperature=0.7,
                top_p=0.9,
                return_metrics=True,
            )
            cost = time.perf_counter() - start_t
            print(f"Manual Inference Latency: {cost:.4f}s")

            # è®°å½• VRAM å³°å€¼
            if torch.cuda.is_available():
                metrics["vram_peak_gb"] = round(
                    torch.cuda.max_memory_allocated() / (1024 ** 3), 3
                )
            self.last_streaming_metrics = metrics
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            response = "Error: OOM (æ˜¾å­˜ä¸è¶³)ã€‚è¯·å°è¯•å‡å°‘è§†é¢‘å¸§æ•°ã€‚"
            metrics = {"ttft": 0.0}

        # å•å¸§æ¨ç†å¯¹æ¯”
        sf_response = ""
        sf_metrics = {}
        if self.last_frame_pil is not None:
            try:
                sf_response, sf_metrics = self.inference_engine.single_frame_inference(
                    image=self.last_frame_pil,
                    question=question,
                    max_new_tokens=256,
                    min_new_tokens=8,
                    do_sample=False,
                )
                self.last_single_frame_metrics = sf_metrics
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                sf_response = "âš ï¸ å•å¸§æ¨ç† OOM"
                sf_metrics = {"ttft": 0.0, "vram_peak_gb": 0.0, "input_tokens": 0}
                self.last_single_frame_metrics = sf_metrics

        self.history_synchronizer.update("assistant", f"ğŸ“¡ [æµå¼æ¨ç† - KV Cache å¤ç”¨]\n{response}")
        if sf_response:
            self.history_synchronizer.update("assistant", f"ğŸ–¼ï¸ [å•å¸§æ¨ç† - ä»…æœ€åä¸€å¸§]\n{sf_response}")

        ttft_val = metrics.get("ttft", 0.0)

        # è·å–ç¼“å­˜ä¿¡æ¯
        cache_info = self.inference_engine.get_cache_info()
        cache_str = (
            f"Seq: {cache_info['cache_seq_length']} | "
            f"Mem: {cache_info['cache_memory_gb']:.2f}GB | "
            f"Frames: {cache_info['total_frames']}"
        )
        comparison = self._build_comparison_text()
        yield self.history_synchronizer.get_chat_history(), f"{ttft_val:.3f}", cache_str, comparison

    def start_chat(self, video_path, frame_interval, chunk_size, current_history):
        """
        æ™ºèƒ½å¯åŠ¨ï¼šæµå¼ç¼–ç è§†é¢‘å¸§ã€‚
        
        æ”¯æŒ chunk ç¼–ç æ¨¡å¼ï¼š
          - chunk_size=1: é€å¸§ image æ¨¡å¼
          - chunk_size=2,4,6...: å¤šå¸§ video chunk æ¨¡å¼ï¼ˆæ¨èï¼‰
        """
        if not video_path:
            raise gr.Error("Please upload a video file.")

        if VideoReader is None:
            raise gr.Error("Missing dependency: decord. Please install it first.")

        # 1. å¼ºåˆ¶é‡ç½®æ ‡å¿—ä½ï¼ˆåˆ‡æ¢åˆ°æµå¼æ¨¡å¼ï¼‰
        self.mode = "streaming"
        self.native_frame_buffer = []
        self.pause_event.set()
        self.stop_signal = False 
        self.current_streaming_time = 0.0

        if self.inference_engine is None:
            self.inference_engine = QwenInferenceWrapper()

        self.inference_engine.reset()

        print(f"ğŸ”„ Loading video for streaming encode: {video_path}")
        torch.cuda.empty_cache()

        vr = VideoReader(video_path, ctx=cpu(0))
        avg_fps = vr.get_avg_fps()
        target_fps = 1.0 / frame_interval
        step = max(1, int(avg_fps / target_fps))
        frame_indices = np.arange(0, len(vr), step)

        self.history_synchronizer.reset()
        fps_display = 0.0
        chunk_size = max(1, int(chunk_size))
        
        print(f"ğŸš€ å¼€å§‹æµå¼ç¼–ç ï¼Œæ€»å¸§æ•°: {len(frame_indices)}, chunk_size={chunk_size}")
        self.inference_engine.log_vram("Start-Encode")
        self.is_streaming = True
        
        try:
            last_timestamp = 0.0
            last_frame = None
            frame_buffer = []  # ç´¯ç§¯å¸§ç”¨äº chunk ç¼–ç 

            for idx, frame_index in enumerate(frame_indices):
                if self.stop_signal:
                    print("ğŸ›‘ Inference loop stopped by user.")
                    break

                if not self.pause_event.is_set():
                    self.pause_event.wait()
                    if self.stop_signal: break
                
                img_np = vr[frame_index].asnumpy()
                img_pil = Image.fromarray(img_np)
                self.last_frame_pil = img_pil
                timestamp = frame_index / avg_fps

                frame_buffer.append(img_pil)

                # å½“ buffer æ»¡æ—¶è¿›è¡Œ chunk ç¼–ç 
                if len(frame_buffer) >= chunk_size:
                    inference_start = time.perf_counter()

                    if chunk_size == 1:
                        # å•å¸§ image æ¨¡å¼
                        response = self.inference_engine.process_frame(frame_buffer[0])
                    else:
                        # å¤šå¸§ video chunk æ¨¡å¼
                        chunk_fps = target_fps  # ä½¿ç”¨é‡‡æ ·åçš„å¸§ç‡
                        response = self.inference_engine.process_video_chunk(
                            frame_buffer, fps=chunk_fps
                        )

                    encoded_count = len(frame_buffer)  # è®°å½•ç¼–ç å¸§æ•°ï¼ˆæ¸…ç©ºå‰ï¼‰
                    frame_buffer = []
                    self.current_streaming_time = timestamp

                    if idx % 10 == 0:
                        self.inference_engine.log_vram(f"Encode-{idx}")

                    self.history_synchronizer.update(
                        "assistant",
                        f"System: [T={timestamp:.1f}s] {response}"
                    )

                    cost_time = time.perf_counter() - inference_start
                    if cost_time > 0:
                        fps_display = encoded_count / cost_time

                last_timestamp = timestamp
                last_frame = img_np

                # è·å–ç¼“å­˜ä¿¡æ¯
                cache_info = self.inference_engine.get_cache_info()
                cache_str = (
                    f"Seq: {cache_info['cache_seq_length']} | "
                    f"Mem: {cache_info['cache_memory_gb']:.2f}GB | "
                    f"Frames: {cache_info['total_frames']}"
                )
                yield timestamp, img_np, None, self.history_synchronizer.get_chat_history(), f"{fps_display:.2f}", cache_str

            # å¤„ç†å‰©ä½™å¸§
            if frame_buffer and not self.stop_signal:
                if chunk_size == 1:
                    self.inference_engine.process_frame(frame_buffer[0])
                else:
                    self.inference_engine.process_video_chunk(frame_buffer, fps=target_fps)
                frame_buffer = []

        except Exception as e:
            print(f"Runtime Error: {e}")
            raise e
        finally:
            self.is_streaming = False
            self.stop_signal = False 
            self.inference_engine.log_vram("Finished")
            
        if not self.stop_signal and last_frame is not None:
            cache_info = self.inference_engine.get_cache_info()
            cache_str = (
                f"Seq: {cache_info['cache_seq_length']} | "
                f"Mem: {cache_info['cache_memory_gb']:.2f}GB | "
                f"Frames: {cache_info['total_frames']}"
            )
            yield last_timestamp, last_frame, None, self.history_synchronizer.get_chat_history(), f"{fps_display:.2f}", cache_str

    def _build_comparison_text(self):
        """æ„å»ºæµå¼ vs åŸç”Ÿ vs å•å¸§ä¸‰æ¨¡å¼å¯¹æ¯”æ–‡æœ¬"""
        s = self.last_streaming_metrics
        n = self.last_native_metrics
        f = self.last_single_frame_metrics

        if not s and not n and not f:
            return ""

        lines = ["â•â•â•â•â•â• ä¸‰æ¨¡å¼æ¨ç†å¯¹æ¯” â•â•â•â•â•â•"]

        if s:
            s_vram_str = f"{s['vram_peak_gb']}GB" if 'vram_peak_gb' in s else "N/A"
            lines.append(f"ğŸ“¡ æµå¼: TTFT={s.get('ttft', 0):.3f}s | VRAM Peak={s_vram_str}")
        else:
            lines.append("ğŸ“¡ æµå¼: (å°šæœªæµ‹è¯•ï¼Œè¯·å…ˆè¿è¡Œ Start Streaming)")

        if n:
            lines.append(
                f"ğŸ“¦ åŸç”Ÿ: TTFT={n.get('ttft', 0):.3f}s | "
                f"VRAM Peak={n.get('vram_peak_gb', 0):.2f}GB | "
                f"Input Tok={n.get('input_tokens', 0)}"
            )
        else:
            lines.append("ğŸ“¦ åŸç”Ÿ: (å°šæœªæµ‹è¯•ï¼Œè¯·å…ˆè¿è¡Œ Start Native)")

        if f:
            lines.append(
                f"ğŸ–¼ï¸ å•å¸§: TTFT={f.get('ttft', 0):.3f}s | "
                f"VRAM Peak={f.get('vram_peak_gb', 0):.2f}GB | "
                f"Input Tok={f.get('input_tokens', 0)}"
            )
        else:
            lines.append("ğŸ–¼ï¸ å•å¸§: (å°šæœªæµ‹è¯•)")

        lines.append("â”€â”€â”€ å¯¹æ¯”åˆ†æ â”€â”€â”€")

        # æµå¼ vs åŸç”Ÿ
        if s and n and s.get('ttft', 0) > 0 and n.get('ttft', 0) > 0:
            speedup = n['ttft'] / s['ttft']
            lines.append(f"ğŸš€ æµå¼ vs åŸç”Ÿ TTFT åŠ é€Ÿ: {speedup:.1f}x")

            s_vram = s.get('vram_peak_gb')
            n_vram = n.get('vram_peak_gb')
            if (isinstance(s_vram, (int, float)) and isinstance(n_vram, (int, float))
                    and s_vram > 0 and n_vram > 0):
                savings = (1 - s_vram / n_vram) * 100
                lines.append(f"ğŸ’¾ æµå¼ vs åŸç”Ÿ VRAM èŠ‚çœ: {savings:.1f}% ({n_vram:.2f}GB â†’ {s_vram:.2f}GB)")

        # è§†é¢‘ç†è§£ vs å•å¸§
        if f and f.get('ttft', 0) > 0:
            if s and s.get('ttft', 0) > 0:
                ratio = f['ttft'] / s['ttft']
                lines.append(f"âš¡ æµå¼ vs å•å¸§ TTFT: Ã—{ratio:.1f} (æµå¼ç•¥æ…¢ä½†æ‹¥æœ‰å®Œæ•´æ—¶åºç†è§£)")
            if n and n.get('ttft', 0) > 0:
                ratio = n['ttft'] / f['ttft']
                lines.append(f"ğŸ¢ åŸç”Ÿ vs å•å¸§ TTFT: Ã—{ratio:.1f} (åŸç”Ÿå¤„ç†å…¨å¸§ï¼Œå»¶è¿Ÿè¿œé«˜äºå•å¸§)")

        if (s or n) and f:
            lines.append("â”€â”€â”€ ç»“è®º â”€â”€â”€")
            lines.append("ğŸ¬ è§†é¢‘ç†è§£(æµå¼/åŸç”Ÿ)åˆ©ç”¨å¤šå¸§æ—¶åºä¿¡æ¯ï¼Œå›ç­”æ›´å‡†ç¡®å…¨é¢")
            lines.append("ğŸ–¼ï¸ å•å¸§ä»…çœ‹åˆ°ä¸€å¸§ç”»é¢ï¼Œç¼ºä¹æ—¶åºåŠ¨æ€ç†è§£èƒ½åŠ›")
            if s:
                lines.append("âœ… æµå¼æ¨ç†ï¼šæ¥è¿‘å•å¸§çš„é€Ÿåº¦ + å®Œæ•´è§†é¢‘çš„ç†è§£åŠ› = æœ€ä½³æ–¹æ¡ˆ")

        return "\n".join(lines)

    def start_native_chat(self, video_path, frame_interval, current_history):
        """
        åŸç”Ÿæ¨ç†æ¨¡å¼å¯åŠ¨ï¼šæå–å¸§å¹¶ç¼“å­˜ï¼Œä¸åšå®æ—¶ç¼–ç ã€‚
        ç”¨æˆ·æé—®æ—¶ä¸€æ¬¡æ€§å°†æ‰€æœ‰å¸§é€å…¥æ¨¡å‹æ¨ç†ã€‚
        """
        if not video_path:
            raise gr.Error("è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶ã€‚")
        if VideoReader is None:
            raise gr.Error("ç¼ºå°‘ä¾èµ–: decordã€‚è¯·å…ˆå®‰è£…ã€‚")

        # åˆ‡æ¢åˆ°åŸç”Ÿæ¨¡å¼
        self.mode = "native"
        self.native_frame_buffer = []
        self.pause_event.set()
        self.stop_signal = False
        self.current_streaming_time = 0.0
        self.history_synchronizer.reset()

        # é‡ç½®æµå¼å¼•æ“çŠ¶æ€
        if self.inference_engine is not None:
            self.inference_engine.reset()
        torch.cuda.empty_cache()

        vr = VideoReader(video_path, ctx=cpu(0))
        avg_fps = vr.get_avg_fps()
        target_fps = 1.0 / frame_interval
        step = max(1, int(avg_fps / target_fps))
        frame_indices = np.arange(0, len(vr), step)

        self.native_fps = target_fps
        self.is_streaming = True

        print(f"ğŸ“¹ [Native Mode] æå– {len(frame_indices)} å¸§ (ä»…ç¼“å­˜ï¼Œä¸ç¼–ç )")

        last_frame = None
        fps_display = 0.0

        try:
            for idx, frame_index in enumerate(frame_indices):
                if self.stop_signal:
                    print("ğŸ›‘ Native frame extraction stopped by user.")
                    break
                if not self.pause_event.is_set():
                    self.pause_event.wait()
                    if self.stop_signal:
                        break

                extract_start = time.perf_counter()
                img_np = vr[frame_index].asnumpy()
                img_pil = Image.fromarray(img_np)
                timestamp = frame_index / avg_fps

                self.native_frame_buffer.append(img_pil)
                self.current_streaming_time = timestamp
                last_frame = img_np

                extract_cost = time.perf_counter() - extract_start
                if extract_cost > 0:
                    fps_display = 1.0 / extract_cost

                if idx == 0:
                    self.history_synchronizer.update(
                        "assistant",
                        "System: [Native Mode] ğŸ“¦ å¼€å§‹æå–å¸§... (ä¸è¿›è¡Œæ¨¡å‹ç¼–ç )"
                    )

                status = (
                    f"[Native] å·²ç¼“å­˜: {len(self.native_frame_buffer)}/{len(frame_indices)} å¸§ | "
                    f"T={timestamp:.1f}s | æ— æ¨¡å‹ç¼–ç "
                )
                yield (timestamp, img_np, None,
                       self.history_synchronizer.get_chat_history(),
                       f"{fps_display:.1f}", status)

        except Exception as e:
            print(f"Native extraction error: {e}")
            raise e
        finally:
            self.is_streaming = False
            self.stop_signal = False

        # å®Œæˆæ¶ˆæ¯
        n_frames = len(self.native_frame_buffer)
        self.history_synchronizer.update(
            "assistant",
            f"System: âœ… [Native] å¸§ç¼“å­˜å®Œæˆ: {n_frames} å¸§ | FPS={self.native_fps:.1f}\n"
            f"ğŸ’¡ è¯·åœ¨å³ä¾§è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹å°†ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ {n_frames} å¸§è¿›è¡Œæ¨ç†ã€‚"
        )

        if last_frame is not None:
            final_status = (
                f"[Native] âœ… Ready: {n_frames} frames | "
                f"FPS={self.native_fps:.1f} | ç­‰å¾…æé—®"
            )
            yield (self.current_streaming_time, last_frame, None,
                   self.history_synchronizer.get_chat_history(),
                   f"{n_frames}", final_status)

    def toggle_pause(self):
        if self.pause_event.is_set():
            self.pause_event.clear()
            return "Resume Video", self.history_synchronizer.get_chat_history()
        else:
            self.pause_event.set()
            return "Pause Video", self.history_synchronizer.get_chat_history()

    def stop_chat(self):
        print("ğŸ›‘ Stop command received.")
        self.stop_signal = True 
        self.pause_event.set() 
        time.sleep(0.1)
        self.is_streaming = False
        self.current_streaming_time = 0.0
        self.native_frame_buffer = []
        self.last_frame_pil = None
        self.last_single_frame_metrics = {}
        self.mode = "streaming"
        if self.inference_engine is not None:
            self.inference_engine.reset()
        return 0, None, None, [], "0.00", "", "", ""

    def create_interface(self):
        with gr.Blocks(title="Qwen2.5-VL Streaming Video Chat") as demo:
            pil_frames_state = gr.State()

            gr.Markdown("# ğŸ¬ Qwen2.5-VL Streaming Video Chat\næµå¼è§†é¢‘ç†è§£ + å®æ—¶é—®ç­” | æ”¯æŒæµå¼ vs åŸç”Ÿæ¨ç†å¯¹æ¯”")

            with gr.Row():
                with gr.Column(scale=3):
                    gr_frame_display = gr.Image(label="Current Frame", interactive=False, height=400)
                    with gr.Row():
                        gr_time_display = gr.Number(label="Video Time (s)", value=0)
                        gr_fps_display = gr.Textbox(label="Inference FPS", value="0.00")
                        gr_ttft_display = gr.Textbox(label="Ask TTFT (s)", value="")
                    with gr.Row():
                        gr_cache_display = gr.Textbox(label="KV Cache Status", value="", interactive=False)
                    with gr.Row():
                        gr_pause_button = gr.Button("Pause Video")
                        gr_stop_button = gr.Button("Stop Video", variant="stop")

                with gr.Column(scale=2):
                    gr_chat_interface = gr.Chatbot(label="Chat History", height=500)
                    gr_question_input = gr.Textbox(label="Manual Question (Auto-pauses video)")

            with gr.Row():
                with gr.Column():
                    gr_video_upload = gr.Video(label="1. Upload Video")
                with gr.Column():
                    gr_frame_interval = gr.Slider(
                        minimum=0.1, maximum=5.0, step=0.1, value=1.0,
                        interactive=True, label="2. Frame Interval (s)"
                    )
                    gr_chunk_size = gr.Slider(
                        minimum=1, maximum=8, step=1, value=4,
                        interactive=True, label="3. Chunk Size (frames, æ¨è 2/4/6)"
                    )
                    with gr.Row():
                        gr_start_button = gr.Button("4a. Start Streaming â–¶ï¸", variant="primary")
                        gr_start_native_button = gr.Button("4b. Start Native ğŸ“¦", variant="secondary")

            with gr.Row():
                gr_comparison_display = gr.Textbox(
                    label="ğŸ“Š ä¸‰æ¨¡å¼å¯¹æ¯”: Streaming vs Native vs Single-Frame",
                    value="åˆ†åˆ«è¿è¡Œæµå¼å’ŒåŸç”Ÿæ¨¡å¼åï¼Œæé—®æ—¶è‡ªåŠ¨å¯¹æ¯”ä¸‰ç§æ¨ç†æ¨¡å¼",
                    lines=8,
                    interactive=False,
                )

            gr_question_input.submit(
                self.generate_answer,
                inputs=[gr_question_input],
                outputs=[gr_chat_interface, gr_ttft_display, gr_cache_display, gr_comparison_display],
                queue=True,
            )
            gr_start_button.click(
                self.start_chat,
                inputs=[gr_video_upload, gr_frame_interval, gr_chunk_size, gr_chat_interface],
                outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display, gr_cache_display],
            )
            gr_start_native_button.click(
                self.start_native_chat,
                inputs=[gr_video_upload, gr_frame_interval, gr_chat_interface],
                outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display, gr_cache_display],
            )
            gr_pause_button.click(
                self.toggle_pause,
                inputs=[],
                outputs=[gr_pause_button, gr_chat_interface],
            )
            gr_stop_button.click(
                self.stop_chat,
                inputs=[],
                outputs=[gr_time_display, gr_frame_display, pil_frames_state, gr_chat_interface, gr_fps_display, gr_ttft_display, gr_cache_display, gr_comparison_display],
            )
        
        return demo
