"""
Delayed Batch Inference Engine - æ–°æ–¹æ¡ˆæ ¸å¿ƒå®ç°

æ ¸å¿ƒæ€æƒ³ï¼š
1. æµå¼æ”¶é›†å¸§ï¼ˆadd_frameï¼‰
2. å»¶è¿Ÿåˆ°æé—®æ—¶æ‰æ‰¹é‡ç¼–ç ï¼ˆaskï¼‰
3. ä½¿ç”¨ video æ¨¡å¼äº«å— temporal merge å‹ç¼©
4. Vision Encoder èƒ½çœ‹åˆ°æ‰€æœ‰å¸§ï¼Œå»ºç«‹å®Œæ•´çš„è·¨å¸§æ³¨æ„åŠ›

ä¼˜åŠ¿ï¼š
- âœ… Vision Encoder è·¨å¸§æ³¨æ„åŠ›ï¼šå®Œæ•´
- âœ… KV Cache å®Œæ•´æ€§ï¼šå®Œæ•´ï¼ˆä¸ä¸¢å¤±å†å²ï¼‰
- âœ… å®ç°éš¾åº¦ï¼šä¸­ç­‰
- âœ… æ˜¾å­˜æ§åˆ¶ï¼šé€šè¿‡æ™ºèƒ½å¸§ç®¡ç†
"""

import torch
from transformers import AutoProcessor, TextIteratorStreamer
import gc
import time
from threading import Thread
from typing import List, Dict, Optional, Tuple, Generator
from PIL import Image

from .smart_frame_manager import SmartFrameManager
from .video_sampler import VideoSampler, validate_time_encoding


class DelayedBatchInferenceEngine:
    """å»¶è¿Ÿæ‰¹é‡ç¼–ç æ¨ç†å¼•æ“ - å¢åŠ åŠ¨æ€é‡‡æ ·æ”¯æŒ"""
    
    def __init__(
        self,
        model,
        processor,
        device: str = "cuda",
        star_memory_size: int = 20,
        stream_window_size: int = 20,
        max_pixels: int = 4 * 224 * 224,  # å€Ÿé‰´Flash-VStreamçš„ä½åˆ†è¾¨ç‡ç­–ç•¥
        min_pixels: int = 4 * 28 * 28,
        # === æ–°å¢å‚æ•° ===
        target_fps: float = None,  # ç›®æ ‡é‡‡æ ·é¢‘ç‡ï¼ŒNoneè¡¨ç¤ºä¸é‡‡æ ·
        enable_absolute_time_encoding: bool = True,  # æ˜¯å¦å¯ç”¨ç»å¯¹æ—¶é—´ç¼–ç 
        use_disk_cache: bool = True,  # æ˜¯å¦ä½¿ç”¨ç¡¬ç›˜ç¼“å­˜ï¼ˆèŠ‚çœå†…å­˜ï¼‰
        max_sampled_frames: int = 48,  # æœ€å¤§é‡‡æ ·å¸§æ•°ï¼ˆé˜²æ­¢OOMï¼Œ24GBæ˜¾å­˜å»ºè®®48ï¼‰
    ):
        """
        Args:
            model: Qwen2.5-VL æ¨¡å‹
            processor: å¯¹åº”çš„ processor
            device: è®¾å¤‡
            star_memory_size: Star Memory å®¹é‡
            stream_window_size: Stream Memory çª—å£å¤§å°
            max_pixels: æœ€å¤§åƒç´ ï¼ˆç”¨äº video æ¨¡å¼ï¼‰
            min_pixels: æœ€å°åƒç´ 
            target_fps: ç›®æ ‡é‡‡æ ·é¢‘ç‡
            enable_absolute_time_encoding: æ˜¯å¦å¯ç”¨ç»å¯¹æ—¶é—´ç¼–ç 
            use_disk_cache: æ˜¯å¦ä½¿ç”¨ç¡¬ç›˜ç¼“å­˜ï¼ˆæ¨èTrueï¼Œå¤§å¹…èŠ‚çœå†…å­˜ï¼‰
            max_sampled_frames: æœ€å¤§é‡‡æ ·å¸§æ•°ï¼ˆé˜²æ­¢OOMï¼‰
        """
        self.model = model
        self.processor = processor
        self.device = device
        self.max_pixels = max_pixels
        self.min_pixels = min_pixels
        self.target_fps = target_fps
        self.enable_absolute_time_encoding = enable_absolute_time_encoding
        self.use_disk_cache = use_disk_cache
        self.max_sampled_frames = max_sampled_frames
        
        # æ™ºèƒ½å¸§ç®¡ç†å™¨ï¼ˆæ”¯æŒç¡¬ç›˜ç¼“å­˜ï¼‰
        self.frame_manager = SmartFrameManager(
            star_memory_size=star_memory_size,
            stream_window_size=stream_window_size,
            use_disk_cache=use_disk_cache,
        )
        
        # KV Cacheï¼ˆåœ¨æé—®æ—¶ç”Ÿæˆï¼‰
        self.video_cache = None
        self.cache_is_valid = False
        
        # è®°å½•æ¨¡å‹æ•°æ®ç±»å‹
        try:
            self.model_dtype = next(model.parameters()).dtype
        except StopIteration:
            self.model_dtype = torch.float32
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.encode_count = 0
        self.total_frames_processed = 0
        
        # å­˜å‚¨é‡‡æ ·å…ƒæ•°æ®
        self.last_sample_metadata = None
        
        # ä»processorè·å–æ—¶é—´ç¼–ç ç›¸å…³é…ç½®
        self.temporal_patch_size = getattr(
            processor, 'temporal_patch_size', 
            getattr(model.config.vision_config, 'temporal_patch_size', 2)
        )
        self.tokens_per_second = getattr(
            model.config.vision_config, 'tokens_per_second', 4
        )
        
        if target_fps is not None:
            self.video_sampler = VideoSampler(
                target_fps=target_fps,
                temporal_patch_size=self.temporal_patch_size,
                tokens_per_second=self.tokens_per_second,
                max_sampled_frames=max_sampled_frames,
            )
            print(f"   ğŸ¯ Target FPS: {target_fps}")
            print(f"   ğŸ“Š Max Sampled Frames: {max_sampled_frames}")
            print(f"   â±ï¸  Absolute Time Encoding: {enable_absolute_time_encoding}")
        else:
            self.video_sampler = None
        
        print(f"âœ… DelayedBatchInferenceEngine Initialized")
        print(f"   ğŸ“ Max Pixels: {max_pixels} ({int(max_pixels**0.5)}x{int(max_pixels**0.5)})")
        print(f"   ğŸ¬ Strategy: æµå¼æ”¶é›† + å»¶è¿Ÿæ‰¹é‡ç¼–ç ")
    
    def add_frame(self, frame: Image.Image, timestamp: float = None) -> str:
        """
        æ·»åŠ æ–°å¸§ï¼ˆæµå¼æ”¶é›†ï¼‰
        
        Args:
            frame: PIL Image
            timestamp: æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        
        Returns:
            çŠ¶æ€ä¿¡æ¯
        """
        if timestamp is None:
            timestamp = time.time()
        
        result = self.frame_manager.add_frame(frame, timestamp)
        self.total_frames_processed += 1
        
        # æ ‡è®° cache å¤±æ•ˆï¼ˆæœ‰æ–°å¸§åŠ å…¥ï¼Œä¸‹æ¬¡ ask æ—¶éœ€è¦é‡æ–°ç¼–ç ï¼‰
        self.cache_is_valid = False
        
        # æ„å»ºçŠ¶æ€æ¶ˆæ¯
        status = f"Frame #{self.total_frames_processed} added to Stream Memory"
        if result['added_to_star']:
            status += f" + Star Memory ({result['reason']})"
        
        status += f" | Star: {result['star_count']}, Stream: {result['stream_count']}"
        
        return status
    
    def _encode_all_frames(self) -> Dict[str, any]:
        """
        æ‰¹é‡ç¼–ç æ‰€æœ‰å¸§ï¼ˆä¿®æ”¹ç‰ˆ - æ”¯æŒåŠ¨æ€é‡‡æ ·å’Œæ—¶é—´ç¼–ç ï¼‰
        
        å…³é”®ä¿®å¤ï¼šä½¿ç”¨ sample_from_timestamps åŸºäºçœŸå®æ—¶é—´æˆ³é‡‡æ ·ï¼Œ
        è€Œé sample_frames åŸºäºç´¢å¼•é‡‡æ ·ã€‚è¿™å¯¹äº Star+Stream æ··åˆçš„
        ç¨€ç–å¸§åºåˆ—è‡³å…³é‡è¦ï¼Œå¦åˆ™æ—¶é—´ç¼–ç ä¼šå®Œå…¨é”™ä¹±ã€‚
        """
        # ========== å¼ºåˆ¶é‡Šæ”¾æ˜¾å­˜ï¼ˆå…³é”®ï¼ï¼‰==========
        # å¿…é¡»åœ¨ç¼–ç å‰å½»åº•é‡Šæ”¾æ—§çš„ KV cacheï¼Œå¦åˆ™ä¼š OOM
        if self.video_cache is not None:
            # å¦‚æœæ˜¯ DynamicCache ç±»å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if hasattr(self.video_cache, 'key_cache'):
                for layer_cache in self.video_cache.key_cache:
                    del layer_cache
                for layer_cache in self.video_cache.value_cache:
                    del layer_cache
            del self.video_cache
            self.video_cache = None
        
        # å¤šæ¬¡è°ƒç”¨ gc ç¡®ä¿å½»åº•é‡Šæ”¾
        for _ in range(3):
            gc.collect()
        
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ç¡®ä¿é‡Šæ”¾å®Œæˆ
            # æ‰“å°æ˜¾å­˜çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"   ğŸ§¹ æ˜¾å­˜é‡Šæ”¾å: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB")
        
        # è·å–æ‰€æœ‰éœ€è¦ç¼–ç çš„å¸§ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        all_frames, timestamps, metadata = self.frame_manager.get_all_frames()
        
        if not all_frames:
            return {'success': False, 'reason': 'no frames'}
        
        # === å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŸºäºæ—¶é—´æˆ³çš„é‡‡æ · ===
        second_per_grid_ts = None
        if self.video_sampler is not None:
            # ä½¿ç”¨ sample_from_timestamps è€Œé sample_frames
            # è¿™ç¡®ä¿äº†ç¨€ç–å¸§ï¼ˆå¦‚ t=0s çš„ Star å¸§ + t=50~55s çš„ Stream å¸§ï¼‰
            # èƒ½æ­£ç¡®åæ˜ çœŸå®çš„æ—¶é—´è·¨åº¦
            all_frames, second_per_grid_t, sample_meta = self.video_sampler.sample_from_timestamps(
                frames=all_frames,
                timestamps=timestamps,
            )
            
            self.last_sample_metadata = sample_meta
            
            if self.enable_absolute_time_encoding:
                # å°† second_per_grid_t è½¬æ¢ä¸º tensor
                second_per_grid_ts = torch.tensor(
                    [second_per_grid_t], 
                    dtype=torch.float32,
                    device=self.device
                )
            
            print(f"   ğŸ“¹ é‡‡æ ·: {sample_meta['original_frames']} â†’ {sample_meta['sampled_frames']} å¸§")
            print(f"   â±ï¸  second_per_grid_t: {second_per_grid_t:.4f}s")
            print(f"   ğŸ• å®é™…æ—¶é—´è·¨åº¦: {sample_meta['video_duration']:.2f}s (ä» t={metadata['min_timestamp']:.1f}s åˆ° t={metadata['max_timestamp']:.1f}s)")
        
        # è®°å½•å¸§æ•°ï¼ˆåé¢ä¼šåˆ é™¤ all_framesï¼‰
        frames_encoded_count = len(all_frames)
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆä½¿ç”¨ video æ¨¡å¼ï¼‰
        messages = [{
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": all_frames,
                    "min_pixels": self.min_pixels,
                    "max_pixels": self.max_pixels,
                },
                {"type": "text", "text": "Watch this video stream."},
            ],
        }]
        
        # åº”ç”¨ chat template
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=False
        )
        
        # å¤„ç†è¾“å…¥
        t_start = time.time()
        inputs = self.processor(
            text=[text_prompt],
            videos=[all_frames],
            padding=True,
            return_tensors="pt",
        ).to(self.device)
        
        # === æ–°å¢ï¼šæ³¨å…¥ second_per_grid_ts ===
        if second_per_grid_ts is not None:
            inputs['second_per_grid_ts'] = second_per_grid_ts
        
        # è½¬æ¢æ•°æ®ç±»å‹
        if "pixel_values_videos" in inputs and inputs["pixel_values_videos"] is not None:
            inputs["pixel_values_videos"] = inputs["pixel_values_videos"].to(
                device=self.device,
                dtype=self.model_dtype,
            )
        
        # ç¼–ç ï¼ˆç”Ÿæˆ KV Cacheï¼‰
        with torch.inference_mode():
            outputs = self.model(
                **{k: v for k, v in inputs.items() if k not in ["attention_mask"]},
                attention_mask=inputs.get("attention_mask", None),
                past_key_values=None,  # ä»é›¶å¼€å§‹
                use_cache=True,
                output_hidden_states=False,
                logits_to_keep=1,
            )
        
        # ä¿å­˜ KV Cache
        self.video_cache = self._detach_past(outputs.past_key_values)
        self.cache_is_valid = True
        self.encode_count += 1
        
        t_end = time.time()
        
        # æå–è§†è§‰ token æ•°é‡ï¼ˆåœ¨é‡Šæ”¾ inputs ä¹‹å‰ï¼‰
        visual_tokens = self._extract_visual_tokens_from_inputs(inputs)
        cache_length = self._get_past_len(self.video_cache)
        video_grid_thw = inputs.get('video_grid_thw')
        
        # ========== ç«‹å³é‡Šæ”¾ä¸­é—´å˜é‡ï¼Œå›æ”¶æ˜¾å­˜ ==========
        del outputs
        del inputs
        del all_frames
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        
        print(f"   âœ… ç¼–ç å®Œæˆï¼")
        print(f"   â±ï¸  è€—æ—¶: {t_end - t_start:.2f}s")
        print(f"   ğŸ¯ Visual Tokens: {visual_tokens}")
        print(f"   ğŸ’¾ KV Cache Length: {cache_length}")
        print(f"   ğŸ“ video_grid_thw: {video_grid_thw}")
        
        result = {
            'success': True,
            'frames_encoded': frames_encoded_count,
            'visual_tokens': visual_tokens,
            'cache_length': cache_length,
            'encoding_time': t_end - t_start,
            'video_grid_thw': video_grid_thw,
        }
        
        if self.last_sample_metadata:
            result['sample_metadata'] = self.last_sample_metadata
        
        return result
    
    def _estimate_original_fps(self) -> float:
        """ä¼°ç®—åŸå§‹å¸§ç‡ï¼ˆåŸºäºå¸§ç®¡ç†å™¨çš„æ—¶é—´æˆ³ï¼‰"""
        # ä» SmartFrameManager è·å–æ—¶é—´æˆ³ä¿¡æ¯
        timestamps = []
        for entry in self.frame_manager.star_memory:
            timestamps.append(entry['timestamp'])
        for entry in self.frame_manager.stream_memory:
            timestamps.append(entry['timestamp'])
        
        if len(timestamps) < 2:
            return 30.0  # é»˜è®¤30fps
        
        timestamps = sorted(timestamps)
        duration = timestamps[-1] - timestamps[0]
        
        if duration > 0:
            return len(timestamps) / duration
        return 30.0
    
    def ask(
        self,
        question: str,
        max_new_tokens: int = 512,
        min_new_tokens: int = 1,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1,
    ) -> Tuple[str, Dict]:
        """
        æé—®ï¼ˆå¦‚æœ cache å¤±æ•ˆï¼Œä¼šè‡ªåŠ¨é‡æ–°ç¼–ç æ‰€æœ‰å¸§ï¼‰
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            å…¶ä»–å‚æ•°: ç”Ÿæˆå‚æ•°
        
        Returns:
            (answer, metrics)
        """
        t_total_start = time.time()
        
        # 1. å¦‚æœ cache å¤±æ•ˆï¼Œé‡æ–°ç¼–ç æ‰€æœ‰å¸§
        encode_result = None
        if not self.cache_is_valid:
            encode_result = self._encode_all_frames()
            if not encode_result['success']:
                return f"Error: {encode_result['reason']}", {}
        
        # 2. ç¼–ç é—®é¢˜
        question_prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        question_inputs = self.processor.tokenizer(
            question_prompt,
            return_tensors="pt",
            add_special_tokens=False,
        ).to(self.device)
        
        # 3. æ„å»ºå®Œæ•´çš„ attention mask
        past_len = self._get_past_len(self.video_cache)
        full_mask = self._build_full_attention_mask(question_inputs.attention_mask, past_len)
        
        cache_position = None
        if past_len and past_len > 0:
            cache_position = torch.arange(
                past_len,
                past_len + question_inputs.input_ids.shape[1],
                device=question_inputs.input_ids.device,
            )
        
        # 4. ç”Ÿæˆå›ç­”
        t_gen_start = time.time()
        with torch.inference_mode():
            generated_ids = self.model.generate(
                input_ids=question_inputs.input_ids,
                attention_mask=full_mask,
                past_key_values=self.video_cache,
                cache_position=cache_position,
                max_new_tokens=max_new_tokens,
                min_new_tokens=min_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                pad_token_id=self.processor.tokenizer.pad_token_id,
                eos_token_id=self.processor.tokenizer.eos_token_id,
                use_cache=True,
            )
        t_gen_end = time.time()
        
        # 5. è§£ç è¾“å‡º
        output_ids = generated_ids[0, question_inputs.input_ids.shape[1]:]
        answer = self.processor.tokenizer.decode(output_ids, skip_special_tokens=True)
        output_token_count = len(output_ids)
        
        # ========== å…³é”®ï¼šé‡Šæ”¾ generate äº§ç”Ÿçš„ä¸´æ—¶å˜é‡ ==========
        del generated_ids
        del question_inputs
        del full_mask
        if cache_position is not None:
            del cache_position
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        
        t_total_end = time.time()
        
        # 6. ç»Ÿè®¡ä¿¡æ¯
        metrics = {
            'total_latency': t_total_end - t_total_start,
            'generation_latency': t_gen_end - t_gen_start,
            'output_tokens': output_token_count,
        }
        
        if encode_result:
            metrics['encoding_latency'] = encode_result['encoding_time']
            metrics['frames_encoded'] = encode_result['frames_encoded']
            metrics['visual_tokens'] = encode_result['visual_tokens']
        
        return answer, metrics
    
    def ask_stream(
        self,
        question: str,
        max_new_tokens: int = 512,
        min_new_tokens: int = 1,
        do_sample: bool = True,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1,
        timeout: float = 60.0,
    ) -> Generator[str, None, Dict]:
        """
        æµå¼æé—®ï¼ˆToken Streaming è¾“å‡ºï¼‰- å¤ç”¨å®˜æ–¹ TextIteratorStreamer æ–¹æ¡ˆ
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            timeout: streamer è¶…æ—¶æ—¶é—´
            å…¶ä»–å‚æ•°: ç”Ÿæˆå‚æ•°
        
        Yields:
            str: é€ä¸ªç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        
        Returns:
            æœ€åé€šè¿‡ generator.send(None) æˆ–éå†å®Œåå¯è·å– metricsï¼ˆå®é™…é€šè¿‡å±æ€§ï¼‰
        
        Usage:
            for text in engine.ask_stream(question):
                print(text, end='', flush=True)
            print()  # æ¢è¡Œ
            # metrics å¯é€šè¿‡ engine.last_stream_metrics è·å–
        """
        t_total_start = time.time()
        
        # 1. å¦‚æœ cache å¤±æ•ˆï¼Œé‡æ–°ç¼–ç æ‰€æœ‰å¸§
        encode_result = None
        if not self.cache_is_valid:
            encode_result = self._encode_all_frames()
            if not encode_result['success']:
                yield f"Error: {encode_result['reason']}"
                return
        
        # 2. ç¼–ç é—®é¢˜
        question_prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        question_inputs = self.processor.tokenizer(
            question_prompt,
            return_tensors="pt",
            add_special_tokens=False,
        ).to(self.device)
        
        # 3. æ„å»ºå®Œæ•´çš„ attention mask
        past_len = self._get_past_len(self.video_cache)
        full_mask = self._build_full_attention_mask(question_inputs.attention_mask, past_len)
        
        cache_position = None
        if past_len and past_len > 0:
            cache_position = torch.arange(
                past_len,
                past_len + question_inputs.input_ids.shape[1],
                device=question_inputs.input_ids.device,
            )
        
        # 4. åˆ›å»º Streamerï¼ˆå¤ç”¨å®˜æ–¹æ–¹æ¡ˆï¼‰
        streamer = TextIteratorStreamer(
            self.processor.tokenizer,
            timeout=timeout,
            skip_prompt=True,
            skip_special_tokens=True,
        )
        
        # 5. æ„å»ºç”Ÿæˆå‚æ•°
        gen_kwargs = {
            'input_ids': question_inputs.input_ids,
            'attention_mask': full_mask,
            'past_key_values': self.video_cache,
            'cache_position': cache_position,
            'max_new_tokens': max_new_tokens,
            'min_new_tokens': min_new_tokens,
            'do_sample': do_sample,
            'temperature': temperature,
            'top_p': top_p,
            'repetition_penalty': repetition_penalty,
            'pad_token_id': self.processor.tokenizer.pad_token_id,
            'eos_token_id': self.processor.tokenizer.eos_token_id,
            'use_cache': True,
            'streamer': streamer,
        }
        
        # 6. åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ generateï¼ˆå®˜æ–¹æ–¹æ¡ˆï¼‰
        t_gen_start = time.time()
        thread = Thread(target=self.model.generate, kwargs=gen_kwargs)
        thread.start()
        
        # 7. æµå¼è¾“å‡º
        output_tokens = 0
        generated_text = ""
        for new_text in streamer:
            if new_text:
                output_tokens += 1  # è¿‘ä¼¼è®¡æ•°
                generated_text += new_text
                yield new_text
        
        # 8. ç­‰å¾…ç”Ÿæˆå®Œæˆ
        thread.join()
        t_gen_end = time.time()
        t_total_end = time.time()
        
        # ========== å…³é”®ï¼šé‡Šæ”¾ generate äº§ç”Ÿçš„ä¸´æ—¶å˜é‡ ==========
        del gen_kwargs
        del question_inputs
        del full_mask
        if cache_position is not None:
            del cache_position
        del streamer
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        
        # 9. ä¿å­˜ metrics åˆ°å®ä¾‹å±æ€§ï¼ˆä¾›è°ƒç”¨è€…è·å–ï¼‰
        self.last_stream_metrics = {
            'total_latency': t_total_end - t_total_start,
            'generation_latency': t_gen_end - t_gen_start,
            'output_tokens': output_tokens,
            'generated_text': generated_text,
        }
        
        if encode_result:
            self.last_stream_metrics['encoding_latency'] = encode_result['encoding_time']
            self.last_stream_metrics['frames_encoded'] = encode_result['frames_encoded']
            self.last_stream_metrics['visual_tokens'] = encode_result['visual_tokens']
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        frame_stats = self.frame_manager.get_statistics()
        
        return {
            'total_frames_added': self.total_frames_processed,
            'encode_count': self.encode_count,
            'cache_valid': self.cache_is_valid,
            **frame_stats,
        }
    
    def reset(self):
        """é‡ç½®å¼•æ“"""
        self.frame_manager.reset()
        self.video_cache = None
        self.cache_is_valid = False
        self.encode_count = 0
        self.total_frames_processed = 0
        
        gc.collect()
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
        
        print("ğŸ”„ DelayedBatchInferenceEngine Reset.")
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _detach_past(self, past_key_values):
        """åˆ†ç¦» past_key_values ä»è®¡ç®—å›¾"""
        if past_key_values is None:
            return None
        if hasattr(past_key_values, "get_seq_length"):
            return past_key_values
        return tuple(tuple(p.detach() for p in layer) for layer in past_key_values)
    
    def _get_past_len(self, past_key_values):
        """è·å– past_key_values çš„é•¿åº¦"""
        if past_key_values is None:
            return 0
        if hasattr(past_key_values, "get_seq_length"):
            return past_key_values.get_seq_length()
        return past_key_values[0][0].shape[-2]
    
    def _build_full_attention_mask(self, attention_mask, past_len):
        """æ„å»ºå®Œæ•´çš„ attention maskï¼ˆåŒ…æ‹¬ pastï¼‰"""
        if past_len is None or past_len == 0:
            return attention_mask
        past_mask = torch.ones(
            (attention_mask.shape[0], past_len),
            dtype=attention_mask.dtype,
            device=attention_mask.device,
        )
        return torch.cat([past_mask, attention_mask], dim=1)
    
    def _extract_visual_tokens_from_inputs(self, inputs):
        """ä» processor è¾“å‡ºä¸­æå– visual tokens çš„æ•°é‡"""
        if "video_grid_thw" not in inputs:
            return 0
        
        video_grid_thw = inputs["video_grid_thw"]
        merge_length = getattr(self.processor, "merge_size", 2) ** 2
        
        # video_grid_thw shape: (num_videos, 3) -> (T, H, W)
        num_video_tokens = (video_grid_thw[0].prod() // merge_length).item() if len(video_grid_thw) > 0 else 0
        return num_video_tokens
