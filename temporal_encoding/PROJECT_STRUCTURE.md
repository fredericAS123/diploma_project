# ğŸ“ Diploma Project: Streaming VLM Temporal Encoding

åŸºäº Qwen2.5-VL çš„æµå¼è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆStreaming VLMï¼‰æ¨ç†ç³»ç»Ÿã€‚

å®ç°**æŒç»­å‰å‘ä¼ æ’­ä¿å­˜ KV Cacheï¼Œåœ¨æ”¶åˆ°ç”¨æˆ·é—®é¢˜æ—¶å¿«é€Ÿä½¿ç”¨å·²æœ‰ KV Cache è¿›è¡Œå›ç­”**çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
diploma_project/
â”‚
â”œâ”€â”€ README.md                              # é¡¹ç›®æ ¹è¯´æ˜
â”‚
â”œâ”€â”€ temporal_encoding/                     # â­ æ ¸å¿ƒæ¨¡å—ï¼šæµå¼æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ model/                             # æµå¼æ¨ç†æ ¸å¿ƒä»£ç 
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # æ¨¡å—å¯¼å‡º (StreamQwenModel, VideoStreamingInference, KVCacheManager)
â”‚   â”‚   â”œâ”€â”€ stream_qwen_model.py           # æµå¼ M-RoPE ä½ç½®è¿½è¸ªæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ video_stream_inference.py      # é«˜å±‚æµå¼æ¨ç†å¼•æ“
â”‚   â”‚   â””â”€â”€ cache_manager.py              # KV Cache ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ test_step1_cache.py               # GPU æµ‹è¯•ï¼šKV Cache + Stream State å¿«ç…§æ¢å¤
â”‚   â”œâ”€â”€ test_step2_cache_logic.py          # CPU æµ‹è¯•ï¼šKVCacheManager çº¯é€»è¾‘
â”‚   â”œâ”€â”€ test_step3_prompt.py              # CPU æµ‹è¯•ï¼šPrompt è£å‰ªé€»è¾‘
â”‚   â”œâ”€â”€ test_step4_choice_cache.py         # GPU æµ‹è¯•ï¼šask_choice() ç¼“å­˜éš”ç¦»
â”‚   â”œâ”€â”€ test_step5_e2e.py                 # GPU æµ‹è¯•ï¼šç«¯åˆ°ç«¯å¤šå¸§æ—¶åºç†è§£
â”‚   â”œâ”€â”€ test_step6_stream_vs_native.py     # ğŸ”¥ GPU æµ‹è¯•ï¼šæµå¼ vs åŸç”Ÿç¦»çº¿å¯¹æ¯”
â”‚   â”œâ”€â”€ test_step7_multi_chunk.py          # GPU æµ‹è¯•ï¼šå¤šå¸§ Chunk è§„æ¨¡æ€§èƒ½
â”‚   â””â”€â”€ TESTING_PROMPT.md                  # æµ‹è¯•æ–‡æ¡£ä¸è¿è¡ŒæŒ‡å—
â”‚
â”œâ”€â”€ qwen2_5_vl/                            # å‚è€ƒä»£ç ä¸åˆ†æè„šæœ¬
â”‚   â”œâ”€â”€ configuration_qwen2_5_vl.py        # Qwen2.5-VL æ¨¡å‹é…ç½®æºç 
â”‚   â”œâ”€â”€ modeling_qwen2_5_vl.py             # Qwen2.5-VL æ¨¡å‹å®ç°æºç 
â”‚   â”œâ”€â”€ modular_qwen2_5_vl.py             # æ¨¡å—åŒ–æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ processing_qwen2_5_vl.py           # å¤„ç†å™¨å®ç°
â”‚   â”œâ”€â”€ task1_inference_verify.py          # Task 1: åŸºç¡€æ¨ç†éªŒè¯
â”‚   â”œâ”€â”€ task2_mrope_analysis.py            # Task 2: M-RoPE åˆ†æè„šæœ¬
â”‚   â”œâ”€â”€ task2_mrope_analysis_report.txt    # M-RoPE åˆ†ææŠ¥å‘Š
â”‚   â”œâ”€â”€ task3_stream_mrope_analysis.py     # Task 3: æµå¼ M-RoPE åˆ†æ
â”‚   â”œâ”€â”€ task3_mrope_report.txt             # æµå¼ M-RoPE æŠ¥å‘Š
â”‚   â”œâ”€â”€ task4_video_native_analysis.py     # Task 4: åŸç”Ÿè§†é¢‘åˆ†æ
â”‚   â”œâ”€â”€ task4_mrope_report.txt             # è§†é¢‘ M-RoPE æŠ¥å‘Š
â”‚   â”œâ”€â”€ task5_stream_absolute_time_experiment.py  # Task 5: ç»å¯¹æ—¶é—´å®éªŒ
â”‚   â””â”€â”€ task5_stream_absolute_time_report.txt     # ç»å¯¹æ—¶é—´å®éªŒæŠ¥å‘Š
â”‚
â””â”€â”€ web_demo/                              # Web æ¼”ç¤ºç•Œé¢
    â”œâ”€â”€ main.py                            # FastAPI å…¥å£
    â”œâ”€â”€ Qwen_inference.py                  # æ¨ç†å°è£…
    â”œâ”€â”€ RoPE_learning.py                   # RoPE å­¦ä¹ è„šæœ¬
    â”œâ”€â”€ test_Qwen.py                       # å¿«é€Ÿæµ‹è¯•
    â”œâ”€â”€ webui_gradio.py                    # Gradio Web UI
    â””â”€â”€ webui_Qwen2_5_3B.py               # 3B æ¨¡å‹ Web UI
```

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### ç³»ç»Ÿè®¾è®¡

```
[è§†é¢‘æµ]
   â”‚
   â”œâ”€ Frame 1,2 â”€â”€> append_video_chunk() â”€â”€> ViT (Conv3D) â”€â”€> LLM Prefill â”€â”€> KV Cache
   â”œâ”€ Frame 3,4 â”€â”€> append_video_chunk() â”€â”€> ViT (Conv3D) â”€â”€> LLM Chunk Prefill â”€â”€> KV Cache (ç´¯ç§¯)
   â”œâ”€ Frame 5,6 â”€â”€> ...
   â”‚
   â””â”€ ç”¨æˆ·æé—® â”€â”€> ask() â”€â”€> Snapshot Cache â”€â”€> QA Prefill â”€â”€> Decode â”€â”€> ç­”æ¡ˆ
                                                                              â”‚
                                                                   Restore Cache (ä¿æŠ¤è§†é¢‘æµçŠ¶æ€)
```

### ä¸‰å±‚æ¶æ„

| å±‚çº§ | æ–‡ä»¶ | èŒè´£ |
|------|------|------|
| **åº”ç”¨å±‚** | `video_stream_inference.py` | é«˜å±‚ APIï¼š`append_frame()`, `ask()`, `ask_choice()`, `reset()` |
| **æ¨¡å‹å±‚** | `stream_qwen_model.py` | 3 åˆ†æ”¯ M-RoPE ä½ç½®è¿½è¸ª + `stream_state` ç®¡ç† |
| **ç¼“å­˜å±‚** | `cache_manager.py` | KV Cache ç”Ÿå‘½å‘¨æœŸï¼šsnapshot/restore/clone/clear |

---

## ğŸ”‘ å…³é”®æŠ€æœ¯ç‚¹

### 1. 3 åˆ†æ”¯ M-RoPE ä½ç½®è¿½è¸ª

åŸºäº [StreamingVLM](https://github.com/mit-han-lab/streaming-vlm) çš„ Append æ¨¡å¼ï¼š

| åˆ†æ”¯ | æ¡ä»¶ | è¡Œä¸º |
|------|------|------|
| **Branch 1** (é¦–æ¬¡ Prefill) | æ—  KV Cache | æ ‡å‡† `get_rope_index` è®¡ç®— 3D (T,H,W) ä½ç½® |
| **Branch 2** (Chunk Prefill) | æœ‰ Cache + `seq_len > 1` | å±€éƒ¨ `get_rope_index` + å…¨å±€åç§» `offset = last_cache_position + 1` |
| **Branch 3** (Decode) | æœ‰ Cache + `seq_len == 1` | `position = last_cache_position + 1`ï¼ˆ3 ç»´ç»Ÿä¸€ï¼‰ |

**ä½ç½®è¿½è¸ªï¼š**
```python
# å– 3 ç»´çš„è·¨ç»´åº¦æœ€å¤§å€¼ï¼ˆä¸ get_rope_index ä¸­ st_idx = .max()+1 è¯­ä¹‰ä¸€è‡´ï¼‰
self._last_cache_position = int(position_ids[:, 0, -1].max().item())
```

### 2. KV Cache å¿«ç…§/æ¢å¤

```python
# ask() å‰ï¼šä¿æŠ¤è§†é¢‘ç¼“å­˜ + æ¨¡å‹æµå¼çŠ¶æ€
cache_manager.snapshot(model)   # æ·±æ‹·è´ cache + model.stream_state

# QA å®Œæˆåï¼šæ¢å¤åˆ°é—®ç­”å‰çš„çŠ¶æ€
cache_manager.restore(model)    # cache + stream_state ä¸€å¹¶æ¢å¤

# ç»§ç»­è¿½åŠ æ–°å¸§ï¼šä½ç½®è®¡ç®—è‡ªåŠ¨ä»æ­£ç¡®ä½ç½®ç»§ç»­
engine.append_video_chunk(new_frames)
```

### 3. Qwen2.5-VL åŒ RoPE ç³»ç»Ÿ

| ç»„ä»¶ | RoPE ç±»å‹ | ç»´åº¦ | ä½œç”¨åŸŸ |
|------|-----------|------|--------|
| **ViT** | 2D (H, W) | ç©ºé—´ä½ç½® | Chunk å†…æ³¨æ„åŠ›ï¼ˆé›¶è·¨ chunk äº¤äº’ï¼‰ |
| **LLM** | 3D M-RoPE (T, H, W) | æ—¶ç©ºä½ç½® | å…¨å±€åºåˆ—ï¼Œ`mrope_section` é€šé“åˆ†å‰² |

**å…³é”®å‘ç°ï¼š** ViT å¯¹ä¸åŒ temporal chunk ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä½ç½®ç¼–ç ï¼ˆ.repeat(t,1)ï¼‰ï¼Œæ—¶åºå»ºæ¨¡å®Œå…¨ç”± LLM çš„ M-RoPE è´Ÿè´£ã€‚

---

## ğŸ”§ æ ¸å¿ƒ API

### VideoStreamingInference

```python
from temporal_encoding.model import StreamQwenModel, VideoStreamingInference
from transformers import AutoProcessor

# åˆå§‹åŒ–
processor = AutoProcessor.from_pretrained(model_path)
model = StreamQwenModel.from_pretrained(model_path, torch_dtype=torch.bfloat16).to("cuda")
engine = VideoStreamingInference(model, processor, "cuda")

# æµå¼ç¼–ç ï¼ˆæ¨è 4 å¸§ chunkï¼‰
engine.append_video_chunk([frame0, frame1, frame2, frame3], fps=4.0)
engine.append_video_chunk([frame4, frame5, frame6, frame7], fps=4.0)

# å›ç­”é—®é¢˜ï¼ˆä¸æ±¡æŸ“è§†é¢‘ç¼“å­˜ï¼‰
answer, metrics = engine.ask("What happened?", max_new_tokens=128, update_state=False)
print(f"Answer: {answer}")
print(f"TTFT: {metrics['ttft']:.3f}s")

# ç»§ç»­è¿½åŠ å¸§
engine.append_video_chunk([frame8, frame9], fps=2.0)

# å¤šé€‰é¢˜
choice = engine.ask_choice("What color?", choices=["Red", "Blue", "Green"])

# ç›‘æ§
info = engine.get_cache_info()

# é‡ç½®ï¼ˆæ–°è§†é¢‘ï¼‰
engine.reset()
```

### StreamQwenModel

```python
# stream_state å¯¼å‡º/æ¢å¤ï¼ˆç”¨äºè‡ªå®šä¹‰ç¼“å­˜ç®¡ç†ï¼‰
state = model.stream_state                # å¯¼å‡º
model.stream_state = saved_state           # æ¢å¤
model.reset_stream_state()                 # é‡ç½®

# forward æ—¶è‡ªåŠ¨è®¡ç®— position_idsï¼ˆå¤–éƒ¨æ— éœ€ä¼ å…¥ï¼‰
outputs = model(input_ids=ids, attention_mask=mask, past_key_values=cache, use_cache=True)
```

### KVCacheManager

```python
manager = KVCacheManager()
manager.cache = outputs.past_key_values    # ä¿å­˜ç¼“å­˜

manager.snapshot(model)                    # å¿«ç…§ï¼ˆå« stream_stateï¼‰
# ... åš QA ...
manager.restore(model)                     # æ¢å¤

cloned = manager.clone(manager.cache)      # ç‹¬ç«‹å‰¯æœ¬
full_mask = manager.build_full_attention_mask(new_mask)
manager.clear()                            # é‡Šæ”¾å†…å­˜
```

---

## ğŸ› å·²ä¿®å¤çš„ Bug

### Bug 1 (Critical): `_last_cache_position` ç»´åº¦é”™è¯¯
- **é—®é¢˜ï¼š** åªå–äº† T ç»´ `position_ids[0, 0, -1]`ï¼Œå¿½ç•¥äº† H/W å¯èƒ½æ›´å¤§
- **ä¿®å¤ï¼š** `position_ids[:, 0, -1].max().item()` å–è·¨ç»´åº¦æœ€å¤§å€¼
- **å½±å“ï¼š** åç»­ chunk åç§»é”™è¯¯å¯¼è‡´ä½ç½®å†²çª

### Bug 2 (Critical): `rope_deltas` ä½¿ç”¨è¿‡æœŸå€¼
- **é—®é¢˜ï¼š** `StreamQwenModelOutput.rope_deltas` ä½¿ç”¨äº†çˆ¶ç±» `outputs.rope_deltas`
- **ä¿®å¤ï¼š** ä½¿ç”¨æˆ‘ä»¬è®¡ç®—çš„ `rope_deltas` å€¼
- **å½±å“ï¼š** åç»­ decode ä½ç½®è®¡ç®—é”™è¯¯

### Bug 3 (Medium): TTFT æµ‹é‡ç‚¹é”™è¯¯
- **é—®é¢˜ï¼š** TTFT åœ¨ç¬¬ä¸€ä¸ª decode step ä¹‹åæ‰è®°å½•
- **ä¿®å¤ï¼š** ç§»åŠ¨åˆ° prefill å®Œæˆåç«‹å³è®°å½•
- **å½±å“ï¼š** TTFT æŒ‡æ ‡ä¸å‡†ç¡®ï¼ˆåŒ…å«äº†ä¸€æ¬¡ decode å»¶è¿Ÿï¼‰

---

## ğŸ“Š æ¨è Chunk é…ç½®

| Chunk å¤§å° | temporal_patch_size å¯¹é½ | T å€¼ | ç‰¹ç‚¹ | æ¨èåœºæ™¯ |
|------------|-------------------------|------|------|----------|
| 2 å¸§ | âœ… | 1 | æœ€ä½å»¶è¿Ÿ | å®æ—¶äº¤äº’ |
| 4 å¸§ | âœ… | 2 | å»¶è¿Ÿ/è´¨é‡å‡è¡¡ | **é€šç”¨æ¨è** |
| 6 å¸§ | âœ… | 3 | è¾ƒé«˜åå | å‡†å®æ—¶ |
| 8 å¸§ | âœ… | 4 | æœ€é«˜åå | æ‰¹å¤„ç† |
| 3 å¸§ | âŒï¼ˆå¡«å……è‡³ 4ï¼‰ | 2 | æµªè´¹è®¡ç®— | ä¸æ¨è |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. ç¯å¢ƒå‡†å¤‡
pip install torch transformers accelerate Pillow opencv-python

# 2. è¿è¡Œ CPU æµ‹è¯•éªŒè¯é€»è¾‘
cd temporal_encoding
python test_step2_cache_logic.py
python test_step3_prompt.py

# 3. è¿è¡Œ GPU æµ‹è¯•éªŒè¯åŠŸèƒ½
python test_step1_cache.py
python test_step5_e2e.py

# 4. ğŸ”¥ è¿è¡Œæ ¸å¿ƒå¯¹æ¯”æµ‹è¯•
python test_step6_stream_vs_native.py
```

---

**Last Updated:** 2026-02-10
