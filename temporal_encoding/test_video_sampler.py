"""
è§†é¢‘é‡‡æ ·å’Œæ—¶é—´ç¼–ç æµ‹è¯•

æµ‹è¯•ç›®æ ‡ï¼š
1. éªŒè¯é‡‡æ ·åå¸§æ•°æ­£ç¡®
2. éªŒè¯ second_per_grid_t è®¡ç®—æ­£ç¡®
3. éªŒè¯æ—¶é—´ç¼–ç èƒ½æ­£ç¡®è¦†ç›–è§†é¢‘æ—¶é•¿
4. ç«¯åˆ°ç«¯æµ‹è¯•ä¸æ¨¡å‹é›†æˆ
"""

import pytest
import torch
import numpy as np
from PIL import Image
from typing import List
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from model.video_sampler import (
    VideoSampler,
    calculate_expected_temporal_positions,
    validate_time_encoding,
)


class TestVideoSampler:
    """è§†é¢‘é‡‡æ ·å™¨å•å…ƒæµ‹è¯•"""
    
    @pytest.fixture
    def create_dummy_frames(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„å‡è§†é¢‘å¸§"""
        def _create(num_frames: int, size: tuple = (224, 224)) -> List[Image.Image]:
            frames = []
            for i in range(num_frames):
                # åˆ›å»ºæœ‰æ—¶é—´æ ‡è®°çš„å¸§
                arr = np.zeros((*size, 3), dtype=np.uint8)
                arr[:, :, 0] = i % 256  # Ré€šé“ç¼–ç å¸§å·
                frames.append(Image.fromarray(arr))
            return frames
        return _create
    
    def test_basic_sampling_1fps(self, create_dummy_frames):
        """æµ‹è¯•1fpsé‡‡æ · - 50ç§’è§†é¢‘åº”è¯¥å¾—åˆ°50å¸§"""
        # åˆ›å»º50ç§’30fpsçš„è§†é¢‘ = 1500å¸§
        original_fps = 30.0
        duration = 50.0
        total_frames = int(duration * original_fps)
        frames = create_dummy_frames(total_frames)
        
        # 1fpsé‡‡æ ·
        sampler = VideoSampler(target_fps=1.0)
        sampled, second_per_grid_t, meta = sampler.sample_frames(
            frames=frames,
            original_fps=original_fps,
            video_duration=duration,
        )
        
        # éªŒè¯å¸§æ•°ï¼ˆåº”è¯¥æ˜¯50ï¼Œå¯¹é½åˆ°temporal_patch_size=2çš„å€æ•°ï¼‰
        assert len(sampled) == 50, f"Expected 50 frames, got {len(sampled)}"
        
        # éªŒè¯æ˜¯temporal_patch_sizeçš„å€æ•°
        assert len(sampled) % 2 == 0, "Frame count should be multiple of temporal_patch_size"
        
        # éªŒè¯second_per_grid_t
        # 50å¸§ / 2 = 25ä¸ªtemporal grids
        # 50ç§’ / 25 grids = 2ç§’/grid
        expected_second_per_grid = duration / (len(sampled) // 2)
        assert abs(second_per_grid_t - expected_second_per_grid) < 0.01, \
            f"second_per_grid_t mismatch: {second_per_grid_t} vs {expected_second_per_grid}"
        
        print(f"âœ… 1fpsé‡‡æ ·æµ‹è¯•é€šè¿‡")
        print(f"   åŸå§‹: {total_frames}å¸§ @ {original_fps}fps")
        print(f"   é‡‡æ ·å: {len(sampled)}å¸§")
        print(f"   second_per_grid_t: {second_per_grid_t}s")
        print(f"   å‹ç¼©æ¯”: {meta['compression_ratio']:.2f}x")
    
    def test_time_encoding_validation(self, create_dummy_frames):
        """æµ‹è¯•æ—¶é—´ç¼–ç éªŒè¯"""
        # åˆ›å»º10ç§’è§†é¢‘
        frames = create_dummy_frames(300)  # 30fps * 10ç§’
        
        sampler = VideoSampler(target_fps=2.0)  # 2fps
        sampled, second_per_grid_t, meta = sampler.sample_frames(
            frames=frames,
            original_fps=30.0,
            video_duration=10.0,
        )
        
        # éªŒè¯æ—¶é—´ç¼–ç 
        is_valid, details = validate_time_encoding(
            sampled_frames=len(sampled),
            second_per_grid_t=second_per_grid_t,
            expected_duration=10.0,
            tolerance=0.5,  # å…è®¸0.5ç§’è¯¯å·®
        )
        
        assert is_valid, f"Time encoding validation failed: {details}"
        
        print(f"âœ… æ—¶é—´ç¼–ç éªŒè¯é€šè¿‡")
        print(f"   è¦†ç›–æ—¶é•¿: {details['total_covered_time']:.2f}s")
        print(f"   é¢„æœŸæ—¶é•¿: {details['expected_duration']:.2f}s")
        print(f"   è¯¯å·®: {details['time_error']:.2f}s")
    
    def test_temporal_position_calculation(self):
        """æµ‹è¯•temporal positionè®¡ç®—ä¸å®˜æ–¹ä¸€è‡´"""
        # æ¨¡æ‹Ÿï¼š20å¸§ï¼Œæ¯ä¸ªgrid 2ç§’ï¼Œtokens_per_second=4
        num_frames = 20
        second_per_grid_t = 2.0
        
        positions = calculate_expected_temporal_positions(
            num_frames=num_frames,
            second_per_grid_t=second_per_grid_t,
            temporal_patch_size=2,
            tokens_per_second=4,
        )
        
        # æœŸæœ›çš„ä½ç½®åºåˆ—ï¼š
        # Grid 0: 0*2*4=0, å¸§0å’Œå¸§1çš„ä½ç½®éƒ½æ˜¯0
        # Grid 1: 1*2*4=8, å¸§2å’Œå¸§3çš„ä½ç½®éƒ½æ˜¯8
        # Grid 2: 2*2*4=16, ...
        # ä»¥æ­¤ç±»æ¨
        expected = []
        for grid_idx in range(10):  # 20å¸§/2 = 10ä¸ªgrids
            pos = grid_idx * 2 * 4
            expected.extend([pos, pos])  # æ¯ä¸ªgridæœ‰2å¸§
        
        assert positions == expected, f"Position mismatch:\n  Got: {positions}\n  Expected: {expected}"
        
        print(f"âœ… Temporal positionè®¡ç®—æµ‹è¯•é€šè¿‡")
        print(f"   Positions: {positions[:10]}... (showing first 10)")
    
    def test_different_fps_scenarios(self, create_dummy_frames):
        """æµ‹è¯•ä¸åŒFPSåœºæ™¯"""
        test_cases = [
            {'target_fps': 0.5, 'duration': 60, 'original_fps': 30},  # 0.5fps, 60ç§’
            {'target_fps': 1.0, 'duration': 30, 'original_fps': 24},  # 1fps, 30ç§’
            {'target_fps': 2.0, 'duration': 10, 'original_fps': 60},  # 2fps, 10ç§’
            {'target_fps': 4.0, 'duration': 5, 'original_fps': 30},   # 4fps, 5ç§’
        ]
        
        for case in test_cases:
            total_frames = int(case['duration'] * case['original_fps'])
            frames = create_dummy_frames(total_frames)
            
            sampler = VideoSampler(target_fps=case['target_fps'])
            sampled, second_per_grid_t, meta = sampler.sample_frames(
                frames=frames,
                original_fps=case['original_fps'],
                video_duration=case['duration'],
            )
            
            # éªŒè¯å¸§æ•°åˆç†
            expected_frames = case['duration'] * case['target_fps']
            # å¯¹é½åˆ°temporal_patch_sizeçš„å€æ•°
            expected_frames = max(2, int(expected_frames // 2) * 2)
            
            assert abs(len(sampled) - expected_frames) <= 2, \
                f"Frame count mismatch for {case}: got {len(sampled)}, expected ~{expected_frames}"
            
            # éªŒè¯æ—¶é—´ç¼–ç 
            is_valid, _ = validate_time_encoding(
                sampled_frames=len(sampled),
                second_per_grid_t=second_per_grid_t,
                expected_duration=case['duration'],
                tolerance=1.0,
            )
            assert is_valid, f"Time encoding invalid for {case}"
            
            print(f"âœ… FPS={case['target_fps']}, Duration={case['duration']}s: "
                  f"{len(sampled)} frames, second_per_grid_t={second_per_grid_t:.3f}s")


class TestIntegrationWithModel:
    """ä¸æ¨¡å‹é›†æˆçš„ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @pytest.fixture
    def model_and_processor(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆä½¿ç”¨å°æ¨¡å‹æˆ–mockï¼‰"""
        try:
            from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
            
            # å°è¯•åŠ è½½å°æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡
            model_name = "Qwen/Qwen2.5-VL-2B-Instruct"  # ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•
            
            processor = AutoProcessor.from_pretrained(model_name)
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
            )
            
            return model, processor
        except Exception as e:
            pytest.skip(f"Model not available: {e}")
    
    @pytest.mark.slow
    def test_second_per_grid_ts_injection(self, model_and_processor):
        """æµ‹è¯• second_per_grid_ts æ­£ç¡®æ³¨å…¥åˆ°æ¨¡å‹"""
        model, processor = model_and_processor
        
        # åˆ›å»ºæµ‹è¯•å¸§
        frames = [Image.new('RGB', (224, 224), color=(i*10, 0, 0)) for i in range(20)]
        
        # å‡†å¤‡è¾“å…¥
        messages = [{
            "role": "user",
            "content": [
                {"type": "video", "video": frames},
                {"type": "text", "text": "Describe this video."},
            ],
        }]
        
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], videos=[frames], return_tensors="pt").to(model.device)
        
        # æ³¨å…¥è‡ªå®šä¹‰ second_per_grid_ts
        custom_second_per_grid = 2.0  # æ¯ä¸ªgrid 2ç§’
        inputs['second_per_grid_ts'] = torch.tensor([custom_second_per_grid]).to(model.device)
        
        # è¿è¡Œå‰å‘ä¼ æ’­
        with torch.inference_mode():
            outputs = model(**inputs, output_hidden_states=True)
        
        # éªŒè¯è¾“å‡ºæœ‰æ•ˆ
        assert outputs.logits is not None
        assert outputs.logits.shape[0] == 1  # batch size = 1
        
        print(f"âœ… second_per_grid_ts æ³¨å…¥æµ‹è¯•é€šè¿‡")
        print(f"   æ³¨å…¥çš„ second_per_grid_t: {custom_second_per_grid}s")
        print(f"   è¾“å‡º logits shape: {outputs.logits.shape}")
    
    @pytest.mark.slow
    def test_end_to_end_with_sampler(self, model_and_processor):
        """ç«¯åˆ°ç«¯æµ‹è¯•ï¼šé‡‡æ · + æ—¶é—´ç¼–ç  + æ¨¡å‹æ¨ç†"""
        model, processor = model_and_processor
        
        # æ¨¡æ‹Ÿ30ç§’30fpsçš„è§†é¢‘
        original_fps = 30.0
        duration = 30.0
        total_frames = int(duration * original_fps)
        frames = [Image.new('RGB', (224, 224), color=(i % 256, 0, 0)) for i in range(total_frames)]
        
        # 1fpsé‡‡æ ·
        sampler = VideoSampler(target_fps=1.0)
        sampled_frames, second_per_grid_t, meta = sampler.sample_frames(
            frames=frames,
            original_fps=original_fps,
            video_duration=duration,
        )
        
        print(f"\nğŸ“Š é‡‡æ ·ç»“æœ:")
        print(f"   åŸå§‹: {total_frames} å¸§")
        print(f"   é‡‡æ ·å: {len(sampled_frames)} å¸§")
        print(f"   second_per_grid_t: {second_per_grid_t:.4f}s")
        
        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        messages = [{
            "role": "user",
            "content": [
                {"type": "video", "video": sampled_frames},
                {"type": "text", "text": "What happens in this video?"},
            ],
        }]
        
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], videos=[sampled_frames], return_tensors="pt").to(model.device)
        
        # æ³¨å…¥æ—¶é—´ç¼–ç å‚æ•°
        inputs['second_per_grid_ts'] = torch.tensor([second_per_grid_t]).to(model.device)
        
        # ç”Ÿæˆ
        with torch.inference_mode():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=50,
            )
        
        output_text = processor.batch_decode(
            generated_ids[:, inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )[0]
        
        print(f"\nğŸ¤– æ¨¡å‹è¾“å‡º: {output_text[:200]}...")
        print(f"âœ… ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡")


class TestTimeEncodingAccuracy:
    """æ—¶é—´ç¼–ç ç²¾åº¦æµ‹è¯•"""
    
    def test_frame_time_mapping(self):
        """æµ‹è¯•å¸§åˆ°æ—¶é—´çš„æ˜ å°„ç²¾åº¦"""
        # åœºæ™¯ï¼š50ç§’è§†é¢‘ï¼Œ1fpsé‡‡æ · = 50å¸§
        video_duration = 50.0
        target_fps = 1.0
        temporal_patch_size = 2
        tokens_per_second = 4
        
        sampler = VideoSampler(
            target_fps=target_fps,
            temporal_patch_size=temporal_patch_size,
            tokens_per_second=tokens_per_second,
        )
        
        # è®¡ç®—å‚æ•°
        num_frames = int(video_duration * target_fps)
        num_frames = max(temporal_patch_size, (num_frames // temporal_patch_size) * temporal_patch_size)
        num_grids = num_frames // temporal_patch_size
        second_per_grid_t = video_duration / num_grids
        
        print(f"\nğŸ“Š æ—¶é—´ç¼–ç ç²¾åº¦æµ‹è¯•:")
        print(f"   è§†é¢‘æ—¶é•¿: {video_duration}s")
        print(f"   é‡‡æ ·åå¸§æ•°: {num_frames}")
        print(f"   Temporal grids: {num_grids}")
        print(f"   second_per_grid_t: {second_per_grid_t}s")
        
        # è®¡ç®—æ¯ä¸ªgridå¯¹åº”çš„å®é™…æ—¶é—´
        print(f"\n   Gridæ—¶é—´æ˜ å°„:")
        for i in range(min(10, num_grids)):  # æ˜¾ç¤ºå‰10ä¸ª
            grid_time = i * second_per_grid_t
            position_id = int(i * second_per_grid_t * tokens_per_second)
            print(f"     Grid {i}: æ—¶é—´={grid_time:.2f}s, Position ID={position_id}")
        
        # éªŒè¯æœ€åä¸€ä¸ªgridçš„æ—¶é—´æ¥è¿‘è§†é¢‘ç»“æŸ
        last_grid_time = (num_grids - 1) * second_per_grid_t
        time_error = abs(last_grid_time - (video_duration - second_per_grid_t))
        
        assert time_error < 0.5, f"Last grid time error too large: {time_error}s"
        
        print(f"\n   æœ€åä¸€ä¸ªGridæ—¶é—´: {last_grid_time:.2f}s")
        print(f"   âœ… æ—¶é—´æ˜ å°„ç²¾åº¦æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    # è¿è¡Œå¿«é€Ÿæµ‹è¯•
    pytest.main([__file__, "-v", "-k", "not slow"])