"""
å®éªŒ B: OOM-Free é•¿è§†é¢‘å¤„ç†

éªŒè¯:
  1) å¯ç”¨ Level 1 KV Cache æ·˜æ±° (Sink + Window, å…¨è‡ªåŠ¨å‚æ•°)
  2) ä»¥ 4 å¸§/chunkã€fps=2 çš„æ–¹å¼é€æ®µç¼–ç æ•´ä¸ª 1.mp4
  3) æ˜¾å­˜ä¿æŒç¨³å®šï¼Œä¸ OOM
  4) cache_len åœ¨è§¦å‘æ·˜æ±°åä¿æŒ â‰¤ max_cache_tokens
  5) æœ€åæä¸€ä¸ªé—®é¢˜éªŒè¯ cache å¯ç”¨æ€§
"""
import os
import sys
import gc
import time
import torch
from datetime import datetime
from PIL import Image

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from model import StreamQwenModel, VideoStreamingInference, EvictionConfig

MODEL_PATH = os.environ.get(
    "QWEN_MODEL_PATH",
    "/root/autodl-tmp/Qwen/Qwen2___5-VL-3B-Instruct",
)
VIDEO_PATH = os.environ.get(
    "VIDEO_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/202208312002.mp4",
)
REPORT_PATH = os.environ.get(
    "REPORT_PATH",
    "/root/autodl-tmp/diploma_project/temporal_encoding/test_eviction_exp_b_report.txt",
)

# â”€â”€ æ·˜æ±°å‚æ•° (å…¨è‡ªåŠ¨) â”€â”€
# â¬‡ï¸ è¿™æ˜¯éœ€è¦å®éªŒè°ƒä¼˜çš„æ ¸å¿ƒè¶…å‚æ•°ã€‚
# 100K=ä¿å®ˆ(3.4GB cache), 130K=ä¸­ç­‰(4.5GB), 150K=æ¿€è¿›(5.2GB, æ¥è¿‘æé™)
# å³°å€¼ cache = max + 1 chunk (~5.4K), ä¸å¯è¶… ~155K (4090 24GB)
# è¿‡å°â†’windowä¸è¶³â†’è¿‘æœŸä¿¡æ¯ä¸¢å¤±â†’å›ç­”è´¨é‡ä¸‹é™; è¿‡å¤§â†’OOM
# å»ºè®®ä» 130K å¼€å§‹, è‹¥ç¨³å®šåˆ™å°è¯• 150K
MAX_CACHE_TOKENS = 150_000  # ä¸­ç­‰é…ç½®, ~4.5 GB cache, total ~11.6 GB

# â”€â”€ ç¼–ç å‚æ•° â”€â”€
CHUNK_FRAMES = 4      # æ¯æ¬¡è¿½åŠ  4 å¸§ (ä¸ test_step10 ä¸€è‡´)
SAMPLE_FPS = 2.0      # é‡‡æ ·å¸§ç‡
PRINT_INTERVAL = 10   # æ¯ 10 ä¸ª chunk æ‰“å°ä¸€æ¬¡


class TeeWriter:
    def __init__(self, *writers):
        self._writers = writers
    def write(self, text):
        for w in self._writers:
            w.write(text)
        self.flush()
    def flush(self):
        for w in self._writers:
            w.flush()


def get_vram_gb():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        return {
            "allocated": round(torch.cuda.memory_allocated() / (1024 ** 3), 3),
            "reserved": round(torch.cuda.memory_reserved() / (1024 ** 3), 3),
            "max_allocated": round(torch.cuda.max_memory_allocated() / (1024 ** 3), 3),
        }
    return {}


def extract_frames_from_video(video_path, fps=2.0):
    """ä»è§†é¢‘æ–‡ä»¶ä¸­æŒ‰æŒ‡å®š fps é‡‡æ ·å¸§ã€‚è¿”å› PIL Image åˆ—è¡¨ã€‚"""
    try:
        import decord
        from decord import VideoReader, cpu
        decord.bridge.set_bridge("native")
        vr = VideoReader(video_path, ctx=cpu(0))
        video_fps = vr.get_avg_fps()
        total_frames = len(vr)
        duration = total_frames / video_fps
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        print(f"  Video: {video_path}")
        print(f"  Duration: {duration:.1f}s, FPS: {video_fps:.1f}, Total frames: {total_frames}")
        print(f"  Sampling at {fps} fps â†’ {len(indices)} frames")
        frames = []
        for idx in indices:
            frame = vr[idx].asnumpy()
            frames.append(Image.fromarray(frame))
        return frames, duration
    except ImportError:
        import cv2
        cap = cv2.VideoCapture(video_path)
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / video_fps if video_fps > 0 else 0
        sample_interval = video_fps / fps
        indices = [int(i * sample_interval) for i in range(int(total_frames / sample_interval))]
        indices = [i for i in indices if i < total_frames]
        print(f"  Video: {video_path}")
        print(f"  Duration: {duration:.1f}s, FPS: {video_fps:.1f}, Total frames: {total_frames}")
        print(f"  Sampling at {fps} fps â†’ {len(indices)} frames")
        frames = []
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
        cap.release()
        return frames, duration


def main():
    report_dir = os.path.dirname(REPORT_PATH)
    if report_dir:
        os.makedirs(report_dir, exist_ok=True)

    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        tee = TeeWriter(sys.stdout, f)
        original_stdout, original_stderr = sys.stdout, sys.stderr
        sys.stdout = tee
        sys.stderr = tee

        try:
            print("=" * 70)
            print("EXPERIMENT B: OOM-Free Long Video Processing with KV Cache Eviction")
            print("=" * 70)
            print(f"Report time: {datetime.now().isoformat(timespec='seconds')}")
            print(f"Eviction config: max_cache_tokens={MAX_CACHE_TOKENS}, "
                  f"sink=auto, window=auto")
            print(f"Expected: test_step10 shows OOM at 40 chunks (160 frames) without eviction.")
            print(f"With eviction, should process ALL chunks without OOM.")
            print()

            # â”€â”€ 0) æ£€æŸ¥æ–‡ä»¶ â”€â”€
            if not os.path.exists(MODEL_PATH):
                print(f"âŒ Model not found: {MODEL_PATH}")
                return
            if not os.path.exists(VIDEO_PATH):
                print(f"âŒ Video not found: {VIDEO_PATH}")
                return

            # â”€â”€ 1) åŠ è½½æ¨¡å‹ â”€â”€
            print("[1] Loading model...")
            from transformers import AutoProcessor
            device = "cuda"
            dtype = torch.bfloat16
            processor = AutoProcessor.from_pretrained(MODEL_PATH)
            model = StreamQwenModel.from_pretrained(
                MODEL_PATH, torch_dtype=dtype
            ).to(device)
            model.eval()
            vram_model = get_vram_gb()
            print(f"  VRAM after model load: {vram_model}")
            print()

            # â”€â”€ 2) æå–è§†é¢‘å¸§ â”€â”€
            print("[2] Extracting frames from video...")
            all_frames, duration = extract_frames_from_video(VIDEO_PATH, fps=SAMPLE_FPS)
            total_frame_count = len(all_frames)
            expected_chunks = (total_frame_count + CHUNK_FRAMES - 1) // CHUNK_FRAMES
            print(f"  Total frames extracted: {total_frame_count}")
            print(f"  Expected chunks (4 frames/chunk): {expected_chunks}")
            print(f"  âš ï¸ Without eviction, OOM at ~40 chunks ({40*CHUNK_FRAMES} frames).")
            print(f"  With eviction (max={MAX_CACHE_TOKENS}), should handle all {expected_chunks} chunks.")
            print()

            # â”€â”€ 3) åˆ›å»ºå¼•æ“ (å¯ç”¨ Level 1 æ·˜æ±°, å…¨è‡ªåŠ¨å‚æ•°) â”€â”€
            print("[3] Creating streaming inference engine with eviction...")
            eviction_config = EvictionConfig(
                max_cache_tokens=MAX_CACHE_TOKENS,
                # sink_size=0  â†’ è‡ªåŠ¨æ£€æµ‹é¦– chunk
                # window_size=0 â†’ è‡ªåŠ¨è®¡ç®—
            )
            engine = VideoStreamingInference(
                model, processor, device, eviction_config=eviction_config
            )
            print()

            # â”€â”€ 4) é€ chunk ç¼–ç  â”€â”€
            print("[4] Encoding video chunks...")
            t_start = time.time()
            vram_history = []
            cache_history = []
            chunk_count = 0
            first_eviction_chunk = None

            for i in range(0, total_frame_count, CHUNK_FRAMES):
                chunk = all_frames[i : i + CHUNK_FRAMES]
                if len(chunk) == 0:
                    continue
                # è¡¥é½å¶æ•°å¸§ (temporal_patch_size=2)
                if len(chunk) % 2 != 0:
                    chunk.append(chunk[-1])

                result = engine.append_video_chunk(chunk, fps=SAMPLE_FPS)
                chunk_count += 1

                info = engine.get_cache_info()
                cache_len = info["cache_seq_length"]

                # è®°å½•é¦–æ¬¡æ·˜æ±°
                if "eviction_stats" in info:
                    es = info["eviction_stats"]
                    if es.get("total_evictions", 0) > 0 and first_eviction_chunk is None:
                        first_eviction_chunk = chunk_count

                if chunk_count % PRINT_INTERVAL == 0 or chunk_count == 1:
                    vram = get_vram_gb()
                    vram_history.append({
                        "chunk": chunk_count,
                        "cache_len": cache_len,
                        "vram_alloc": vram.get("allocated", 0),
                        "vram_reserved": vram.get("reserved", 0),
                    })
                    cache_history.append(cache_len)

                    eviction_str = ""
                    if "eviction_stats" in info:
                        es = info["eviction_stats"]
                        eviction_str = (
                            f", evictions={es.get('total_evictions', 0)}, "
                            f"evicted={es.get('total_tokens_evicted', 0)}"
                        )

                    print(
                        f"  Chunk {chunk_count:>4d}/{expected_chunks}: "
                        f"cache_len={cache_len:>6d}, "
                        f"mem={info.get('cache_memory_gb', 0):.3f} GB, "
                        f"VRAM={vram.get('allocated', 0):.2f}/{vram.get('reserved', 0):.2f} GB"
                        f"{eviction_str}"
                    )

            t_encode = time.time() - t_start

            # æ±‡æ€»
            print(f"\n  âœ… Encoding completed: {chunk_count} chunks, "
                  f"{total_frame_count} frames in {t_encode:.1f}s")
            final_vram = get_vram_gb()
            print(f"  Final VRAM: {final_vram}")
            if first_eviction_chunk:
                print(f"  First eviction at chunk: {first_eviction_chunk}")

            # è·å– evictor çŠ¶æ€
            evictor = engine.cache_manager.evictor
            if evictor:
                print(f"  Effective sink_size: {evictor.effective_sink_size}")
                print(f"  Effective window_size: {evictor.effective_window_size}")
                print(f"  Avg chunk tokens: {evictor._avg_chunk_tokens:.0f}")
            print()

            # â”€â”€ 5) éªŒè¯ ask ä»å¯ç”¨ â”€â”€
            print("[5] Verification: asking a question...")
            final_info = engine.get_cache_info()
            print(f"  Pre-ask cache: len={final_info['cache_seq_length']}, "
                  f"mem={final_info.get('cache_memory_gb', 0):.3f} GB")

            answer, metrics = engine.ask(
                "Briefly describe what you saw in the entire video.",
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
            )
            print(f"  Answer: {answer}")
            print(f"  TTFT: {metrics['ttft']:.3f}s")

            # éªŒè¯ ask å cache æ¢å¤ (snapshot/restore)
            post_ask_info = engine.get_cache_info()
            print(f"  Post-ask cache: len={post_ask_info['cache_seq_length']}")
            assert post_ask_info['cache_seq_length'] == final_info['cache_seq_length'], \
                "ask() å cache é•¿åº¦åº”æ¢å¤ (snapshot/restore)"
            print(f"  âœ… Cache restored after ask()")
            print()

            # â”€â”€ 6) æ€»ç»“ â”€â”€
            print("=" * 70)
            print("SUMMARY")
            print("=" * 70)
            print(f"  Video: {VIDEO_PATH} ({duration:.0f}s)")
            print(f"  Total frames: {total_frame_count}")
            print(f"  Total chunks: {chunk_count}")
            print(f"  Encoding time: {t_encode:.1f}s "
                  f"({total_frame_count / t_encode:.2f} frames/sec)")
            print(f"  Max cache tokens: {MAX_CACHE_TOKENS}")
            print(f"  Final cache_len: {final_info['cache_seq_length']}")
            print(f"  Final VRAM: allocated={final_vram.get('allocated', 0):.2f} GB, "
                  f"reserved={final_vram.get('reserved', 0):.2f} GB, "
                  f"max_allocated={final_vram.get('max_allocated', 0):.2f} GB")

            if "eviction_stats" in final_info:
                es = final_info["eviction_stats"]
                print(f"  Total evictions: {es.get('total_evictions', 0)}")
                print(f"  Total tokens evicted: {es.get('total_tokens_evicted', 0)}")

            # é€šè¿‡åˆ¤å®š
            print()
            print("â”€" * 70)
            print("PASS/FAIL CRITERIA:")
            all_pass = True

            # 1) æ²¡ OOM (èµ°åˆ°è¿™é‡Œè¯´æ˜æ²¡ OOM)
            print(f"  âœ… [P1] No OOM â€” processed all {chunk_count} chunks "
                  f"(test_step10 OOM at 40 chunks without eviction)")

            # 2) cache_len â‰¤ max_cache_tokens
            if final_info['cache_seq_length'] <= MAX_CACHE_TOKENS:
                print(f"  âœ… [P2] cache_len ({final_info['cache_seq_length']}) "
                      f"â‰¤ max ({MAX_CACHE_TOKENS})")
            else:
                print(f"  âŒ [P2] cache_len ({final_info['cache_seq_length']}) "
                      f"> max ({MAX_CACHE_TOKENS})")
                all_pass = False

            # 3) æœ‰æ·˜æ±°å‘ç”Ÿ
            if "eviction_stats" in final_info:
                es = final_info["eviction_stats"]
                if es.get("total_evictions", 0) > 0:
                    print(f"  âœ… [P3] Eviction occurred "
                          f"({es['total_evictions']} times, "
                          f"{es['total_tokens_evicted']} tokens)")
                else:
                    print(f"  âŒ [P3] No eviction occurred â€” config may not have been applied")
                    all_pass = False

            # 4) VRAM æœªè¶… 23 GB
            max_alloc = final_vram.get("max_allocated", 0)
            if max_alloc < 23.0:
                print(f"  âœ… [P4] Max VRAM allocated ({max_alloc:.2f} GB) < 23 GB")
            else:
                print(f"  âš ï¸ [P4] Max VRAM allocated ({max_alloc:.2f} GB) â‰¥ 23 GB")
                all_pass = False

            # 5) ask æ­£å¸¸
            if answer and len(answer) > 5:
                print(f"  âœ… [P5] ask() returned valid answer ({len(answer)} chars)")
            else:
                print(f"  âŒ [P5] ask() returned empty/short answer")
                all_pass = False

            print()
            if all_pass:
                print("ğŸ‰ EXPERIMENT B: ALL PASSED")
            else:
                print("âš ï¸ EXPERIMENT B: SOME CHECKS FAILED â€” see above")

        except torch.cuda.OutOfMemoryError:
            print(f"\nâŒ EXPERIMENT B FAILED: CUDA OOM!")
            print(f"  This means eviction did not prevent OOM.")
            print(f"  Possible causes:")
            print(f"    1) Eviction not triggered â€” check EvictionConfig")
            print(f"    2) max_cache_tokens too large â€” try 50,000")
            print(f"    3) torch reserved memory fragmentation")
            import traceback
            traceback.print_exc()

        except Exception as e:
            print(f"\nâŒ EXPERIMENT B FAILED: {e}")
            import traceback
            traceback.print_exc()

        finally:
            sys.stdout = original_stdout
            sys.stderr = original_stderr

    print(f"\nReport saved to: {REPORT_PATH}")


if __name__ == "__main__":
    main()